{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to FlowyML \ud83c\udf0a","text":"<p> The Enterprise-Grade ML Pipeline Framework for Humans </p> <p>FlowyML is a production-ready ML pipeline orchestration framework that bridges the gap between rapid experimentation and enterprise deployment where assets are first-class citizens. Write pipelines as simple Python scripts, then scale them to production without rewriting a single line.</p> <p>[!TIP] The promise: Go from notebook to production in hours, not weeks. FlowyML handles orchestration, caching, versioning, and monitoring \u2014 so you can focus on ML, not infrastructure.</p>"},{"location":"#why-flowyml","title":"\ud83c\udfaf Why FlowyML?","text":""},{"location":"#the-problem-flowyml-solves","title":"The Problem FlowyML Solves","text":"<p>Most ML teams face the same painful trade-offs:</p> <ul> <li>Quick prototyping tools (notebooks, scripts) don't scale to production</li> <li>Enterprise MLOps platforms require steep learning curves and complex configuration or is vendor locked</li> <li>Orchestration frameworks force you into rigid patterns or obscure DSLs</li> <li>Experiment tracking is disconnected from execution and deployment</li> </ul> <p>FlowyML eliminates these trade-offs. It's designed for the way data scientists actually work \u2014 with pure Python \u2014 while providing enterprise-grade capabilities when you need them, extensible with plugins, assets are first-class citizens, experiments and storage are integrated and versioned.</p>"},{"location":"#what-makes-flowyml-different","title":"What Makes FlowyML Different","text":"<ul> <li> <p> Zero Boilerplate     ---     Define steps as pure Python functions. No YAML, no DSLs, no complex wiring. Just <code>@step</code> decorators and you're done.</p> <p>Why this matters: Reduces pipeline development time by 70% compared to traditional frameworks. Your team spends time on ML, not on orchestration syntax.</p> </li> <li> <p> Auto-Context Injection     ---     Parameters are automatically injected into steps based on type hints. Change a hyperparameter once, it flows everywhere.</p> <p>Why this matters: Eliminates configuration management headaches. Run the same pipeline with different configs for dev/staging/prod without code changes.</p> </li> <li> <p> Smart Caching     ---     Intelligent caching strategies (code hash, input hash) skip expensive recomputation. Only re-run what actually changed.</p> <p>Why this matters: Saves compute costs by 40-60% in iterative development. A 3-hour pipeline becomes 20 minutes when you're tweaking one step.</p> </li> <li> <p> Real-Time UI     ---     Beautiful, dark-mode dashboard to monitor pipelines, visualize DAGs, and inspect artifacts as they execute.</p> <p>Why this matters: Debug production issues in minutes instead of hours. See exactly what's happening, when it's happening.</p> </li> <li> <p>:plug: Extensive Plugin Ecosystem     ---     FlowyML is built on top of plugins, which allows you to extend its functionality with custom plugins and components from other frameworks.</p> <p>Why this matters: FlowyML is designed to be flexible and composable.</p> </li> <li> <p>:branch: Steps Grouping, branching and conditions     ---     FlowyML allows you to group consecutive steps to run in the same container/executor. Perfect for reducing overhead while maintaining clear step boundaries and separation of concerns with branching and conditions to incorporate logic into your pipelines natively</p> <p>Why this matters: FlowyML allows for clear separation of concerns and composable logic into your pipelines natively.</p> </li> <li> <p> Centralized Hub &amp; Docker     ---     Run locally per project or deploy as a centralized entity for the company. Backend and Frontend are fully dockerized.</p> <p>Why this matters: Start small on your laptop, then scale to a company-wide platform without changing your code.</p> </li> <li> <p> Project-Based Organization     ---     Built-in multi-tenancy for managing multiple teams and initiatives. Scoped runs, artifacts, and metadata.</p> <p>Why this matters: Keep your work organized and secure as your team grows.</p> </li> <li> <p> Pipeline Templates     ---     Stop reinventing the wheel. Use pre-built templates for common ML patterns like Training, ETL, and A/B Testing.</p> <p>Why this matters: Standardize your team's workflows and get started in seconds.</p> </li> </ul>"},{"location":"#real-world-impact","title":"\ud83d\udca1 Real-World Impact","text":"<p>Here's what FlowyML delivers in practice:</p> Challenge Without FlowyML With FlowyML Dev \u2192 Production 4-8 weeks of rewriting Hours using same code Pipeline iteration Full re-runs (hours) Cached steps (minutes) Debugging failures Parse logs, guess state Visual DAG + artifact inspection Team collaboration Divergent scripts Standardized, versioned pipelines Multi-environment Rewrite for each Single pipeline, multiple stacks Experiment tracking Manual logging Automatic lineage + versioning <p>[!IMPORTANT] Production-Ready from Day One: FlowyML isn't just for prototypes. It's built for regulated industries, multi-tenant deployments, and enterprise scale. But you can start simple and grow into those features.</p>"},{"location":"#feature-showcase","title":"\ud83d\ude80 Feature Showcase","text":"<p>FlowyML isn't just another orchestrator. It's a complete toolkit for building, debugging, and deploying ML applications.</p>"},{"location":"#1-zero-boilerplate-orchestration","title":"1. Zero-Boilerplate Orchestration","text":"<p>Write pipelines as standard Python functions. No YAML, no DSLs, no complex wiring.</p> <pre><code>@step(outputs=[\"data\"])\ndef load():\n    return [1, 2, 3]\n\n@step(inputs=[\"data\"], outputs=[\"model\"])\ndef train(data):\n    return Model.train(data)\n\n# It's just Python!\npipeline = Pipeline(\"simple\").add_step(load).add_step(train)\npipeline.run()\n</code></pre>"},{"location":"#2-intelligent-caching","title":"2. \ud83e\udde0 Intelligent Caching","text":"<p>Don't waste time re-running successful steps. FlowyML's multi-strategy caching understands your code and data.</p> <ul> <li>Code Hash: Re-runs only when you change the code.</li> <li>Input Hash: Re-runs only when input data changes.</li> <li>Time-based: Re-runs after a specific duration.</li> </ul> <pre><code># Only re-runs if 'data' changes, ignoring code changes\n@step(cache=\"input_hash\", outputs=[\"processed\"])\ndef expensive_processing(data):\n    return process(data)\n</code></pre>"},{"location":"#3-llm-genai-ready","title":"3. \ud83e\udd16 LLM &amp; GenAI Ready","text":"<p>Built-in tools for the new era of AI. Trace token usage, latency, and costs automatically with built-in observability.</p> <pre><code>from flowyml import trace_llm\n\n@step\n@trace_llm(model=\"gpt-4\", tags=[\"production\"])\ndef generate_summary(text: str):\n    # flowyml automatically tracks:\n    # - Token usage (prompt/completion)\n    # - Cost estimation\n    # - Latency &amp; Success/Failure rates\n    return openai.ChatCompletion.create(...)\n</code></pre>"},{"location":"#4-efficient-step-grouping-separation-of-concerns","title":"4. \u26a1 Efficient Step Grouping &amp; Separation of Concerns","text":"<p>Group consecutive steps to run in the same container/executor. Perfect for reducing overhead while maintaining clear step boundaries and keeping logic separate from configuration.</p> <pre><code>from flowyml.core.resources import ResourceRequirements, GPUConfig\n\n# Group preprocessing steps - they'll share the same container\n@step(outputs=[\"raw\"], execution_group=\"preprocessing\",\n      resources=ResourceRequirements(cpu=\"2\", memory=\"4Gi\"))\ndef load_data():\n    return fetch_from_source()\n\n@step(inputs=[\"raw\"], outputs=[\"clean\"], execution_group=\"preprocessing\",\n      resources=ResourceRequirements(cpu=\"4\", memory=\"8Gi\"))\ndef clean_data(raw):\n    return preprocess(raw)\n\n# FlowyML automatically:\n# \u2705 Analyzes DAG for consecutive steps\n# \u2705 Aggregates resources (cpu=\"4\", memory=\"8Gi\")\n# \u2705 Executes in same environment (no container restart)\n</code></pre> <p>[!TIP] Why this matters: Traditional frameworks (like ZenML) run each step in a separate container, creating unnecessary overhead. FlowyML's intelligent grouping lets you maintain clean step separation while optimizing execution.</p>"},{"location":"#5-dynamic-workflows","title":"5. \ud83d\udd00 Dynamic Workflows","text":"<p>Real-world ML isn't linear. Build complex, adaptive workflows with conditional logic and branching.</p> <pre><code>from flowyml import If, Switch\n\n# Run 'deploy' only if model accuracy &gt; 0.9\npipeline.add_step(\n    If(condition=lambda ctx: ctx[\"accuracy\"] &gt; 0.9)\n    .then(deploy_model)\n    .else_(notify_team)\n)\n\n# Multi-way branching\npipeline.add_step(\n    Switch(selector=lambda ctx: ctx[\"model_type\"])\n    .case(\"classification\", train_classifier)\n    .case(\"regression\", train_regressor)\n    .default(train_generic)\n)\n</code></pre>"},{"location":"#6-universal-plugin-system","title":"6. \ud83e\udde9 Universal Plugin System","text":"<p>Extend with any tool. Even wrap and reuse ZenML components!</p> <pre><code>from flowyml.stacks.plugins import load_component\n\n# Load any ZenML orchestrator\nk8s_orch = load_component(\n    \"zenml:zenml.integrations.kubernetes.orchestrators.KubernetesOrchestrator\"\n)\n\n# Use ZenML integrations\nmlflow_tracker = load_component(\"zenml:zenml.integrations.mlflow.MLflowExperimentTracker\")\ngreat_expectations = load_component(\"zenml:zenml.integrations.great_expectations.DataValidator\")\n</code></pre> <p>[!IMPORTANT] Best of Both Worlds: FlowyML's plugin system gives you access to ZenML's entire ecosystem while maintaining FlowyML's superior developer experience.</p>"},{"location":"#7-human-in-the-loop","title":"7. \ud83d\udc64 Human-in-the-Loop","text":"<p>Pause pipelines for manual approval, review, or intervention.</p> <pre><code>from flowyml import approval\n\npipeline.add_step(train_model)\npipeline.add_step(\n    approval(\n        name=\"approve_deployment\",\n        approver=\"ml-team\",\n        timeout_seconds=3600,\n        auto_approve_if=lambda: os.getenv(\"CI\") == \"true\"\n    )\n)\npipeline.add_step(deploy_model)\n</code></pre>"},{"location":"#8-built-in-experiment-tracking","title":"8. \ud83d\udcca Built-in Experiment Tracking","text":"<p>No external tools needed. Tracking is built-in and automatic.</p> <pre><code>from flowyml.tracking import Experiment\n\nexp = Experiment(\n    name=\"baseline_training\",\n    description=\"Baseline model experiments\",\n    tags={\"framework\": \"pytorch\", \"version\": \"v1\"}\n)\n\nexp.log_run(\n    run_id=\"run_001\",\n    metrics={\"accuracy\": 0.95, \"loss\": 0.12},\n    parameters={\"lr\": 0.01, \"batch_size\": 32}\n)\n\n# Get best performing run\nbest = exp.get_best_run(\"accuracy\", maximize=True)\n</code></pre>"},{"location":"#9-model-leaderboard-registry","title":"9. \ud83c\udfc6 Model Leaderboard &amp; Registry","text":"<p>Track, compare, version, and stage your models.</p> <pre><code>from flowyml import ModelLeaderboard\nfrom flowyml.core import Model\n\n# Leaderboard for model comparison\nleaderboard = ModelLeaderboard(metric=\"accuracy\", higher_is_better=True)\nleaderboard.add_score(model_name=\"bert-base\", run_id=\"run_123\", score=0.92)\nleaderboard.display()  # Beautiful console output\n\n# Model registry with stages\nmodel = Model.create(artifact=trained_model, score=0.95)\nmodel.register(name=\"text_classifier\", stage=\"production\", version=\"v1.2.0\")\n</code></pre>"},{"location":"#10-built-in-scheduling","title":"10. \ud83d\udcc5 Built-in Scheduling","text":"<p>Schedule recurring jobs without external orchestrators.</p> <pre><code>from flowyml import PipelineScheduler\n\nscheduler = PipelineScheduler()\n\n# Daily at 2 AM\nscheduler.schedule_daily(\n    name=\"daily_training\",\n    pipeline_func=lambda: pipeline.run(),\n    hour=2, minute=0\n)\n\n# Every 6 hours\nscheduler.schedule_interval(\n    name=\"data_refresh\",\n    pipeline_func=lambda: refresh_pipeline.run(),\n    hours=6\n)\n\nscheduler.start()  # Non-blocking\n</code></pre>"},{"location":"#11-smart-notifications","title":"11. \ud83d\udd14 Smart Notifications","text":"<p>Slack, Email, and custom alerts. All built-in.</p> <pre><code>from flowyml import configure_notifications, get_notifier\n\nconfigure_notifications(\n    console=True,\n    slack_webhook=\"https://hooks.slack.com/services/YOUR/WEBHOOK\",\n    email_config={...}\n)\n\n# Automatic notifications\n# - Pipeline start/success/failure\n# - Data drift detection\n# - Manual triggers available\n\nnotifier = get_notifier()\nnotifier.notify(\"Model deployed!\", level=\"success\")\n</code></pre>"},{"location":"#12-interactive-debugging","title":"12. \ud83c\udfaf Interactive Debugging","text":"<p>Set breakpoints, inspect state, and debug like regular Python code.</p> <pre><code>from flowyml import StepDebugger\n\ndebugger = StepDebugger()\ndebugger.set_breakpoint(\"train_model\")\n\n# Run with debugging enabled\npipeline.run(debug=True)\n\n# When breakpoint hits:\n# - Inspect variables\n# - Check intermediate outputs\n# - Step through execution\n</code></pre>"},{"location":"#13-first-class-asset-types","title":"13. \ud83d\udce6 First-Class Asset Types","text":"<p>Assets are not just files; they are first-class citizens with lineage, metadata, and versioning. Specialized types for ML workflows.</p> <pre><code>from flowyml.core import Dataset, Model, Metrics, FeatureSet\n\n# Type-safe ML assets with metadata\ndataset = Dataset.create(\n    data=df,\n    name=\"training_data\",\n    metadata={\"source\": \"postgres\", \"rows\": 10000}\n)\n\nmodel = Model.create(\n    artifact=trained_model,\n    score=0.95,\n    metadata={\"framework\": \"pytorch\", \"params\": {...}}\n)\n\nmetrics = Metrics.create(values={\"accuracy\": 0.95, \"f1\": 0.93})\n</code></pre>"},{"location":"#14-smart-retries-circuit-breakers","title":"14. \ud83d\udd04 Smart Retries &amp; Circuit Breakers","text":"<p>Handle failures gracefully. Production-ready from day one.</p> <pre><code># Automatic retries\n@step(retry=3, timeout=300)\ndef flaky_api_call():\n    return external_api.fetch()\n\n# Circuit breakers prevent cascading failures\n# Automatically stops calling failing dependencies\n</code></pre>"},{"location":"#15-data-drift-detection","title":"15. \ud83d\udcc8 Data Drift Detection","text":"<p>Monitor distribution shifts. Trigger retraining automatically.</p> <pre><code>from flowyml import detect_drift\n\ndrift_result = detect_drift(\n    reference_data=train_feature,\n    current_data=prod_feature,\n    threshold=0.1  # PSI threshold\n)\n\nif drift_result['drift_detected']:\n    print(f\"\u26a0\ufe0f Drift detected! PSI: {drift_result['psi']:.4f}\")\n    trigger_retraining()\n</code></pre>"},{"location":"#16-pipeline-versioning","title":"16. \ud83c\udf10 Pipeline Versioning","text":"<p>Git-like versioning for pipelines. Track changes, compare, rollback.</p> <pre><code>from flowyml import VersionedPipeline\n\npipeline = VersionedPipeline(\"training\", version=\"v1.0.0\")\npipeline.add_step(load_data)\npipeline.save_version()\n\n# ... make changes ...\npipeline.version = \"v1.1.0\"\npipeline.save_version()\n\n# Compare versions to see exactly what changed (steps, code, context)\ndiff = pipeline.compare_with(\"v1.0.0\")\nprint(diff[\"modified_steps\"])  # ['train_model']\nprint(diff[\"context_changes\"]) # {'learning_rate': {'old': 0.01, 'new': 0.001}}\n</code></pre>"},{"location":"#17-enterprise-production-features","title":"17. \ud83c\udfed Enterprise Production Features","text":"<p>Everything you need to run mission-critical workloads.</p> <ul> <li>\ud83d\udd04 Automatic Retries: Handle transient failures gracefully.</li> <li>\u23f0 Scheduling: Built-in cron scheduler for recurring jobs.</li> <li>\ud83d\udd14 Notifications: Slack/Email alerts on success or failure.</li> <li>\ud83d\udee1\ufe0f Circuit Breakers: Stop cascading failures automatically.</li> </ul>"},{"location":"#18-centralized-hub-docker","title":"18. \ud83c\udfe2 Centralized Hub &amp; Docker","text":"<p>Ready for the enterprise. Run locally per project or deploy as a centralized entity for the company. - Docker Ready: Backend and Frontend are fully dockerized. - Centralized Hub: Share pipelines, artifacts, and experiments across the team.</p> <ul> <li>Remote Execution: Configure local clients to execute on the remote hub.</li> </ul>"},{"location":"#19-universal-integrations","title":"19. \ud83d\udd0c Universal Integrations","text":"<p>Works with your existing stack.</p> <ul> <li>ML Frameworks: PyTorch, TensorFlow, Keras, Scikit-learn, HuggingFace.</li> <li>Cloud Providers: AWS, GCP, Azure (via plugins).</li> <li>Tools: MLflow, Weights &amp; Biases, Great Expectations.</li> </ul>"},{"location":"#20-project-based-organization","title":"20. \ud83d\udcc2 Project-Based Organization","text":"<p>Built-in multi-tenancy for managing multiple teams and initiatives.</p> <pre><code>from flowyml import Project\n\nproject = Project(\"recommendation_system\")\npipeline = project.create_pipeline(\"training\")\n\n# All runs, artifacts, and metadata are automatically scoped to the project\nruns = project.list_runs()\nstats = project.get_stats()\n</code></pre>"},{"location":"#21-pipeline-templates","title":"21. \ud83d\udcdd Pipeline Templates","text":"<p>Stop reinventing the wheel. Use pre-built templates for common ML patterns.</p> <pre><code>from flowyml.core.templates import create_from_template\n\n# Create a standard training pipeline in one line\npipeline = create_from_template(\n    \"ml_training\",\n    data_loader=my_loader,\n    trainer=my_trainer,\n    evaluator=my_evaluator\n)\n</code></pre>"},{"location":"#quick-start","title":"\u26a1\ufe0f Quick Start","text":"<p>See how simple it is \u2014 this is a complete, runnable ML pipeline:</p> <pre><code>from flowyml import Pipeline, step, context\n\n@step(outputs=[\"dataset\"])\ndef load_data():\n    return [1, 2, 3, 4, 5]\n\n@step(inputs=[\"dataset\"], outputs=[\"model\"])\ndef train_model(dataset, learning_rate: float = 0.01):\n    # 'learning_rate' is automatically injected from context!\n    print(f\"Training on {len(dataset)} items with lr={learning_rate}\")\n    return \"my_trained_model\"\n\n# Run it!\nctx = context(learning_rate=0.05)\npipeline = Pipeline(\"quickstart\", context=ctx)\npipeline.add_step(load_data)\npipeline.add_step(train_model)\n\npipeline.run()\n</code></pre> <p>That's it. No YAML. No config files. No boilerplate. Just Python.</p>"},{"location":"#ready-to-build-better-pipelines","title":"\ud83d\ude80 Ready to Build Better Pipelines?","text":"<ul> <li> <p> Getting Started     ---     Build your first pipeline in 5 minutes. No prior MLOps experience required.</p> </li> <li> <p> Core Concepts     ---     Understand pipelines, steps, context, and assets \u2014 the building blocks of FlowyML.</p> </li> <li> <p> Advanced Features     ---     Deep dive into caching, parallelism, debugging, and production patterns.</p> </li> <li> <p> User Guide     ---     Master projects, versioning, scheduling, and multi-tenant deployments.</p> </li> <li> <p>:plug: Integrations     ---     Connect with Keras, GCP, and your existing ML stack.</p> </li> <li> <p> API Reference     ---     Complete API documentation for every class and function.</p> </li> </ul> <p>Questions? Issues? Open an issue on GitHub or check out the Resources page for community links.</p>"},{"location":"INSTALLATION/","title":"Installation Guide \ud83d\udce6","text":"<p>[!TIP] Quick start: If you're just exploring FlowyML, run <code>pip install \"flowyml[all]\"</code> and you're ready to go. Come back to this page when you need production-grade deployment.</p>"},{"location":"INSTALLATION/#system-requirements","title":"System Requirements","text":"<ul> <li>Python: 3.8 or higher (3.10+ recommended for best performance)</li> <li>Operating System: Linux, macOS, Windows (WSL2 recommended for Windows)</li> <li>Memory: Minimum 4GB RAM (8GB+ for larger pipelines)</li> <li>Disk Space: 500MB for full installation</li> </ul> <p>Why these requirements: FlowyML is designed to run anywhere Python runs. The minimum specs support local development; production workloads scale based on your pipeline needs.</p>"},{"location":"INSTALLATION/#installation-options","title":"Installation Options","text":"<p>Choose the installation that matches your use case:</p>"},{"location":"INSTALLATION/#basic-installation-local-development-only","title":"Basic Installation \u2014 Local Development Only","text":"<p>Install FlowyML core package:</p> <pre><code>pip install flowyml\n</code></pre> <p>What you get: Core pipeline orchestration, local artifact storage, metadata tracking.</p> <p>Use this when: You're prototyping locally and don't need cloud deployment or the web UI yet.</p>"},{"location":"INSTALLATION/#full-installation-everything-included-recommended","title":"Full Installation \u2014 Everything Included (Recommended)","text":"<pre><code>pip install \"flowyml[all]\"\n</code></pre> <p>What you get: Web UI, cloud integrations, ML framework support, everything.</p> <p>Use this when: You want to try all FlowyML features without reinstalling. This is the recommended approach for new users.</p>"},{"location":"INSTALLATION/#optional-dependencies-pick-what-you-need","title":"Optional Dependencies \u2014 Pick What You Need","text":"<p>For minimal installations or specific production setups, install only what you need:</p>"},{"location":"INSTALLATION/#ml-framework-support","title":"ML Framework Support","text":"<pre><code># TensorFlow/Keras automatic tracking\npip install \"flowyml[tensorflow]\"\n\n# PyTorch integration\npip install \"flowyml[pytorch]\"\n\n# Scikit-learn utilities\npip install \"flowyml[sklearn]\"\n</code></pre> <p>Use this when: You're building Docker images and want to minimize size, or you only use specific frameworks.</p>"},{"location":"INSTALLATION/#cloud-platform-support","title":"Cloud Platform Support","text":"<pre><code># Google Cloud Platform (Vertex AI, GCS, Container Registry)\npip install \"flowyml[gcp]\"\n\n# AWS support (coming soon)\npip install \"flowyml[aws]\"\n\n# Azure support (coming soon)\npip install \"flowyml[azure]\"\n</code></pre> <p>Use this when: You're deploying to cloud and want cloud-specific features like managed orchestration (Vertex AI), cloud storage (GCS/S3), and container registries.</p>"},{"location":"INSTALLATION/#web-ui-api-server","title":"Web UI &amp; API Server","text":"<pre><code>pip install \"flowyml[ui]\"\n</code></pre> <p>What you get: The visualization dashboard, REST API,real-time monitoring.</p> <p>Use this when: You need the visual interface for debugging or monitoring, or building tools that integrate with FlowyML's API.</p>"},{"location":"INSTALLATION/#development","title":"Development","text":"<p>Install development dependencies:</p> <pre><code>pip install flowyml[dev]\n</code></pre>"},{"location":"INSTALLATION/#combining-extras","title":"Combining Extras","text":"<p>You can combine multiple extras:</p> <pre><code># TensorFlow + GCP\npip install flowyml[tensorflow,gcp]\n\n# All ML frameworks + GCP\npip install flowyml[tensorflow,pytorch,sklearn,gcp]\n\n# Everything including UI\npip install flowyml[all]\n</code></pre>"},{"location":"INSTALLATION/#from-source","title":"From Source","text":"<pre><code>git clone https://github.com/UnicoLab/FlowyML.git\ncd FlowyML\npip install -e \".[all]\"\n</code></pre>"},{"location":"INSTALLATION/#docker","title":"Docker","text":"<pre><code># Basic image\ndocker pull flowyml/flowyml:latest\n\n# With TensorFlow\ndocker pull flowyml/flowyml:latest-tf\n\n# With PyTorch\ndocker pull flowyml/flowyml:latest-torch\n\n# Full image\ndocker pull flowyml/flowyml:latest-full\n</code></pre>"},{"location":"INSTALLATION/#verification","title":"Verification","text":"<p>Verify installation:</p> <pre><code>import flowyml\nprint(flowyml.__version__)\n\n# Check available features\nfrom flowyml import check_features\ncheck_features()\n</code></pre>"},{"location":"INSTALLATION/#requirements-by-use-case","title":"Requirements by Use Case","text":""},{"location":"INSTALLATION/#local-development","title":"Local Development","text":"<pre><code>pip install flowyml\n</code></pre>"},{"location":"INSTALLATION/#ml-training-tensorflow","title":"ML Training (TensorFlow)","text":"<pre><code>pip install flowyml[tensorflow]\n</code></pre>"},{"location":"INSTALLATION/#ml-training-pytorch","title":"ML Training (PyTorch)","text":"<pre><code>pip install flowyml[pytorch]\n</code></pre>"},{"location":"INSTALLATION/#production-on-gcp","title":"Production on GCP","text":"<pre><code>pip install flowyml[gcp,tensorflow]  # or pytorch\n</code></pre>"},{"location":"INSTALLATION/#full-development-setup","title":"Full Development Setup","text":"<pre><code>pip install flowyml[all,dev]\n</code></pre>"},{"location":"INSTALLATION/#installation-best-practices","title":"Installation Best Practices \ud83d\udca1","text":""},{"location":"INSTALLATION/#use-virtual-environments","title":"Use Virtual Environments","text":"<p>Always use virtual environments to avoid dependency conflicts:</p> <pre><code># Using venv (built-in)\npython -m venv flowyml-env\nsource flowyml-env/bin/activate  # Windows: flowyml-env\\Scripts\\activate\npip install \"flowyml[all]\"\n\n# Using conda\nconda create -n flowyml python=3.10\nconda activate flowyml\npip install \"flowyml[all]\"\n</code></pre> <p>Why this matters: Prevents conflicts with other Python projects and makes it easy to reproduce your environment.</p>"},{"location":"INSTALLATION/#pin-versions-in-production","title":"Pin Versions in Production","text":"<p>For production deployments, pin exact versions:</p> <pre><code># Generate requirements file\npip freeze &gt; requirements.txt\n\n# Or use Poetry (recommended)\npip install flowyml[all]\npoetry lock\n</code></pre> <p>Why this matters: Ensures reproducible deployments and prevents surprise breakages from dependency updates.</p>"},{"location":"INSTALLATION/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":""},{"location":"INSTALLATION/#module-not-found-errors-for-optional-features","title":"\"Module not found\" errors for optional features","text":"<p>Problem: You see <code>ImportError</code> or <code>ModuleNotFoundError</code> when using specific features.</p> <p>Solution: Install the corresponding extra:</p> <pre><code># For TensorFlow features\npip install \"flowyml[tensorflow]\"\n\n# For GCP features\npip install \"flowyml[gcp]\"\n\n# Or just install everything\npip install \"flowyml[all]\"\n</code></pre>"},{"location":"INSTALLATION/#dependency-version-conflicts","title":"Dependency version conflicts","text":"<p>Problem: pip reports conflicts between FlowyML's dependencies and existing packages.</p> <p>Solution: Create a fresh virtual environment:</p> <pre><code># Deactivate current environment if active\ndeactivate\n\n# Create new environment\npython -m venv new-flowyml-env\nsource new-flowyml-env/bin/activate\npip install \"flowyml[all]\"\n</code></pre>"},{"location":"INSTALLATION/#python-version-too-old","title":"Python version too old","text":"<p>Problem: Installation fails with Python version errors.</p> <p>Solution: Upgrade Python to 3.8 or higher:</p> <pre><code># Check current version\npython --version\n\n# Using conda (recommended)\nconda create -n flowyml python=3.10\nconda activate flowyml\n\n# Or use pyenv\npyenv install 3.10.0\npyenv local 3.10.0\n</code></pre>"},{"location":"INSTALLATION/#gcp-authentication-issues","title":"GCP authentication issues","text":"<p>Problem: Can't access GCS or Vertex AI.</p> <p>Solution: Authenticate with gcloud:</p> <pre><code># Install gcloud CLI first: https://cloud.google.com/sdk/docs/install\ngcloud auth login\ngcloud auth application-default login\ngcloud config set project YOUR-PROJECT-ID\n</code></pre>"},{"location":"INSTALLATION/#installation-is-slow","title":"Installation is slow","text":"<p>Problem: <code>pip install</code> takes a very long time.</p> <p>Solution: Clear pip cache or use a faster mirror:</p> <pre><code># Clear cache\npip cache purge\n\n# Or upgrade pip/setuptools\npip install --upgrade pip setuptools wheel\n\n# Then retry\npip install \"flowyml[all]\"\n</code></pre>"},{"location":"INSTALLATION/#next-steps","title":"Next Steps","text":"<p>Once installed:</p> <ol> <li>Verify installation: Run <code>flowyml --version</code> to confirm</li> <li>Build your first pipeline: Follow the Getting Started guide</li> <li>Explore examples: Check out the Examples page for real-world patterns</li> </ol> <p>Need help? Visit the Resources page for community support and troubleshooting guides.</p>"},{"location":"QUICK_REFERENCE/","title":"\ud83d\ude80 FlowyML Quick Reference","text":"<p>[!NOTE] What this page is: A cheat sheet for common FlowyML commands and patterns. Perfect for bookmarking.</p> <p>When to use it: You know what you want to do, you just need the syntax.</p>"},{"location":"QUICK_REFERENCE/#decision-guide-which-command-do-i-need","title":"\ud83d\udcdd Decision Guide: Which Command Do I Need?","text":"I want to... Use this command Create a new project <code>flowyml init</code> Run a pipeline locally <code>python pipeline.py</code> or <code>flowyml run pipeline.py</code> Run with different config <code>flowyml run pipeline.py --context key=value</code> Deploy to production <code>flowyml run pipeline.py --stack production</code> Use GPUs <code>flowyml run pipeline.py --resources gpu_training</code> See what would run <code>flowyml run pipeline.py --dry-run</code> List available stacks <code>flowyml stack list</code> Start the web UI <code>flowyml ui start</code>"},{"location":"QUICK_REFERENCE/#cli-commands","title":"CLI Commands","text":""},{"location":"QUICK_REFERENCE/#initialize-project","title":"Initialize Project","text":"<pre><code>flowyml init                    # Create flowyml.yaml in current directory\n</code></pre> <p>When to use: Starting a new FlowyML project. Creates configuration scaffolding.</p>"},{"location":"QUICK_REFERENCE/#stack-management","title":"Stack Management","text":"<pre><code>flowyml stack list             # List all configured stacks\nflowyml stack show STACK_NAME  # Show detailed stack configuration\nflowyml stack set-default NAME # Set which stack runs by default\n</code></pre> <p>When to use: Managing multiple deployment targets (local, staging, production).</p> <p>[!TIP] Pro tip: Run <code>flowyml stack list</code> to verify your configuration before deploying to production.</p>"},{"location":"QUICK_REFERENCE/#run-pipelines","title":"Run Pipelines","text":"<pre><code># Basic run (uses default stack from flowyml.yaml)\nflowyml run pipeline.py\n\n# Specify stack\nflowyml run pipeline.py --stack production\nflowyml run pipeline.py -s production\n\n# Specify resources\nflowyml run pipeline.py --resources gpu_training\nflowyml run pipeline.py -r gpu_training\n\n# Pass context variables\nflowyml run pipeline.py --context data_path=/path/to/data\nflowyml run pipeline.py --context key1=val1 --context key2=val2\nflowyml run pipeline.py -ctx data_path=/path -ctx model_id=123\n\n# Custom config file\nflowyml run pipeline.py --config custom.yaml\nflowyml run pipeline.py -c custom.yaml\n\n# Dry run (show what would be executed)\nflowyml run pipeline.py --stack production --dry-run\n\n# Combined example\nflowyml run pipeline.py --stack production --resources gpu_training --context data_path=gs://bucket/data.csv\n</code></pre> <p>Decision guide: - Use <code>--stack</code> when deploying to different environments (local/staging/prod) - Use <code>--resources</code> when you need specific compute (CPU vs GPU) - Use <code>--context</code> to override parameters without changing code - Use <code>--dry-run</code> to verify configuration before expensive runs</p>"},{"location":"QUICK_REFERENCE/#configuration-file-flowymlyaml","title":"Configuration File (flowyml.yaml)","text":""},{"location":"QUICK_REFERENCE/#minimal-configuration","title":"Minimal Configuration","text":"<pre><code>stacks:\n  local:\n    type: local\n\ndefault_stack: local\n</code></pre>"},{"location":"QUICK_REFERENCE/#full-configuration","title":"Full Configuration","text":"<pre><code># Stack definitions\nstacks:\n  local:\n    type: local\n    artifact_store:\n      path: .flowyml/artifacts\n    metadata_store:\n      path: .flowyml/metadata.db\n\n  production:\n    type: gcp\n    project_id: ${GCP_PROJECT_ID}\n    region: us-central1\n    artifact_store:\n      type: gcs\n      bucket: ${GCP_BUCKET}\n    container_registry:\n      type: gcr\n      uri: gcr.io/${GCP_PROJECT_ID}\n    orchestrator:\n      type: vertex_ai\n\n# Default stack\ndefault_stack: local\n\n# Resource presets\nresources:\n  default:\n    cpu: \"2\"\n    memory: \"8Gi\"\n\n  gpu_training:\n    cpu: \"8\"\n    memory: \"32Gi\"\n    gpu: \"nvidia-tesla-v100\"\n    gpu_count: 2\n\n# Docker configuration\ndocker:\n  dockerfile: ./Dockerfile     # Auto-detect existing Dockerfile\n  use_poetry: true            # Or use Poetry from pyproject.toml\n  # requirements_file: requirements.txt  # Or use requirements.txt\n  base_image: python:3.11-slim\n  env_vars:\n    PYTHONUNBUFFERED: \"1\"\n</code></pre>"},{"location":"QUICK_REFERENCE/#environment-variables","title":"Environment Variables","text":"<p>Create <code>.env</code> file: <pre><code>GCP_PROJECT_ID=my-project\nGCP_BUCKET=my-artifacts\nGCP_SERVICE_ACCOUNT=my-sa@project.iam.gserviceaccount.com\n</code></pre></p> <p>Reference in <code>flowyml.yaml</code>: <pre><code>stacks:\n  production:\n    project_id: ${GCP_PROJECT_ID}\n</code></pre></p>"},{"location":"QUICK_REFERENCE/#pipeline-code-clean-simple","title":"Pipeline Code (Clean &amp; Simple)","text":"<pre><code>from flowyml import Pipeline, step, context\n\n@step(outputs=[\"result\"])\ndef my_step(input_data: str):\n    # Your logic\n    return input_data.upper()\n\n# NO infrastructure code needed!\nctx = context(input_data=\"hello\")\npipeline = Pipeline(\"my_pipeline\", context=ctx)\npipeline.add_step(my_step)\n\nif __name__ == \"__main__\":\n    result = pipeline.run()\n    print(f\"Result: {result.outputs['result']}\")  # \"HELLO\"\n</code></pre>"},{"location":"QUICK_REFERENCE/#common-workflows","title":"Common Workflows","text":""},{"location":"QUICK_REFERENCE/#development-production","title":"Development \u2192 Production","text":"<pre><code># 1. Develop locally\nflowyml run train.py\n\n# 2. Test on staging\nflowyml run train.py --stack staging\n\n# 3. Deploy to production\nflowyml run train.py --stack production --resources gpu_training\n</code></pre>"},{"location":"QUICK_REFERENCE/#different-regions","title":"Different Regions","text":"<pre><code># flowyml.yaml\nstacks:\n  us-prod:\n    type: gcp\n    region: us-central1\n\n  eu-prod:\n    type: gcp\n    region: europe-west1\n</code></pre> <pre><code># Deploy to US\nflowyml run pipeline.py --stack us-prod\n\n# Deploy to EU\nflowyml run pipeline.py --stack eu-prod\n</code></pre>"},{"location":"QUICK_REFERENCE/#gpu-vs-cpu-workloads","title":"GPU vs CPU Workloads","text":"<pre><code># flowyml.yaml\nresources:\n  cpu_only:\n    cpu: \"4\"\n    memory: \"16Gi\"\n\n  gpu_large:\n    cpu: \"16\"\n    memory: \"64Gi\"\n    gpu: \"nvidia-tesla-v100\"\n    gpu_count: 4\n</code></pre> <pre><code># CPU workload\nflowyml run preprocess.py --resources cpu_only\n\n# GPU training\nflowyml run train.py --resources gpu_large\n</code></pre>"},{"location":"QUICK_REFERENCE/#docker-patterns","title":"Docker Patterns","text":""},{"location":"QUICK_REFERENCE/#use-existing-dockerfile","title":"Use Existing Dockerfile","text":"<pre><code>docker:\n  dockerfile: ./Dockerfile\n  build_context: .\n</code></pre>"},{"location":"QUICK_REFERENCE/#use-poetry","title":"Use Poetry","text":"<pre><code>docker:\n  use_poetry: true\n</code></pre>"},{"location":"QUICK_REFERENCE/#use-requirementstxt","title":"Use Requirements.txt","text":"<pre><code>docker:\n  requirements_file: requirements.txt\n</code></pre>"},{"location":"QUICK_REFERENCE/#dynamic-build","title":"Dynamic Build","text":"<pre><code>docker:\n  base_image: python:3.11-slim\n  requirements:\n    - tensorflow&gt;=2.12.0\n    - pandas&gt;=2.0.0\n  env_vars:\n    PYTHONUNBUFFERED: \"1\"\n</code></pre>"},{"location":"QUICK_REFERENCE/#installation","title":"Installation","text":"<pre><code># Basic\npip install flowyml\n\n# With GCP support\npip install flowyml[gcp]\n\n# With ML frameworks\npip install flowyml[tensorflow]\npip install flowyml[pytorch]\n\n# Everything\npip install flowyml[all]\n</code></pre>"},{"location":"QUICK_REFERENCE/#troubleshooting","title":"Troubleshooting","text":""},{"location":"QUICK_REFERENCE/#stack-not-found","title":"\"Stack not found\"","text":"<pre><code># Check available stacks\nflowyml stack list\n\n# Verify config file\ncat flowyml.yaml\n</code></pre>"},{"location":"QUICK_REFERENCE/#missing-dependencies","title":"\"Missing dependencies\"","text":"<pre><code># Install required extras\npip install flowyml[gcp]\n</code></pre>"},{"location":"QUICK_REFERENCE/#real-world-workflow-examples","title":"\ud83c\udfaf Real-World Workflow Examples","text":""},{"location":"QUICK_REFERENCE/#scenario-1-development-production","title":"Scenario 1: Development \u2192 Production","text":"<p>Goal: Test locally, then deploy to production with GPUs.</p> <pre><code># 1. Develop and test locally\nflowyml run train.py\n\n# 2. Verify on staging with production-like resources\nflowyml run train.py --stack staging --resources gpu_small\n\n# 3. Deploy to production with full resources\nflowyml run train.py --stack production --resources gpu_large\n</code></pre> <p>Why this pattern works: Same code, different infrastructure. Zero rewrites.</p>"},{"location":"QUICK_REFERENCE/#scenario-2-multi-region-deployment","title":"Scenario 2: Multi-Region Deployment","text":"<p>Goal: Deploy the same pipeline to different cloud regions.</p> <pre><code># flowyml.yaml\nstacks:\n  us-prod:\n    type: gcp\n    region: us-central1\n  eu-prod:\n    type: gcp\n    region: europe-west1\n</code></pre> <pre><code># Deploy to US region\nflowyml run pipeline.py --stack us-prod\n\n# Deploy to EU region\nflowyml run pipeline.py --stack eu-prod\n</code></pre> <p>Why this pattern works: Data residency compliance, latency optimization, disaster recovery.</p>"},{"location":"QUICK_REFERENCE/#scenario-3-different-workload-types","title":"Scenario 3: Different Workload Types","text":"<p>Goal: Run preprocessing on CPU, training on GPU.</p> <pre><code># flowyml.yaml\nresources:\n  cpu_only:\n    cpu: \"4\"\n    memory: \"16Gi\"\n  gpu_large:\n    cpu: \"16\"\n    memory: \"64Gi\"\n    gpu: \"nvidia-tesla-v100\"\n    gpu_count: 4\n</code></pre> <pre><code># Step 1: Preprocess data (CPU only)\nflowyml run preprocess.py --resources cpu_only\n\n# Step 2: Train model (GPU required)\nflowyml run train.py --resources gpu_large\n</code></pre> <p>Why this pattern works: Optimize costs \u2014 only pay for GPUs when you actually need them.</p>"},{"location":"QUICK_REFERENCE/#configuration-patterns","title":"\ud83d\udcdc Configuration Patterns","text":""},{"location":"QUICK_REFERENCE/#when-to-use-each-docker-pattern","title":"When to Use Each Docker Pattern","text":"Pattern Use When Example Existing Dockerfile You have custom setup needs <code>dockerfile: ./Dockerfile</code> Poetry You use Poetry for dependency management <code>use_poetry: true</code> requirements.txt Simple dependencies, widely compatible <code>requirements_file: requirements.txt</code> Dynamic build Quick prototypes, minimal config <code>base_image: python:3.11-slim</code>"},{"location":"QUICK_REFERENCE/#docker-use-existing-dockerfile","title":"Docker: Use Existing Dockerfile","text":"<pre><code>docker:\n  dockerfile: ./Dockerfile\n  build_context: .\n</code></pre> <p>Use when: You have specialized dependencies, custom binary installs, or complex build steps.</p>"},{"location":"QUICK_REFERENCE/#docker-use-poetry","title":"Docker: Use Poetry","text":"<pre><code>docker:\n  use_poetry: true\n</code></pre> <p>Use when: Managing dependencies with Poetry and want deterministic builds.</p>"},{"location":"QUICK_REFERENCE/#docker-use-requirementstxt","title":"Docker: Use Requirements.txt","text":"<pre><code>docker:\n  requirements_file: requirements.txt\n</code></pre> <p>Use when: Simple, traditional Python projects with pip.</p>"},{"location":"QUICK_REFERENCE/#docker-dynamic-build","title":"Docker: Dynamic Build","text":"<pre><code>docker:\n  base_image: python:3.11-slim\n  requirements:\n    - tensorflow&gt;=2.12.0\n    - pandas&gt;=2.0.0\n  env_vars:\n    PYTHONUNBUFFERED: \"1\"\n</code></pre> <p>Use when: Rapid prototyping or simple dependencies that don't need a Dockerfile.</p>"},{"location":"QUICK_REFERENCE/#troubleshooting-quick-fixes","title":"\ud83d\udee0\ufe0f Troubleshooting Quick Fixes","text":""},{"location":"QUICK_REFERENCE/#stack-not-found_1","title":"\"Stack not found\"","text":"<pre><code># Check what stacks you have\nflowyml stack list\n\n# Verify config file exists and is valid\ncat flowyml.yaml\n\n# Set a default stack if none is set\nflowyml stack set-default local\n</code></pre>"},{"location":"QUICK_REFERENCE/#missing-dependencies_1","title":"\"Missing dependencies\"","text":"<pre><code># Install required extras for GCP\npip install \"flowyml[gcp]\"\n\n# Or install everything\npip install \"flowyml[all]\"\n</code></pre>"},{"location":"QUICK_REFERENCE/#permission-denied-on-gcp","title":"\"Permission denied\" on GCP","text":"<pre><code># Authenticate with Google Cloud\ngcloud auth login\ngcloud auth application-default login\ngcloud config set project YOUR-PROJECT-ID\n\n# Verify authentication\ngcloud auth list\n</code></pre>"},{"location":"QUICK_REFERENCE/#pipeline-runs-locally-but-fails-on-cloud","title":"\"Pipeline runs locally but fails on cloud\"","text":"<p>Common causes: 1. Missing dependencies in Docker image 2. Incorrect file paths (use cloud URIs like <code>gs://</code> not local paths) 3. Authentication not configured</p> <p>Solution pattern: <pre><code># Test with dry-run first\nflowyml run pipeline.py --stack production --dry-run\n\n# Check the generated Docker image\ndocker images | grep flowyml\n\n# Test the Docker image locally\ndocker run -it &lt;image-id&gt; /bin/bash\n</code></pre></p>"},{"location":"QUICK_REFERENCE/#pro-tips","title":"\u2705 Pro Tips","text":"<ol> <li>Use <code>flowyml init</code> to start new projects \u2014 Creates proper structure</li> <li>Store <code>flowyml.yaml</code> in version control \u2014 Reproducible deployments</li> <li>Use <code>.env</code> for secrets \u2014 Never commit credentials!</li> <li>Define resource presets for common workloads \u2014 Consistency across team</li> <li>Use <code>--dry-run</code> to verify configuration \u2014 Catch errors before expensive runs</li> <li>Keep pipeline code infrastructure-free \u2014 Business logic separate from deployment</li> <li>Use environment variables for dynamic values \u2014 One config, many environments</li> <li>Pin dependencies in production \u2014 Avoid surprise breakages</li> <li>Test stack configs locally first \u2014 Use local stack with production patterns</li> <li>Name stacks by purpose, not just environment \u2014 <code>us-prod-gpu</code> &gt; <code>prod2</code></li> </ol>"},{"location":"QUICK_REFERENCE/#learn-more","title":"\ud83d\udcda Learn More","text":"<ul> <li>Full CLI reference: CLI Documentation</li> <li>Stack configuration guide: Architecture: Stacks</li> <li>Production deployment: Deployment Guide</li> <li>Complete examples: Examples</li> </ul> <p>[!TIP] Bookmark this page! Use it as your go-to reference when you need quick command syntax.</p>"},{"location":"advanced_features/","title":"Advanced Features Guide","text":""},{"location":"advanced_features/#step-grouping","title":"\u26a1 Step Grouping","text":"<p>Group consecutive pipeline steps to execute in the same container/environment, reducing overhead while maintaining clear step boundaries.</p>"},{"location":"advanced_features/#basic-usage","title":"Basic Usage","text":"<pre><code>from flowyml import Pipeline, step\nfrom flowyml.core.resources import ResourceRequirements, GPUConfig\n\n# Group preprocessing steps\n@step(outputs=[\"raw_data\"], execution_group=\"preprocessing\")\ndef load_data():\n    return fetch_from_source()\n\n@step(inputs=[\"raw_data\"], outputs=[\"clean_data\"], execution_group=\"preprocessing\")\ndef clean_data(raw_data):\n    return preprocess(raw_data)\n\n@step(inputs=[\"clean_data\"], outputs=[\"features\"], execution_group=\"preprocessing\")\ndef extract_features(clean_data):\n    return transform(clean_data)\n\npipeline = Pipeline(\"data_pipeline\")\npipeline.add_step(load_data)\npipeline.add_step(clean_data)\npipeline.add_step(extract_features)\n\nresult = pipeline.run()\n# All three steps run in the same container\n</code></pre>"},{"location":"advanced_features/#resource-aggregation","title":"Resource Aggregation","text":"<p>When steps are grouped, flowyml automatically aggregates their resource requirements:</p> <pre><code>@step(\n    outputs=[\"data\"],\n    execution_group=\"training\",\n    resources=ResourceRequirements(cpu=\"2\", memory=\"4Gi\")\n)\ndef prepare_data():\n    return \"data\"\n\n@step(\n    inputs=[\"data\"],\n    outputs=[\"model\"],\n    execution_group=\"training\",\n    resources=ResourceRequirements(\n        cpu=\"8\",\n        memory=\"16Gi\",\n        gpu=GPUConfig(gpu_type=\"nvidia-a100\", count=2)\n    )\n)\ndef train_model(data):\n    return \"model\"\n\n# Group executes with: cpu=\"8\", memory=\"16Gi\", gpu=2x A100 (max of all)\n</code></pre> <p>Aggregation Strategy: - CPU/Memory: Maximum across all steps - GPU: Maximum count, best GPU type (A100 &gt; V100 &gt; T4) - Storage: Maximum across all steps</p>"},{"location":"advanced_features/#smart-sequential-analysis","title":"Smart Sequential Analysis","text":"<p>flowyml only groups steps that can execute consecutively in the DAG:</p> <pre><code># Consecutive: A \u2192 B \u2192 C (all in \"group1\")\n# Result: Single group with all three steps \u2705\n\n# Non-consecutive: A (\"group1\") \u2192 X (no group) \u2192 B (\"group1\")\n# Result: A and B run separately (not consecutive) \u2705\n</code></pre>"},{"location":"advanced_features/#inspection","title":"Inspection","text":"<p>After building, inspect created groups:</p> <pre><code>pipeline.build()\n\nfor group in pipeline.step_groups:\n    print(f\"Group: {group.group_name}\")\n    print(f\"  Steps: {[s.name for s in group.steps]}\")\n    print(f\"  Resources: CPU={group.aggregated_resources.cpu}\")\n</code></pre> <p>See Step Grouping Guide for complete documentation.</p>"},{"location":"advanced_features/#genai-llm-monitoring","title":"\ud83e\udd16 GenAI &amp; LLM Monitoring","text":""},{"location":"advanced_features/#llm-call-tracing","title":"LLM Call Tracing","text":"<p>flowyml provides built-in tracing for LLM calls, allowing you to monitor token usage, costs, and execution traces.</p>"},{"location":"advanced_features/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from flowyml import trace_llm\n\n@trace_llm(name=\"text_generation\")\ndef generate_text(prompt: str):\n    response = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return response.choices[0].message.content\n\n# Automatically traced\nresult = generate_text(\"Write a haiku about ML\")\n</code></pre>"},{"location":"advanced_features/#nested-traces","title":"Nested Traces","text":"<pre><code>from flowyml import trace_llm, tracer\n\n@trace_llm(name=\"rag_pipeline\", event_type=\"chain\")\ndef rag_pipeline(query: str):\n    # Child trace 1\n    context = retrieve_context(query)\n    # Child trace 2\n    answer = generate_answer(query, context)\n    return answer\n\n@trace_llm(name=\"retrieval\", event_type=\"tool\")\ndef retrieve_context(query: str):\n    # Your retrieval logic\n    return context\n\n@trace_llm(name=\"generation\", event_type=\"llm\")\ndef generate_answer(query: str, context: str):\n    # Your generation logic\n    return answer\n</code></pre>"},{"location":"advanced_features/#viewing-traces","title":"Viewing Traces","text":"<p>Traces are automatically saved to the metadata store and can be viewed via:</p> <ol> <li> <p>Python API: <pre><code>from flowyml.storage.metadata import SQLiteMetadataStore\n\nstore = SQLiteMetadataStore()\ntrace_events = store.get_trace(trace_id=\"...\")\n</code></pre></p> </li> <li> <p>Web UI: Navigate to <code>http://localhost:8080/api/traces</code> to see all traces.</p> </li> </ol>"},{"location":"advanced_features/#notification-system","title":"\ud83d\udd14 Notification System","text":""},{"location":"advanced_features/#setup-notifications","title":"Setup Notifications","text":"<pre><code>from flowyml import configure_notifications\n\nconfigure_notifications(\n    console=True,\n    slack_webhook=\"https://hooks.slack.com/services/YOUR/WEBHOOK/URL\",\n    email_config={\n        'smtp_host': 'smtp.gmail.com',\n        'smtp_port': 587,\n        'username': 'your-email@gmail.com',\n        'password': 'your-app-password',\n        'from_addr': 'your-email@gmail.com',\n        'to_addrs': ['team@company.com']\n    }\n)\n</code></pre>"},{"location":"advanced_features/#using-notifications","title":"Using Notifications","text":"<pre><code>from flowyml import get_notifier\n\nnotifier = get_notifier()\n\n# Manual notifications\nnotifier.notify(\n    title=\"Model Training Complete\",\n    message=\"Accuracy: 95.2%\",\n    level=\"success\"  # 'info', 'warning', 'error', 'success'\n)\n\n# Event-based notifications\nnotifier.on_pipeline_start(\"training_pipeline\", run_id)\nnotifier.on_pipeline_success(\"training_pipeline\", run_id, duration=120.5)\nnotifier.on_pipeline_failure(\"training_pipeline\", run_id, error=\"Out of memory\")\nnotifier.on_drift_detected(\"user_age\", psi=0.25)\n</code></pre>"},{"location":"advanced_features/#custom-notification-channels","title":"Custom Notification Channels","text":"<pre><code>from flowyml import NotificationManager, NotificationChannel, Notification\n\nclass CustomNotifier(NotificationChannel):\n    def send(self, notification: Notification) -&gt; bool:\n        # Your custom logic (e.g., Discord, Teams, PagerDuty)\n        return True\n\nmanager = NotificationManager()\nmanager.add_channel(CustomNotifier())\n</code></pre>"},{"location":"advanced_features/#pipeline-scheduling","title":"\ud83d\udcc5 Pipeline Scheduling","text":""},{"location":"advanced_features/#schedule-pipelines","title":"Schedule Pipelines","text":"<pre><code>from flowyml import PipelineScheduler, Pipeline\n\nscheduler = PipelineScheduler()\n\n# Create your pipeline\npipeline = Pipeline(\"daily_training\")\n# ... add steps ...\n\n# Schedule daily at 2:00 AM\nscheduler.schedule_daily(\n    name=\"daily_training\",\n    pipeline_func=lambda: pipeline.run(),\n    hour=2,\n    minute=0\n)\n\n# Schedule every 6 hours\nscheduler.schedule_interval(\n    name=\"data_refresh\",\n    pipeline_func=lambda: refresh_pipeline.run(),\n    hours=6\n)\n\n# Schedule hourly at :15\nscheduler.schedule_hourly(\n    name=\"hourly_check\",\n    pipeline_func=lambda: check_pipeline.run(),\n    minute=15\n)\n\n# Start the scheduler\nscheduler.start()  # Non-blocking (daemon thread)\n\n# Or run blocking\n# scheduler.start(blocking=True)\n</code></pre>"},{"location":"advanced_features/#managing-schedules","title":"Managing Schedules","text":"<pre><code># List all schedules\nscheduler.list_schedules()\n\n# Disable a schedule\nscheduler.disable(\"daily_training\")\n\n# Enable a schedule\nscheduler.enable(\"daily_training\")\n\n# Remove a schedule\nscheduler.unschedule(\"daily_training\")\n\n# Stop the scheduler\nscheduler.stop()\n</code></pre>"},{"location":"advanced_features/#model-leaderboard","title":"\ud83c\udfc6 Model Leaderboard","text":""},{"location":"advanced_features/#track-model-performance","title":"Track Model Performance","text":"<pre><code>from flowyml import ModelLeaderboard\n\n# Create leaderboard for a metric\nleaderboard = ModelLeaderboard(\n    metric=\"accuracy\",\n    higher_is_better=True\n)\n\n# Add model scores\nleaderboard.add_score(\n    model_name=\"bert-base-uncased\",\n    run_id=\"run_123\",\n    score=0.92,\n    metadata={\"epochs\": 10, \"learning_rate\": 0.001}\n)\n\nleaderboard.add_score(\n    model_name=\"distilbert-base\",\n    run_id=\"run_124\",\n    score=0.89\n)\n\n# Display rankings\nleaderboard.display(n=10)\n</code></pre> <p>Output: <pre><code>\ud83c\udfc6 Model Leaderboard - accuracy\n================================================================================\nRank   Model Name                     Score           Run ID\n--------------------------------------------------------------------------------\n1      bert-base-uncased              0.9200          run_123\n2      distilbert-base                0.8900          run_124\n================================================================================\n</code></pre></p>"},{"location":"advanced_features/#compare-runs","title":"Compare Runs","text":"<pre><code>from flowyml import compare_runs\n\ncomparison = compare_runs(\n    run_ids=[\"run_123\", \"run_124\", \"run_125\"],\n    metrics=[\"accuracy\", \"f1_score\", \"precision\", \"recall\"]\n)\n\nprint(comparison)\n</code></pre>"},{"location":"advanced_features/#pipeline-templates","title":"\ud83d\udce6 Pipeline Templates","text":""},{"location":"advanced_features/#using-templates","title":"Using Templates","text":"<pre><code>from flowyml import create_from_template, list_templates\n\n# See available templates\ntemplates = list_templates()\nprint(templates)  # ['ml_training', 'etl', 'data_pipeline', 'ab_test']\n\n# Create ML training pipeline\npipeline = create_from_template(\n    'ml_training',\n    name='my_training_pipeline',\n    data_loader=load_data,\n    preprocessor=preprocess_data,\n    trainer=train_model,\n    evaluator=evaluate_model,\n    model_saver=save_model\n)\n\nresult = pipeline.run()\n</code></pre>"},{"location":"advanced_features/#available-templates","title":"Available Templates","text":""},{"location":"advanced_features/#1-ml-training-template","title":"1. ML Training Template","text":"<pre><code>pipeline = create_from_template(\n    'ml_training',\n    data_loader=lambda: load_dataset(),\n    preprocessor=lambda dataset: preprocess(dataset),\n    trainer=lambda processed_data: train(processed_data),\n    evaluator=lambda model, data: evaluate(model, data),\n    model_saver=lambda model: save(model)\n)\n</code></pre>"},{"location":"advanced_features/#2-etl-template","title":"2. ETL Template","text":"<pre><code>pipeline = create_from_template(\n    'etl',\n    extractor=lambda: extract_from_db(),\n    transformer=lambda raw_data: transform(raw_data),\n    loader=lambda transformed_data: load_to_warehouse(transformed_data)\n)\n</code></pre>"},{"location":"advanced_features/#3-ab-test-template","title":"3. A/B Test Template","text":"<pre><code>pipeline = create_from_template(\n    'ab_test',\n    data_loader=lambda: load_test_data(),\n    model_a_trainer=lambda data: train_variant_a(data),\n    model_b_trainer=lambda data: train_variant_b(data),\n    comparator=lambda metrics_a, metrics_b: compare(metrics_a, metrics_b)\n)\n</code></pre>"},{"location":"advanced_features/#checkpointing","title":"\ud83d\udcbe Checkpointing","text":""},{"location":"advanced_features/#enable-checkpointing","title":"Enable Checkpointing","text":"<pre><code>from flowyml import PipelineCheckpoint\n\ncheckpoint = PipelineCheckpoint(run_id=\"my_run_123\")\n\n# Save state after expensive computation\n@step(outputs=[\"processed_data\"])\ndef expensive_preprocessing():\n    data = do_expensive_work()\n    checkpoint.save_step_state(\"preprocessing\", data)\n    return data\n\n# Check if checkpoint exists\nif checkpoint.exists():\n    resume_point = checkpoint.resume_point()\n    print(f\"Can resume from: {resume_point}\")\n\n    # Load previous state\n    state = checkpoint.load_step_state(\"preprocessing\")\n</code></pre>"},{"location":"advanced_features/#wrapper-for-automatic-checkpointing","title":"Wrapper for Automatic Checkpointing","text":"<pre><code>from flowyml import checkpoint_enabled_pipeline\n\npipeline = Pipeline(\"training\")\n# ... add steps ...\n\n# Enable checkpointing\npipeline = checkpoint_enabled_pipeline(pipeline, run_id=\"run_123\")\n\n# Run will now prompt to resume if checkpoint exists\nresult = pipeline.run()\n</code></pre>"},{"location":"advanced_features/#human-in-the-loop","title":"\ud83d\udc64 Human-in-the-Loop","text":""},{"location":"advanced_features/#approval-steps","title":"Approval Steps","text":"<pre><code>from flowyml import Pipeline, step, approval\n\npipeline = Pipeline(\"deployment\")\n\n@step(outputs=[\"model\"])\ndef train_model():\n    # Training logic\n    return model\n\n# Add approval gate\napproval_step = approval(\n    name=\"approve_deployment\",\n    approver=\"ml-team\",\n    timeout_seconds=3600,\n    auto_approve_if=lambda: os.getenv(\"CI\") == \"true\"\n)\n\npipeline.add_step(train_model)\npipeline.add_step(approval_step)\n\n@step(inputs=[\"model\"])\ndef deploy_model(model):\n    # Deployment logic\n    pass\n\npipeline.add_step(deploy_model)\n</code></pre> <p>When run interactively: <pre><code>\u270b Step 'approve_deployment' requires approval.\n   Waiting for approval from: ml-team\n   Timeout: 3600s\n   Approve execution? [y/N]: y\n\u2705 Approved.\n</code></pre></p>"},{"location":"advanced_features/#keras-integration","title":"\ud83e\uddea Keras Integration","text":""},{"location":"advanced_features/#automatic-experiment-tracking","title":"Automatic Experiment Tracking","text":"<pre><code>from flowyml import flowymlKerasCallback\nimport tensorflow as tf\n\nmodel = tf.keras.Sequential([...])\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n\n# Add flowyml callback\nmodel.fit(\n    x_train, y_train,\n    epochs=10,\n    validation_data=(x_val, y_val),\n    callbacks=[\n        flowymlKerasCallback(\n            experiment_name=\"mnist_classification\",\n            run_name=\"baseline_v1\",\n            log_model=True,\n            log_every_epoch=True\n        )\n    ]\n)\n</code></pre> <p>Automatically logs: - Training and validation metrics - Model architecture - Optimizer configuration - Training parameters (epochs, batch size) - Model checkpoints (if <code>log_model=True</code>)</p>"},{"location":"advanced_features/#data-drift-detection","title":"\ud83d\udcca Data Drift Detection","text":""},{"location":"advanced_features/#monitor-distribution-shifts","title":"Monitor Distribution Shifts","text":"<pre><code>from flowyml import detect_drift, compute_stats\n\n# Training data (reference)\ntrain_feature = train_df['age'].values\n\n# Production data (current)\nprod_feature = prod_df['age'].values\n\n# Detect drift\ndrift_result = detect_drift(\n    reference_data=train_feature,\n    current_data=prod_feature,\n    threshold=0.1  # PSI threshold\n)\n\nif drift_result['drift_detected']:\n    print(f\"\u26a0\ufe0f Drift detected!\")\n    print(f\"PSI: {drift_result['psi']:.4f}\")\n    print(f\"Reference stats: {drift_result['reference_stats']}\")\n    print(f\"Current stats: {drift_result['current_stats']}\")\n\n    # Send notification\n    from flowyml import get_notifier\n    notifier = get_notifier()\n    notifier.on_drift_detected('age', drift_result['psi'])\n</code></pre>"},{"location":"advanced_features/#compute-statistics","title":"Compute Statistics","text":"<pre><code>from flowyml import compute_stats\n\nstats = compute_stats(data_array)\nprint(stats)\n</code></pre> <p>Output: <pre><code>{\n    'count': 10000,\n    'mean': 35.2,\n    'std': 12.5,\n    'min': 18.0,\n    'max': 85.0,\n    'median': 34.0,\n    'q25': 26.0,\n    'q75': 45.0\n}\n</code></pre></p>"},{"location":"advanced_features/#integration-examples","title":"\ud83d\udd17 Integration Examples","text":""},{"location":"advanced_features/#complete-example","title":"Complete Example","text":"<pre><code>from flowyml import (\n    Pipeline, step, context,\n    configure_notifications,\n    PipelineScheduler,\n    ModelLeaderboard,\n    detect_drift,\n    get_notifier\n)\n\n# 1. Setup notifications\nconfigure_notifications(console=True, slack_webhook=\"...\")\n\n# 2. Create pipeline\nctx = context(model_type=\"random_forest\", n_estimators=100)\npipeline = Pipeline(\"production_training\", context=ctx)\n\n@step(outputs=[\"train_data\", \"val_data\"])\ndef load_and_validate():\n    train = load_training_data()\n    val = load_validation_data()\n\n    # Check for drift\n    drift = detect_drift(historical_data, train)\n    if drift['drift_detected']:\n        get_notifier().on_drift_detected('features', drift['psi'])\n\n    return train, val\n\n@step(inputs=[\"train_data\"], outputs=[\"model\"])\ndef train_model(train_data, model_type: str, n_estimators: int):\n    model = RandomForestClassifier(n_estimators=n_estimators)\n    model.fit(train_data.X, train_data.y)\n    return model\n\n@step(inputs=[\"model\", \"val_data\"], outputs=[\"metrics\"])\ndef evaluate(model, val_data):\n    predictions = model.predict(val_data.X)\n    accuracy = accuracy_score(val_data.y, predictions)\n    return {\"accuracy\": accuracy}\n\npipeline.add_step(load_and_validate)\npipeline.add_step(train_model)\npipeline.add_step(evaluate)\n\n# 3. Schedule daily training\nscheduler = PipelineScheduler()\nscheduler.schedule_daily(\n    name=\"daily_retrain\",\n    pipeline_func=lambda: pipeline.run(),\n    hour=2\n)\nscheduler.start()\n\n# 4. Track on leaderboard\nleaderboard = ModelLeaderboard(\"accuracy\")\n# After each run, add to leaderboard\n</code></pre> <p>For more examples, see the <code>/examples</code> directory in the repository.</p>"},{"location":"architecture/","title":"Architecture \ud83c\udfd7\ufe0f","text":"<p>flowyml is designed as a modular, layered system that separates pipeline definition, execution, storage, and visualization.</p>"},{"location":"architecture/#high-level-architecture","title":"High-Level Architecture \ud83d\uddfa\ufe0f","text":"<pre><code>graph TD\n    User[User Code] --&gt; API[Core API]\n    API --&gt; Compiler[DAG Compiler]\n    Compiler --&gt; Executor[Executor Engine]\n\n    Executor --&gt; Local[Local Runner]\n    Executor --&gt; Dist[Distributed Runner]\n\n    Local --&gt; Cache[Cache Store]\n    Local --&gt; Artifacts[Artifact Store]\n    Local --&gt; Metadata[Metadata Store]\n\n    Metadata --&gt; UI[UI Backend]\n    UI --&gt; Frontend[React Dashboard]\n</code></pre>"},{"location":"architecture/#core-components","title":"Core Components \ud83e\udde9","text":""},{"location":"architecture/#1-pipeline-definition-layer","title":"1. Pipeline Definition Layer","text":"<ul> <li>Pipeline: The container for steps and configuration.</li> <li>Step: A unit of work, wrapped by the <code>@step</code> decorator.</li> <li>Context: Manages parameters and runtime state.</li> </ul>"},{"location":"architecture/#2-execution-engine","title":"2. Execution Engine","text":"<ul> <li>DAG: Directed Acyclic Graph representing dependencies.</li> <li>Executor: Handles the execution of the DAG.<ul> <li><code>LocalExecutor</code>: Runs steps in the current process/thread.</li> <li><code>DistributedExecutor</code>: (Planned) Runs steps on remote workers.</li> </ul> </li> <li>Cache: Intercepts execution to return stored results if inputs haven't changed.</li> </ul>"},{"location":"architecture/#3-storage-layer","title":"3. Storage Layer","text":"<ul> <li>Artifact Store: Stores large binary objects (datasets, models).<ul> <li>Local Filesystem</li> <li>S3 / GCS / Azure Blob (via <code>fsspec</code>)</li> </ul> </li> <li>Metadata Store: Stores lightweight metadata about runs, steps, and artifacts.<ul> <li>SQLite (default)</li> <li>PostgreSQL / MySQL</li> </ul> </li> </ul>"},{"location":"architecture/#4-ui-architecture","title":"4. UI Architecture","text":"<p>The UI follows a decoupled client-server architecture.</p>"},{"location":"architecture/#backend-fastapi","title":"Backend (FastAPI)","text":"<ul> <li>API Endpoints: REST API for pipelines, runs, and assets.</li> <li>Static File Serving: Serves the compiled React frontend.</li> <li>State Management: Reads from the Metadata Store.</li> </ul>"},{"location":"architecture/#frontend-react-vite","title":"Frontend (React + Vite)","text":"<ul> <li>SPA: Single Page Application.</li> <li>Visualization: Uses <code>reactflow</code> for DAG visualization.</li> <li>Real-time: Polls backend for updates (WebSocket planned).</li> </ul>"},{"location":"architecture/#data-flow","title":"Data Flow \ud83d\udd04","text":"<ol> <li>Definition: User defines <code>@pipeline</code> and <code>@step</code> functions.</li> <li>Compilation: When <code>pipeline()</code> is called, flowyml builds a DAG based on data dependencies.</li> <li>Execution:<ul> <li>Executor traverses the DAG topologically.</li> <li>For each step:<ul> <li>Check Cache: If valid cache exists, skip execution and return result.</li> <li>Execute: Run the user function with injected context.</li> <li>Store: Save output artifacts and metadata.</li> </ul> </li> </ul> </li> <li>Monitoring:<ul> <li>Metadata is written to SQLite.</li> <li>UI Backend reads SQLite and serves data to Frontend.</li> </ul> </li> </ol>"},{"location":"architecture/#design-principles","title":"Design Principles \ud83d\udca1","text":"<ul> <li>Zero Config: Works out of the box with sensible defaults.</li> <li>Asset-Centric: Focus on the data (artifacts) produced, not just the tasks.</li> <li>Framework Agnostic: Works with PyTorch, TensorFlow, sklearn, or raw Python.</li> <li>Progressive Disclosure: Simple for beginners, powerful for experts.</li> </ul>"},{"location":"contributing/","title":"Contributing to flowyml \ud83e\udd1d","text":"<p>We welcome contributions to flowyml! Whether it's reporting a bug, improving documentation, or adding a new feature, your help is appreciated.</p>"},{"location":"contributing/#development-setup","title":"Development Setup \ud83d\udee0\ufe0f","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> <li>Node.js 16+ (for UI development)</li> <li>Poetry (recommended) or pip</li> </ul>"},{"location":"contributing/#setting-up-the-environment","title":"Setting up the Environment","text":"<ol> <li> <p>Clone the repository:     <pre><code>git clone https://github.com/flowyml-ai/flowyml.git\ncd flowyml\n</code></pre></p> </li> <li> <p>Install dependencies:     <pre><code># Using pip\npip install -e \".[dev,ui]\"\n\n# Using poetry\npoetry install --with dev,ui\n</code></pre></p> </li> <li> <p>Install pre-commit hooks:     <pre><code>pre-commit install\n</code></pre></p> </li> </ol>"},{"location":"contributing/#ui-development","title":"UI Development \ud83d\udda5\ufe0f","text":"<p>The UI consists of a FastAPI backend and a React frontend.</p>"},{"location":"contributing/#running-in-development-mode","title":"Running in Development Mode","text":"<ol> <li> <p>Start the Backend:     <pre><code># In one terminal\nflowyml ui start --dev\n</code></pre>     This starts the FastAPI server on port 8000 with auto-reload.</p> </li> <li> <p>Start the Frontend:     <pre><code># In another terminal\ncd flowyml/ui/frontend\nnpm install\nnpm run dev\n</code></pre>     This starts the Vite dev server on port 5173 with Hot Module Replacement (HMR).</p> </li> </ol> <p>The frontend proxies API requests to the backend at <code>http://localhost:8000</code>.</p>"},{"location":"contributing/#building-for-production","title":"Building for Production \ud83d\udce6","text":"<p>To build the frontend for production distribution:</p> <pre><code>cd flowyml/ui/frontend\nnpm run build\n</code></pre> <p>This generates static assets in <code>flowyml/ui/frontend/dist</code>, which are served by the Python backend in production mode.</p>"},{"location":"contributing/#testing","title":"Testing \ud83e\uddea","text":""},{"location":"contributing/#running-tests","title":"Running Tests","text":"<p>Run the full test suite:</p> <pre><code>pytest\n</code></pre> <p>Run specific tests:</p> <pre><code>pytest tests/test_core.py\n</code></pre>"},{"location":"contributing/#writing-tests","title":"Writing Tests","text":"<ul> <li>Place unit tests in the <code>tests/</code> directory.</li> <li>Use the <code>BaseTestCase</code> class for tests that require a temporary directory or isolated configuration.</li> <li>Ensure all new features have accompanying tests.</li> </ul>"},{"location":"contributing/#code-style","title":"Code Style \ud83c\udfa8","text":"<p>We follow PEP 8 and use <code>black</code> for formatting.</p> <pre><code># Format code\nblack flowyml tests\n\n# Check style\nflake8 flowyml tests\n</code></pre>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process \ud83d\udd00","text":"<ol> <li>Fork the repository.</li> <li>Create a feature branch (<code>git checkout -b feature/amazing-feature</code>).</li> <li>Commit your changes.</li> <li>Push to the branch.</li> <li>Open a Pull Request.</li> </ol>"},{"location":"contributing/#documentation","title":"Documentation \ud83d\udcdd","text":"<p>Documentation is built with MkDocs.</p> <pre><code># Serve documentation locally\nmkdocs serve\n</code></pre> <p>Update documentation in the <code>docs/</code> directory for any API changes.</p>"},{"location":"deployment/","title":"Deployment Guide","text":"<p>flowyml is designed to be flexible, supporting both local execution and centralized deployment for teams. This guide covers how to deploy flowyml as a centralized hub using Docker.</p>"},{"location":"deployment/#centralized-hub-deployment","title":"Centralized Hub Deployment","text":"<p>A centralized hub allows your team to share pipelines, runs, and artifacts. It consists of the flowyml backend (API &amp; Orchestrator) and the frontend (UI).</p>"},{"location":"deployment/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker</li> <li>Docker Compose</li> </ul>"},{"location":"deployment/#quick-start","title":"Quick Start","text":"<ol> <li> <p>Clone the repository (or use your fork):    <pre><code>git clone https://github.com/unicolab/flowyml.git\ncd flowyml\n</code></pre></p> </li> <li> <p>Start the services:    <pre><code>docker-compose up -d\n</code></pre></p> </li> </ol> <p>This will start:    - Backend on port <code>8000</code>    - Frontend on port <code>8080</code></p> <ol> <li>Access the UI:    Open your browser and navigate to <code>http://localhost:8080</code>.</li> </ol>"},{"location":"deployment/#configuration","title":"Configuration","text":"<p>You can configure the deployment by setting environment variables in <code>docker-compose.yml</code> or a <code>.env</code> file.</p> Variable Description Default <code>flowyml_HOME</code> Path to flowyml data directory <code>/root/.flowyml</code> <code>flowyml_UI_HOST</code> Host to bind the backend to <code>0.0.0.0</code> <code>flowyml_UI_PORT</code> Port for the backend API <code>8000</code>"},{"location":"deployment/#data-persistence","title":"Data Persistence","text":"<p>The <code>docker-compose.yml</code> mounts a volume for data persistence: <pre><code>volumes:\n  - ./.flowyml:/root/.flowyml\n</code></pre> This ensures that your metadata database, artifacts, and logs are preserved across restarts.</p>"},{"location":"deployment/#client-configuration","title":"Client Configuration","text":"<p>To connect your local flowyml CLI or UI to the centralized hub, you need to configure the execution mode.</p>"},{"location":"deployment/#using-the-cli","title":"Using the CLI","text":"<ol> <li> <p>Set the execution mode to remote:    <pre><code>flowyml config set-mode remote\n</code></pre></p> </li> <li> <p>Set the remote server URL:    <pre><code>flowyml config set-url --server http://&lt;hub-ip&gt;:8000 --ui http://&lt;hub-ip&gt;:8080\n</code></pre>    Replace <code>&lt;hub-ip&gt;</code> with the IP address or hostname of your centralized hub.</p> </li> <li> <p>Verify configuration:    <pre><code>flowyml config show\n</code></pre></p> </li> </ol>"},{"location":"deployment/#using-environment-variables","title":"Using Environment Variables","text":"<p>You can also configure the client using environment variables:</p> <pre><code>export flowyml_EXECUTION_MODE=remote\nexport flowyml_REMOTE_SERVER_URL=http://&lt;hub-ip&gt;:8000\nexport flowyml_REMOTE_UI_URL=http://&lt;hub-ip&gt;:8080\n</code></pre>"},{"location":"deployment/#production-considerations","title":"Production Considerations","text":"<ul> <li>Security: The default setup does not include authentication. For production use, ensure the hub is deployed within a private network (VPN/VPC) or behind a secure proxy with authentication (e.g., Nginx with Basic Auth, OAuth2 Proxy).</li> <li>Storage: For heavy workloads, consider using an external database (PostgreSQL) and object storage (S3) instead of the default SQLite and local file system. (See Advanced Configuration).</li> <li>Scalability: The default backend runs a single worker. For high concurrency, you may need to scale the backend service or use a more robust task queue (e.g., Celery/Redis).</li> </ul>"},{"location":"examples/","title":"Examples \ud83d\udcda","text":"<p>This page provides comprehensive working examples showcasing flowyml's features. All examples are available in the <code>examples/</code> directory.</p>"},{"location":"examples/#quick-start-examples","title":"Quick Start Examples","text":""},{"location":"examples/#simple-pipeline","title":"Simple Pipeline","text":"<p>A minimal pipeline demonstrating the basics:</p> <pre><code>from flowyml import Pipeline, step\n\n@step(outputs=[\"numbers\"])\ndef generate_numbers():\n    return list(range(10))\n\n@step(inputs=[\"numbers\"], outputs=[\"doubled\"])\ndef double_numbers(numbers):\n    return [x * 2 for x in numbers]\n\n# Create and run pipeline\npipeline = Pipeline(\"simple_example\")\npipeline.add_step(generate_numbers)\npipeline.add_step(double_numbers)\n\nresult = pipeline.run()\nprint(f\"Result: {result.outputs['doubled']}\")\n</code></pre> <p>\ud83d\udcc4 Full example: <code>examples/simple_pipeline.py</code></p>"},{"location":"examples/#etl-pipeline","title":"ETL Pipeline","text":"<p>Extract, Transform, Load pattern with caching:</p> <pre><code>from flowyml import Pipeline, step, context\nimport pandas as pd\n\n@step(outputs=[\"raw_data\"], cache=\"code_hash\")\ndef extract():\n    \"\"\"Extract data from source.\"\"\"\n    return pd.read_csv(\"data/source.csv\")\n\n@step(inputs=[\"raw_data\"], outputs=[\"clean_data\"], cache=\"input_hash\")\ndef transform(raw_data):\n    \"\"\"Clean and transform data.\"\"\"\n    return raw_data.dropna().reset_index(drop=True)\n\n@step(inputs=[\"clean_data\"], cache=False)\ndef load(clean_data):\n    \"\"\"Load to destination.\"\"\"\n    clean_data.to_sql(\"processed_data\", engine)\n    return len(clean_data)\n\n# Run pipeline\npipeline = Pipeline(\"etl_pipeline\")\npipeline.add_step(extract)\npipeline.add_step(transform)\npipeline.add_step(load)\n\nresult = pipeline.run()\n</code></pre> <p>\ud83d\udcc4 Full example: <code>examples/clean_pipeline.py</code></p>"},{"location":"examples/#machine-learning-examples","title":"Machine Learning Examples","text":""},{"location":"examples/#training-pipeline-with-context","title":"Training Pipeline with Context","text":"<pre><code>from flowyml import Pipeline, step, context\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\nctx = context(\n    test_size=0.2,\n    random_state=42,\n    n_estimators=100,\n    max_depth=10\n)\n\n@step(outputs=[\"X_train\", \"X_test\", \"y_train\", \"y_test\"])\ndef split_data(data, labels, test_size: float, random_state: int):\n    \"\"\"Split data into train/test sets.\"\"\"\n    return train_test_split(\n        data, labels,\n        test_size=test_size,\n        random_state=random_state\n    )\n\n@step(inputs=[\"X_train\", \"y_train\"], outputs=[\"model\"])\ndef train_model(X_train, y_train, n_estimators: int, max_depth: int):\n    \"\"\"Train random forest model.\"\"\"\n    model = RandomForestClassifier(\n        n_estimators=n_estimators,\n        max_depth=max_depth\n    )\n    model.fit(X_train, y_train)\n    return model\n\n@step(inputs=[\"model\", \"X_test\", \"y_test\"], outputs=[\"metrics\"])\ndef evaluate(model, X_test, y_test):\n    \"\"\"Evaluate model performance.\"\"\"\n    score = model.score(X_test, y_test)\n    return {\"accuracy\": score}\n\n# Create ML pipeline\npipeline = Pipeline(\"ml_training\", context=ctx)\npipeline.add_step(split_data)\npipeline.add_step(train_model)\npipeline.add_step(evaluate)\n\nresult = pipeline.run()\nprint(f\"Model accuracy: {result.outputs['metrics']['accuracy']:.2%}\")\n</code></pre>"},{"location":"examples/#advanced-features","title":"Advanced Features","text":""},{"location":"examples/#caching-pipeline","title":"Caching Pipeline","text":"<p>Demonstrates intelligent caching strategies:</p> <pre><code>from flowyml import Pipeline, step\nimport time\n\n@step(cache=\"code_hash\")\ndef expensive_step():\n    \"\"\"Only re-runs if code changes.\"\"\"\n    time.sleep(2)\n    return \"computed_result\"\n\n@step(cache=\"input_hash\")\ndef dependent_step(data, param: int):\n    \"\"\"Re-runs if inputs or parameters change.\"\"\"\n    return f\"{data}_{param}\"\n\n@step(cache=False)\ndef always_fresh():\n    \"\"\"Always executes (e.g., real-time data).\"\"\"\n    return time.time()\n\npipeline = Pipeline(\"caching_demo\", enable_cache=True)\npipeline.add_step(expensive_step)\npipeline.add_step(dependent_step)\npipeline.add_step(always_fresh)\n\n# First run: all steps execute\nresult1 = pipeline.run()\n\n# Second run: expensive_step and dependent_step cached!\nresult2 = pipeline.run()\n</code></pre> <p>\ud83d\udcc4 Full example: <code>examples/caching_pipeline.py</code></p>"},{"location":"examples/#conditional-pipeline","title":"Conditional Pipeline","text":"<p>Use conditional execution based on runtime data:</p> <pre><code>from flowyml import Pipeline, step, when, unless\n\n@step(outputs=[\"data_quality\"])\ndef check_quality():\n    return {\"score\": 0.85, \"issues\": 12}\n\n@step(inputs=[\"data_quality\"], outputs=[\"model\"])\n@when(lambda quality: quality[\"score\"] &gt; 0.8)\ndef train_if_quality_good(data_quality):\n    \"\"\"Only trains if data quality is good.\"\"\"\n    return \"trained_model\"\n\n@step(inputs=[\"data_quality\"])\n@unless(lambda quality: quality[\"issues\"] == 0)\ndef log_issues(data_quality):\n    \"\"\"Logs issues if any exist.\"\"\"\n    print(f\"Found {data_quality['issues']} issues\")\n\npipeline = Pipeline(\"conditional_example\")\npipeline.add_step(check_quality)\npipeline.add_step(train_if_quality_good)\npipeline.add_step(log_issues)\n\nresult = pipeline.run()\n</code></pre> <p>\ud83d\udcc4 Full example: <code>examples/conditional_pipeline.py</code></p>"},{"location":"examples/#working-with-stacks","title":"Working with Stacks","text":""},{"location":"examples/#local-stack-example","title":"Local Stack Example","text":"<pre><code>from flowyml import Pipeline, step, LocalStack\n\n# Create a local stack with custom paths\nstack = LocalStack(\n    artifact_path=\"./my_artifacts\",\n    metadata_path=\"./my_metadata.db\"\n)\n\n@step(outputs=[\"data\"])\ndef process_data():\n    return {\"value\": 42}\n\n# Pipeline automatically materializes outputs to stack's artifact store\npipeline = Pipeline(\"stack_example\", stack=stack)\npipeline.add_step(process_data)\n\nresult = pipeline.run()\n# Artifacts saved to ./my_artifacts/project/date/run_id/data/process_data/\n</code></pre>"},{"location":"examples/#gcp-stack-example","title":"GCP Stack Example","text":"<pre><code>from flowyml import Pipeline, step\nfrom flowyml.stacks.gcp import GCPStack\n\n# Configure GCP stack\nstack = GCPStack(\n    project_id=\"my-ml-project\",\n    region=\"us-central1\",\n    artifact_store={\n        \"bucket\": \"ml-artifacts\",\n        \"prefix\": \"experiments\"\n    }\n)\n\n@step(outputs=[\"results\"], resources={\"gpu\": 1})\ndef train_on_gcp(data):\n    \"\"\"Runs on GCP with GPU.\"\"\"\n    return train_model(data)\n\npipeline = Pipeline(\"gcp_training\", stack=stack)\npipeline.add_step(train_on_gcp)\n\n# Runs on GCP Vertex AI with artifacts in GCS\nresult = pipeline.run()\n</code></pre> <p>\ud83d\udcc2 Full example: <code>examples/gcp_stack/</code></p>"},{"location":"examples/#ui-integration","title":"UI Integration","text":""},{"location":"examples/#real-time-monitoring","title":"Real-time Monitoring","text":"<pre><code>from flowyml import Pipeline, step\nimport time\n\n@step(outputs=[\"data\"])\ndef load_data():\n    time.sleep(2)  # Simulate work\n    return [1, 2, 3, 4, 5]\n\n@step(inputs=[\"data\"], outputs=[\"processed\"])\ndef process(data):\n    time.sleep(3)  # Simulate work\n    return [x * 2 for x in data]\n\n# Start UI first: flowyml ui start\npipeline = Pipeline(\"ui_demo\")\npipeline.add_step(load_data)\npipeline.add_step(process)\n\n# Run pipeline - watch live in UI at http://localhost:8080\nresult = pipeline.run(debug=True)\n</code></pre> <p>\ud83d\udcc4 Full example: <code>examples/simple_pipeline_ui.py</code></p>"},{"location":"examples/#complete-ui-integration","title":"Complete UI Integration","text":"<p>Full-featured example with metrics, artifacts, and visualization:</p> <p>\ud83d\udcc4 Full example: <code>examples/ui_integration_example.py</code></p>"},{"location":"examples/#custom-components","title":"Custom Components","text":""},{"location":"examples/#custom-executor","title":"Custom Executor","text":"<pre><code>from flowyml import Executor, ExecutionResult\n\nclass BatchExecutor(Executor):\n    \"\"\"Custom executor that batches operations.\"\"\"\n\n    def execute_step(self, step, inputs, context_params, cache_store=None):\n        # Custom execution logic\n        result = step.func(**inputs, **context_params)\n        return ExecutionResult(\n            step_name=step.name,\n            success=True,\n            output=result,\n            duration_seconds=0.0\n        )\n\n# Use custom executor\npipeline = Pipeline(\"custom\", executor=BatchExecutor())\n</code></pre> <p>\ud83d\udcc2 Full example: <code>examples/custom_components/</code></p>"},{"location":"examples/#production-patterns","title":"Production Patterns","text":""},{"location":"examples/#complete-ml-pipeline","title":"Complete ML Pipeline","text":"<p>End-to-end machine learning workflow with all features:</p> <pre><code>from flowyml import Pipeline, step, context, LocalStack\nfrom flowyml import Dataset, Model, Metrics\n\nctx = context(\n    data_path=\"data/training.csv\",\n    model_type=\"random_forest\",\n    validation_split=0.2,\n    random_seed=42\n)\n\nstack = LocalStack()\n\n@step(outputs=[\"dataset\"])\ndef load_dataset(data_path: str):\n    \"\"\"Load and create dataset asset.\"\"\"\n    df = pd.read_csv(data_path)\n    return Dataset.create(\n        data=df,\n        name=\"training_data\",\n        properties={\"rows\": len(df), \"columns\": list(df.columns)}\n    )\n\n@step(inputs=[\"dataset\"], outputs=[\"features\", \"labels\"])\ndef prepare_features(dataset, validation_split: float):\n    \"\"\"Engineer features and split data.\"\"\"\n    X, y = dataset.data.drop(\"target\", axis=1), dataset.data[\"target\"]\n    return train_test_split(X, y, test_size=validation_split)\n\n@step(inputs=[\"features\", \"labels\"], outputs=[\"model\"])\ndef train(features, labels, model_type: str):\n    \"\"\"Train and save model.\"\"\"\n    model = get_model(model_type)\n    model.fit(features[0], labels[0])\n    return Model.create(\n        data=model,\n        name=f\"{model_type}_model\",\n        framework=\"sklearn\"\n    )\n\n@step(inputs=[\"model\", \"features\", \"labels\"], outputs=[\"metrics\"])\ndef evaluate(model, features, labels):\n    \"\"\"Evaluate model and save metrics.\"\"\"\n    predictions = model.data.predict(features[1])\n    accuracy = accuracy_score(labels[1], predictions)\n    return Metrics.create(\n        accuracy=accuracy,\n        model_name=model.name\n    )\n\n# Build production pipeline\npipeline = Pipeline(\"ml_production\", context=ctx, stack=stack)\npipeline.add_step(load_dataset)\npipeline.add_step(prepare_features)\npipeline.add_step(train)\npipeline.add_step(evaluate)\n\n# Run with full tracking\nresult = pipeline.run(debug=True)\n\nif result.success:\n    print(f\"\u2713 Training complete!\")\n    print(f\"  Model: {result.outputs['model'].name}\")\n    print(f\"  Accuracy: {result.outputs['metrics'].accuracy:.2%}\")\n</code></pre> <p>\ud83d\udcc4 Full example: <code>examples/demo_pipeline.py</code></p>"},{"location":"examples/#running-examples","title":"Running Examples","text":""},{"location":"examples/#clone-and-setup","title":"Clone and Setup","text":"<pre><code># Clone repository\ngit clone https://github.com/flowyml/flowyml.git\ncd flowyml\n\n# Install with examples dependencies\npip install -e \".[examples]\"\n\n# Or with poetry\npoetry install --extras examples\n</code></pre>"},{"location":"examples/#run-an-example","title":"Run an Example","text":"<pre><code># Run simple pipeline\npython examples/simple_pipeline.py\n\n# Run with UI\nflowyml ui start  # In one terminal\npython examples/simple_pipeline_ui.py  # In another\n</code></pre>"},{"location":"examples/#modify-and-experiment","title":"Modify and Experiment","text":"<p>All examples are designed to be modified and extended. Try:</p> <ul> <li>Changing context parameters</li> <li>Adding new steps</li> <li>Experimenting with different caching strategies</li> <li>Integrating your own data sources</li> </ul>"},{"location":"examples/#example-index","title":"Example Index","text":"Example Features Complexity <code>simple_pipeline.py</code> Basic pipeline structure \u2b50 <code>clean_pipeline.py</code> ETL pattern, caching \u2b50\u2b50 <code>caching_pipeline.py</code> Caching strategies \u2b50\u2b50 <code>conditional_pipeline.py</code> Conditional execution \u2b50\u2b50 <code>simple_pipeline_ui.py</code> UI integration \u2b50\u2b50 <code>demo_pipeline.py</code> Full ML pipeline, assets \u2b50\u2b50\u2b50 <code>ui_integration_example.py</code> Complete UI features \u2b50\u2b50\u2b50 <code>gcp_stack/</code> Cloud deployment \u2b50\u2b50\u2b50\u2b50 <code>custom_components/</code> Extensibility \u2b50\u2b50\u2b50\u2b50"},{"location":"examples/#next-steps","title":"Next Steps","text":"<ul> <li>User Guide: Learn concepts in depth</li> <li>API Reference: Explore all available APIs</li> <li>Contributing: Add your own examples!</li> </ul> <p>Have questions? Open an issue or join our community!</p>"},{"location":"getting-started/","title":"Getting Started with FlowyML \ud83d\ude80","text":"<p>Welcome to FlowyML! In the next 5-10 minutes, you'll go from zero to running your first production-ready pipeline. No prior MLOps experience required.</p>"},{"location":"getting-started/#what-youll-learn-why","title":"What You'll Learn &amp; Why","text":"<p>[!NOTE] What you'll build: A complete ML pipeline with data loading, training, and real-time monitoring.</p> <p>What you'll master: The core concepts that make FlowyML powerful: steps, pipelines, context injection, and the visual UI.</p> <p>Why this matters: These same patterns scale from quick prototypes to enterprise deployments serving millions of predictions.</p>"},{"location":"getting-started/#installation","title":"Installation \ud83d\udce6","text":"<p>FlowyML requires Python 3.9 or higher.</p>"},{"location":"getting-started/#basic-installation","title":"Basic Installation","text":"<pre><code>pip install flowyml\n</code></pre> <p>[!TIP] Pro Tip: Use a virtual environment (<code>venv</code> or <code>conda</code>) to avoid dependency conflicts with other projects.</p>"},{"location":"getting-started/#full-installation-recommended","title":"Full Installation (Recommended)","text":"<p>Includes UI support and common ML dependencies:</p> <pre><code>pip install \"flowyml[all]\"\n</code></pre> <p>What this gets you: The web dashboard, Keras integration, cloud storage backends, and everything you need for production deployments. Start with this unless you have size constraints.</p>"},{"location":"getting-started/#verify-installation","title":"Verify Installation","text":"<pre><code>flowyml --version\n</code></pre> <p>You should see the version number. If not, check that your Python PATH is configured correctly.</p>"},{"location":"getting-started/#your-first-project","title":"Your First Project \ud83d\udcc1","text":"<p>Let's create a new project using the CLI.</p> <pre><code>flowyml init my-first-project\ncd my-first-project\n</code></pre> <p>This creates a directory structure like this:</p> <pre><code>my-first-project/\n\u251c\u2500\u2500 flowyml.yaml         # Project configuration\n\u251c\u2500\u2500 README.md            # Project documentation\n\u251c\u2500\u2500 requirements.txt     # Python dependencies\n\u2514\u2500\u2500 src/\n    \u2514\u2500\u2500 pipeline.py      # Your pipeline code lives here\n</code></pre> <p>[!TIP] Why this structure? It separates code (<code>src/</code>), configuration (<code>flowyml.yaml</code>), and dependencies (<code>requirements.txt</code>) \u2014 exactly what you need for clean version control and team collaboration.</p>"},{"location":"getting-started/#creating-a-pipeline","title":"Creating a Pipeline \ud83e\uddea","text":"<p>Open <code>src/pipeline.py</code> and replace its content with this simple example:</p> <pre><code>from flowyml import Pipeline, step, context\n\n# 1. Define Steps\n@step(outputs=[\"data\"])\ndef fetch_data():\n    print(\"Fetching data...\")\n    return [1, 2, 3, 4, 5]\n\n# 2. Define another Step with inputs\n@step(inputs=[\"data\"], outputs=[\"processed\"])\ndef process_data(data):\n    print(f\"Processing {len(data)} items...\")\n    return [x * 2 for x in data]\n\n# 3. Create and configure the Pipeline\nif __name__ == \"__main__\":\n    # Create pipeline\n    pipeline = Pipeline(\"my_first_pipeline\")\n\n    # Add steps in order\n    pipeline.add_step(fetch_data)\n    pipeline.add_step(process_data)\n\n    # 4. Run it!\n    result = pipeline.run()\n\n    if result.success:\n        print(f\"\u2713 Pipeline finished successfully!\")\n        print(f\"Result: {result.outputs}\")\n    else:\n        print(f\"\u2717 Pipeline failed\")\n</code></pre>"},{"location":"getting-started/#understanding-what-just-happened","title":"Understanding What Just Happened","text":"<p>Let's break down the key concepts:</p> <ol> <li> <p><code>@step</code> decorator: Turns any Python function into a pipeline step. The <code>outputs=[\"data\"]</code> tells FlowyML what this step produces.</p> </li> <li> <p>Data flow: The <code>@step(inputs=[\"data\"], ...)</code> on <code>process_data</code> automatically connects it to <code>fetch_data</code>'s output. No manual wiring needed.</p> </li> <li> <p>Pipeline assembly: <code>pipeline.add_step()</code> builds your DAG. FlowyML figures out the execution order based on data dependencies.</p> </li> <li> <p>Execution: <code>pipeline.run()</code> executes all steps in the right order and returns a result object with status and outputs.</p> </li> </ol> <p>[!IMPORTANT] Why this matters: This same pattern works whether you have 3 steps or 300. The complexity doesn't grow with your pipeline.</p>"},{"location":"getting-started/#running-the-pipeline","title":"Running the Pipeline \u25b6\ufe0f","text":"<p>Execute the script:</p> <pre><code>python src/pipeline.py\n</code></pre> <p>You should see output indicating the steps are executing:</p> <pre><code>Fetching data...\nProcessing 5 items...\n\u2713 Pipeline finished successfully!\nResult: {'processed': [2, 4, 6, 8, 10]}\n</code></pre> <p>[!TIP] Pro Tip: Pipelines are idempotent by default. Run it again and watch how caching kicks in \u2014 steps that haven't changed won't re-execute.</p>"},{"location":"getting-started/#visualizing-with-the-ui","title":"Visualizing with the UI \ud83d\udda5\ufe0f","text":"<p>Now, let's see your pipeline in the FlowyML UI \u2014 this is where the magic happens for debugging and monitoring.</p> <p>Step 1: Start the UI server</p> <pre><code>flowyml ui start\n</code></pre> <p>You'll see: <pre><code>\ud83c\udf0a FlowyML UI server started\n\ud83d\udcca Dashboard: http://localhost:8080\n\ud83d\udd0c API: http://localhost:8080/api\n</code></pre></p> <p>[!NOTE] What's running: A lightweight FastAPI server that displays your pipeline runs,  DAG visualizations, and artifact inspection \u2014 all in real-time.</p> <p>Step 2: Run your pipeline (in a separate terminal)</p> <pre><code>python src/pipeline.py\n</code></pre> <p>Step 3: Watch it live!</p> <p>Open your browser to <code>http://localhost:8080</code>. You'll see:</p> <ul> <li>Pipeline DAG: Visual graph showing step dependencies</li> <li>Real-time execution: Steps highlight as they run</li> <li>Artifact inspection: Click any step to see its inputs/outputs</li> <li>Run history: Compare different runs side-by-side</li> </ul> <p>Why the UI matters: Imagine debugging a failed step at 3 AM in production. Instead of grep'ing through logs, you see exactly: - Which step failed - What its inputs were - The full error traceback - What downstream steps were skipped</p>"},{"location":"getting-started/#adding-context-parameters","title":"Adding Context &amp; Parameters \ud83c\udf9b\ufe0f","text":"<p>Let's make the pipeline configurable using context \u2014 one of FlowyML's killer features.</p> <p>Update your pipeline:</p> <pre><code>from flowyml import Pipeline, step, context\n\n@step(outputs=[\"data\"])\ndef fetch_data(dataset_size: int = 5):  # \u2190 Parameter with default\n    print(f\"Fetching {dataset_size} items...\")\n    return list(range(dataset_size))\n\n@step(inputs=[\"data\"], outputs=[\"processed\"])\ndef process_data(data, multiplier: int = 2):  # \u2190 Another parameter\n    print(f\"Processing with multiplier={multiplier}...\")\n    return [x * multiplier for x in data]\n\nif __name__ == \"__main__\":\n    # Create context with your config\n    ctx = context(\n        dataset_size=10,\n        multiplier=3\n    )\n\n    # Pass context to pipeline\n    pipeline = Pipeline(\"configurable_pipeline\", context=ctx)\n    pipeline.add_step(fetch_data)\n    pipeline.add_step(process_data)\n\n    result = pipeline.run()\n    print(f\"Result: {result.outputs}\")\n</code></pre> <p>Run it again:</p> <pre><code>python src/pipeline.py\n</code></pre> <p>Output: <pre><code>Fetching 10 items...\nProcessing with multiplier=3...\nResult: {'processed': [0, 3, 6, 9, 12, ...]}\n</code></pre></p>"},{"location":"getting-started/#the-power-of-context-injection","title":"The Power of Context Injection","text":"<p>[!TIP] Why this is revolutionary: You just separated configuration from code. The same pipeline can run with different configs for: - Dev: Small dataset for fast iteration - Staging: Medium dataset for integration testing - Production: Full dataset for real predictions</p> <p>Change the context, not the code. This is how you go from prototype to production without rewriting.</p>"},{"location":"getting-started/#next-steps","title":"Next Steps \ud83d\udcda","text":"<p>Congratulations! You've built a complete pipeline with monitoring. Here's where to go next based on your goals:</p>"},{"location":"getting-started/#i-want-to-build-production-pipelines","title":"\ud83c\udfaf I want to build production pipelines","text":"<p>\u2192 Projects &amp; Multi-Tenancy: Learn to organize multiple pipelines, isolate environments, and manage teams</p> <p>\u2192 Scheduling: Automate your pipelines with cron-style scheduling</p> <p>\u2192 Versioning: Track pipeline changes and rollback when needed</p>"},{"location":"getting-started/#i-want-to-optimize-performance","title":"\ud83d\ude80 I want to optimize performance","text":"<p>\u2192 Caching Strategies: Save compute time and costs with intelligent caching</p> <p>\u2192 Parallel Execution: Run independent steps concurrently</p> <p>\u2192 Performance Guide: Benchmark and optimize your pipelines</p>"},{"location":"getting-started/#i-want-advanced-ml-features","title":"\ud83d\udd2c I want advanced ML features","text":"<p>\u2192 Assets &amp; Lineage: Work with typed artifacts (Datasets, Models, Metrics)</p> <p>\u2192 Model Registry: Version and manage models</p> <p>\u2192 LLM Tracing: Track GenAI costs and performance</p>"},{"location":"getting-started/#i-want-to-understand-concepts-deeply","title":"\ud83e\udde0 I want to understand concepts deeply","text":"<p>\u2192 Core Concepts: Pipelines: Master pipeline design patterns</p> <p>\u2192 Core Concepts: Steps: Learn step best practices</p> <p>\u2192 Core Concepts: Context: Advanced context injection techniques</p>"},{"location":"getting-started/#i-want-to-integrate-with-my-stack","title":"\ud83c\udfa8 I want to integrate with my stack","text":"<p>\u2192 Keras Integration: Automatic experiment tracking for Keras</p> <p>\u2192 GCP Integration: Deploy to Google Cloud Platform</p> <p>\u2192 Custom Components: Extend FlowyML for your needs</p> <p>Questions or stuck? Check out the Resources page for community links, tutorials, and support channels.</p> <p>Ready to dive deeper? The User Guide is your next stop for production-grade patterns.</p>"},{"location":"resources/","title":"Step-Level Resource Specification","text":"<p>FlowyML provides comprehensive step-level resource specification that allows you to declare CPU, GPU, memory, and other compute requirements for individual pipeline steps. These specifications are automatically translated to orchestrator-specific formats (Kubernetes, Google Vertex AI, AWS SageMaker, etc.).</p>"},{"location":"resources/#overview","title":"Overview","text":"<p>Resource specification enables you to: - Define compute resources per step (CPU, memory, storage) - Request GPUs with specific types, counts, and memory - Control node placement with affinity rules and tolerations - Optimize costs by right-sizing resources for each step - Ensure compatibility across different orchestrators</p>"},{"location":"resources/#quick-start","title":"Quick Start","text":""},{"location":"resources/#simple-cpu-and-memory","title":"Simple CPU and Memory","text":"<pre><code>from flowyml.core import step\nfrom flowyml.core.resources import ResourceRequirements\n\n@step(resources=ResourceRequirements(cpu=\"2\", memory=\"4Gi\"))\ndef preprocess_data(data):\n    # Runs with 2 CPU cores and 4GB memory\n    return processed_data\n</code></pre>"},{"location":"resources/#gpu-training","title":"GPU Training","text":"<pre><code>from flowyml.core.resources import ResourceRequirements, GPUConfig\n\n@step(\n    resources=ResourceRequirements(\n        cpu=\"4\",\n        memory=\"16Gi\",\n        gpu=GPUConfig(gpu_type=\"nvidia-tesla-v100\", count=2, memory=\"16Gi\")\n    )\n)\ndef train_model(dataset):\n    # Runs with 4 CPUs, 16GB RAM, and 2 V100 GPUs\n    return model\n</code></pre>"},{"location":"resources/#resource-specification-api","title":"Resource Specification API","text":""},{"location":"resources/#resourcerequirements","title":"ResourceRequirements","text":"<p>The main class for specifying step resources:</p> <pre><code>ResourceRequirements(\n    cpu: Optional[str] = None,           # CPU cores\n    memory: Optional[str] = None,        # RAM amount\n    storage: Optional[str] = None,       # Ephemeral storage\n    gpu: Optional[GPUConfig] = None,     # GPU configuration\n    node_affinity: Optional[NodeAffinity] = None  # Node selection\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>cpu</code>: CPU cores required</li> <li>Formats: <code>\"2\"</code> (2 cores), <code>\"500m\"</code> (0.5 cores), <code>\"2.5\"</code> (2.5 cores)</li> <li> <p>Kubernetes-style: <code>\"500m\"</code> = 500 millicores = 0.5 cores</p> </li> <li> <p><code>memory</code>: RAM required</p> </li> <li>Formats: <code>\"4Gi\"</code>, <code>\"8192Mi\"</code>, <code>\"4G\"</code>, <code>\"1073741824\"</code> (bytes)</li> <li>Binary units: Ki, Mi, Gi, Ti (1024-based)</li> <li> <p>Decimal units: K, M, G, T (1000-based)</p> </li> <li> <p><code>storage</code>: Ephemeral storage required</p> </li> <li> <p>Same format as memory: <code>\"100Gi\"</code>, <code>\"500G\"</code>, etc.</p> </li> <li> <p><code>gpu</code>: GPU configuration (see GPUConfig below)</p> </li> <li> <p><code>node_affinity</code>: Node placement rules (see NodeAffinity below)</p> </li> </ul>"},{"location":"resources/#gpuconfig","title":"GPUConfig","text":"<p>Specify GPU requirements:</p> <pre><code>GPUConfig(\n    gpu_type: str,                       # GPU type/model\n    count: int = 1,                      # Number of GPUs\n    memory: Optional[str] = None         # GPU memory per device\n)\n</code></pre> <p>Common GPU Types: - NVIDIA: <code>\"nvidia-tesla-v100\"</code>, <code>\"nvidia-tesla-t4\"</code>, <code>\"nvidia-a100\"</code>, <code>\"nvidia-h100\"</code> - Cloud-specific:   - GCP: <code>\"nvidia-tesla-v100\"</code>, <code>\"nvidia-tesla-p100\"</code>, <code>\"nvidia-tesla-a100\"</code>   - AWS: <code>\"nvidia-tesla-v100\"</code>, <code>\"nvidia-a10g\"</code>, <code>\"nvidia-h100\"</code>   - Azure: <code>\"nvidia-tesla-v100\"</code>, <code>\"nvidia-tesla-t4\"</code>, <code>\"nvidia-a100\"</code></p> <p>Example:</p> <pre><code>gpu=GPUConfig(\n    gpu_type=\"nvidia-a100\",\n    count=8,\n    memory=\"80Gi\"\n)\n</code></pre>"},{"location":"resources/#nodeaffinity","title":"NodeAffinity","text":"<p>Control which nodes your step runs on (primarily for Kubernetes):</p> <pre><code>NodeAffinity(\n    required: dict[str, str] = {},       # Hard constraints\n    preferred: dict[str, str] = {},      # Soft constraints\n    tolerations: list[dict] = []         # Tolerate node taints\n)\n</code></pre> <p>Example:</p> <pre><code>node_affinity=NodeAffinity(\n    required={\"gpu\": \"true\", \"zone\": \"us-central1-a\"},\n    preferred={\"instance-type\": \"n1-standard-8\"},\n    tolerations=[\n        {\"key\": \"nvidia.com/gpu\", \"operator\": \"Exists\"},\n        {\"key\": \"dedicated\", \"value\": \"gpu\", \"effect\": \"NoSchedule\"}\n    ]\n)\n</code></pre>"},{"location":"resources/#complete-examples","title":"Complete Examples","text":""},{"location":"resources/#example-1-data-processing-pipeline","title":"Example 1: Data Processing Pipeline","text":"<pre><code>from flowyml.core import step\nfrom flowyml.core.resources import ResourceRequirements\n\n@step(resources=ResourceRequirements(cpu=\"1\", memory=\"2Gi\"))\ndef load_data(path):\n    # Small resource footprint for data loading\n    return data\n\n@step(resources=ResourceRequirements(cpu=\"8\", memory=\"32Gi\"))\ndef process_data(data):\n    # High memory processing\n    return processed\n\n@step(resources=ResourceRequirements(cpu=\"2\", memory=\"4Gi\"))\ndef save_results(processed):\n    # Moderate resource saving\n    return status\n</code></pre>"},{"location":"resources/#example-2-gpu-training-with-node-affinity","title":"Example 2: GPU Training with Node Affinity","text":"<pre><code>from flowyml.core import step\nfrom flowyml.core.resources import ResourceRequirements, GPUConfig, NodeAffinity\n\n@step(\n    resources=ResourceRequirements(\n        cpu=\"16\",\n        memory=\"128Gi\",\n        storage=\"500Gi\",\n        gpu=GPUConfig(gpu_type=\"nvidia-a100\", count=8, memory=\"80Gi\"),\n        node_affinity=NodeAffinity(\n            required={\n                \"cloud.google.com/gke-nodepool\": \"gpu-pool\",\n                \"nvidia.com/gpu.present\": \"true\"\n            },\n            tolerations=[\n                {\"key\": \"nvidia.com/gpu\", \"operator\": \"Exists\"},\n                {\"key\": \"dedicated\", \"value\": \"gpu-training\", \"effect\": \"NoSchedule\"}\n            ]\n        )\n    )\n)\ndef distributed_training(dataset):\n    # Large-scale distributed training on specific GPU nodes\n    return model\n</code></pre>"},{"location":"resources/#example-3-multi-step-ml-pipeline","title":"Example 3: Multi-Step ML Pipeline","text":"<pre><code>from flowyml.core import step\nfrom flowyml.core.resources import ResourceRequirements, GPUConfig\n\n# Preprocessing: Moderate resources\n@step(resources=ResourceRequirements(cpu=\"4\", memory=\"16Gi\"))\ndef preprocess(raw_data):\n    return features\n\n# Training: GPU-intensive\n@step(resources=ResourceRequirements(\n    cpu=\"8\", memory=\"64Gi\",\n    gpu=GPUConfig(gpu_type=\"nvidia-tesla-v100\", count=4)\n))\ndef train(features):\n    return model\n\n# Evaluation: CPU-only\n@step(resources=ResourceRequirements(cpu=\"2\", memory=\"8Gi\"))\ndef evaluate(model, test_data):\n    return metrics\n\n# Deployment: Lightweight\n@step(resources=ResourceRequirements(cpu=\"1\", memory=\"2Gi\"))\ndef deploy(model):\n    return endpoint\n</code></pre>"},{"location":"resources/#backward-compatibility","title":"Backward Compatibility","text":"<p>Resource specifications are fully backward compatible with the dict format:</p> <pre><code># Old dict format (still works)\n@step(resources={\"cpu\": \"2\", \"memory\": \"4Gi\"})\ndef old_style_step():\n    pass\n\n# New ResourceRequirements format (recommended)\n@step(resources=ResourceRequirements(cpu=\"2\", memory=\"4Gi\"))\ndef new_style_step():\n    pass\n</code></pre>"},{"location":"resources/#orchestrator-support","title":"Orchestrator Support","text":"<p>Resource specifications are automatically translated to orchestrator-specific formats:</p>"},{"location":"resources/#local-orchestrator","title":"Local Orchestrator","text":"<ul> <li>Resource monitoring with warnings if limits exceeded</li> <li>Optional hard limits via cgroups (if available)</li> <li>GPU detection and allocation</li> </ul>"},{"location":"resources/#kubernetes-orchestrator","title":"Kubernetes Orchestrator","text":"<p>Translates to Kubernetes pod specifications:</p> <pre><code>resources:\n  requests:\n    cpu: \"2\"\n    memory: \"4Gi\"\n    nvidia.com/gpu: \"2\"\n  limits:\n    cpu: \"4\"\n    memory: \"16Gi\"\n    nvidia.com/gpu: \"2\"\naffinity:\n  nodeAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: gpu\n          operator: In\n          values:\n          - \"true\"\n</code></pre>"},{"location":"resources/#google-vertex-ai-orchestrator","title":"Google Vertex AI Orchestrator","text":"<p>Translates to Vertex AI machine types and accelerators:</p> <pre><code># FlowyML resource spec\nresources=ResourceRequirements(\n    cpu=\"16\", memory=\"64Gi\",\n    gpu=GPUConfig(gpu_type=\"nvidia-tesla-v100\", count=4)\n)\n\n# Translates to Vertex AI:\n# machine_type: \"n1-standard-16\"\n# accelerator_type: \"NVIDIA_TESLA_V100\"\n# accelerator_count: 4\n</code></pre>"},{"location":"resources/#aws-sagemaker-orchestrator","title":"AWS SageMaker Orchestrator","text":"<p>Translates to SageMaker instance types:</p> <pre><code># FlowyML resource spec\nresources=ResourceRequirements(\n    cpu=\"8\", memory=\"32Gi\",\n    gpu=GPUConfig(gpu_type=\"nvidia-tesla-v100\", count=1)\n)\n\n# Translates to SageMaker:\n# instance_type: \"ml.p3.2xlarge\"  # V100 GPU instance\n</code></pre>"},{"location":"resources/#best-practices","title":"Best Practices","text":""},{"location":"resources/#1-right-size-your-resources","title":"1. Right-Size Your Resources","text":"<pre><code># \u274c Over-provisioning wastes money\n@step(resources=ResourceRequirements(cpu=\"64\", memory=\"512Gi\"))\ndef simple_preprocessing(data):\n    return data.lower()\n\n# \u2705 Match resources to workload\n@step(resources=ResourceRequirements(cpu=\"2\", memory=\"4Gi\"))\ndef simple_preprocessing(data):\n    return data.lower()\n</code></pre>"},{"location":"resources/#2-use-gpus-only-when-needed","title":"2. Use GPUs Only When Needed","text":"<pre><code># \u274c Don't use GPU for CPU-bound tasks\n@step(resources=ResourceRequirements(\n    gpu=GPUConfig(gpu_type=\"nvidia-a100\", count=1)\n))\ndef json_parsing(file):\n    return parse_json(file)\n\n# \u2705 Reserve GPUs for compute-intensive tasks\n@step(resources=ResourceRequirements(cpu=\"2\", memory=\"4Gi\"))\ndef json_parsing(file):\n    return parse_json(file)\n\n@step(resources=ResourceRequirements(\n    cpu=\"8\", memory=\"32Gi\",\n    gpu=GPUConfig(gpu_type=\"nvidia-a100\", count=4)\n))\ndef train_neural_network(data):\n    return model\n</code></pre>"},{"location":"resources/#3-use-node-affinity-for-specific-hardware","title":"3. Use Node Affinity for Specific Hardware","text":"<pre><code># Target specific node pools with specialized hardware\n@step(resources=ResourceRequirements(\n    cpu=\"32\", memory=\"256Gi\",\n    gpu=GPUConfig(gpu_type=\"nvidia-h100\", count=8),\n    node_affinity=NodeAffinity(\n        required={\"hardware\": \"latest-gen\"},\n        tolerations=[{\"key\": \"expensive\", \"operator\": \"Exists\"}]\n    )\n))\ndef cutting_edge_training(data):\n    return model\n</code></pre>"},{"location":"resources/#4-consider-storage-for-large-datasets","title":"4. Consider Storage for Large Datasets","text":"<pre><code>@step(resources=ResourceRequirements(\n    cpu=\"16\",\n    memory=\"64Gi\",\n    storage=\"1Ti\"  # Large ephemeral storage for intermediate files\n))\ndef process_large_dataset(data_path):\n    # Downloads and processes terabytes of data\n    return processed\n</code></pre>"},{"location":"resources/#validation","title":"Validation","text":"<p>Resource specifications are validated at step creation time:</p> <pre><code># \u2705 Valid formats\nResourceRequirements(cpu=\"2\")                    # Integer cores\nResourceRequirements(cpu=\"2.5\")                  # Decimal cores\nResourceRequirements(cpu=\"500m\")                 # Millicores\nResourceRequirements(memory=\"4Gi\")               # Gibibytes\nResourceRequirements(memory=\"4G\")                # Gigabytes\nResourceRequirements(memory=\"4096Mi\")            # Mebibytes\n\n# \u274c Invalid formats (will raise ValueError)\nResourceRequirements(cpu=\"invalid\")              # Not a number\nResourceRequirements(memory=\"4Zb\")               # Invalid unit\nResourceRequirements(gpu=GPUConfig(gpu_type=\"v100\", count=0))  # Count must be &gt;= 1\n</code></pre>"},{"location":"resources/#migration-from-zenml","title":"Migration from ZenML","text":"<p>If you're migrating from ZenML, flowyml resource specifications are similar:</p> <pre><code># ZenML\nfrom zenml import step\nfrom zenml.config import ResourceSettings\n\n@step(settings={\"resources\": ResourceSettings(\n    cpu_count=2, memory=\"4GB\", gpu_count=1\n)})\ndef zenml_step():\n    pass\n\n# FlowyML\nfrom flowyml.core import step\nfrom flowyml.core.resources import ResourceRequirements, GPUConfig\n\n@step(resources=ResourceRequirements(\n    cpu=\"2\",\n    memory=\"4Gi\",  # Note: Gi not GB\n    gpu=GPUConfig(gpu_type=\"nvidia-tesla-v100\", count=1)\n))\ndef flowyml_step():\n    pass\n</code></pre>"},{"location":"resources/#troubleshooting","title":"Troubleshooting","text":""},{"location":"resources/#resources-not-applied","title":"Resources Not Applied","text":"<p>Problem: Resources specified but not enforced</p> <p>Solution: Ensure your orchestrator supports resource specifications: <pre><code># Check if resources are set\nmy_step = my_function_step\nprint(f\"Resources: {my_step.resources}\")\n</code></pre></p>"},{"location":"resources/#gpu-not-available","title":"GPU Not Available","text":"<p>Problem: GPU requested but not available in cluster</p> <p>Solution: Use node affinity to target GPU nodes: <pre><code>@step(resources=ResourceRequirements(\n    gpu=GPUConfig(gpu_type=\"nvidia-v100\", count=1),\n    node_affinity=NodeAffinity(\n        required={\"accelerator\": \"nvidia-gpu\"}\n    )\n))\ndef gpu_step():\n    import torch\n    assert torch.cuda.is_available(), \"GPU not found!\"\n</code></pre></p>"},{"location":"resources/#out-of-memory-errors","title":"Out of Memory Errors","text":"<p>Problem: Step crashes with OOM</p> <p>Solution: Increase memory allocation: <pre><code># Before\n@step(resources=ResourceRequirements(memory=\"4Gi\"))\ndef memory_intensive():\n    pass\n\n# After\n@step(resources=ResourceRequirements(memory=\"16Gi\"))  # Increased 4x\ndef memory_intensive():\n    pass\n</code></pre></p>"},{"location":"resources/#related-documentation","title":"Related Documentation","text":"<ul> <li>Orchestrators Guide - Configure different orchestrators</li> <li>Pipeline Optimization - Optimize pipeline performance</li> <li>Cost Management - Control cloud spending</li> <li>Kubernetes Integration - Deploy on Kubernetes</li> </ul>"},{"location":"resources/#api-reference","title":"API Reference","text":"<p>For complete API documentation, see: - <code>flowyml.core.resources.ResourceRequirements</code> - <code>flowyml.core.resources.GPUConfig</code> - <code>flowyml.core.resources.NodeAffinity</code></p>"},{"location":"advanced/caching/","title":"Caching \u26a1","text":"<p>flowyml's intelligent caching system eliminates redundant work, saving you time and compute costs.</p> <p>[!NOTE] What you'll learn: How to skip expensive steps that have already run</p> <p>Key insight: The fastest code is the code you don't run. flowyml automatically detects when inputs and code haven't changed.</p>"},{"location":"advanced/caching/#why-caching-matters","title":"Why Caching Matters","text":"<p>Without caching: - Wasted time: Re-running data loading (10 mins) just to fix a typo in plotting - Wasted money: Retraining models (GPU hours) when only the evaluation metric changed - Slow iteration: Feedback loops take hours instead of seconds</p> <p>With flowyml caching: - Instant feedback: Skip straight to the step you're working on - Cost savings: Reduce cloud compute bills by 40-60% - Resume capability: Crash in step 5? Fix it and resume instantly; steps 1-4 are cached</p>"},{"location":"advanced/caching/#how-caching-works","title":"How Caching Works","text":"<p>flowyml calculates a Cache Key for every step before it runs. If a matching key exists, the step is skipped.</p> <p>The key combines: 1. Code Hash: The source code of the function 2. Input Hash: The values of all input arguments 3. Configuration: Parameters injected from context</p> <pre><code>graph TD\n    A[Start Step] --&gt; B{Cache Key Exists?}\n    B -- Yes --&gt; C[Load Output from Disk]\n    B -- No --&gt; D[Execute Step]\n    D --&gt; E[Save Output to Disk]\n    C --&gt; F[Return Result]\n    E --&gt; F\n</code></pre>"},{"location":"advanced/caching/#configuration","title":"Configuration \u2699\ufe0f","text":""},{"location":"advanced/caching/#pipeline-level-caching","title":"Pipeline-Level Caching","text":"<p>Enable or disable caching for an entire pipeline:</p> <pre><code>from flowyml import Pipeline\n\n# Enable caching (default)\npipeline = Pipeline(\"my_pipeline\", enable_cache=True)\n\n# Disable caching\npipeline = Pipeline(\"my_pipeline\", enable_cache=False)\n\n# Custom cache directory\npipeline = Pipeline(\"my_pipeline\", cache_dir=\"./custom_cache\")\n</code></pre>"},{"location":"advanced/caching/#step-level-caching","title":"Step-Level Caching","text":"<p>Control caching for individual steps:</p> <pre><code>from flowyml import step\n\n# Use default caching (code_hash)\n@step(outputs=[\"result\"])\ndef cached_step(data):\n    return expensive_computation(data)\n\n# Disable caching for this step\n@step(cache=False, outputs=[\"fresh_data\"])\ndef always_run():\n    return fetch_latest_data()\n</code></pre>"},{"location":"advanced/caching/#decision-guide-caching-strategies","title":"Decision Guide: Caching Strategies","text":"Strategy Behavior Use When <code>code_hash</code> (Default) Invalidate if code OR inputs change Development: You're tweaking logic and need safety. <code>input_hash</code> Invalidate ONLY if inputs change Production/Training: Code is stable, but you're tuning hyperparameters. <code>cache=False</code> Always run Side Effects: Sending emails, writing to DB, fetching live data."},{"location":"advanced/caching/#1-cachecode_hash-safest","title":"1. <code>cache=\"code_hash\"</code> (Safest)","text":"<p>The default. If you change anything in the function (even a comment), it re-runs.</p> <pre><code>@step(cache=\"code_hash\")\ndef process(data):\n    # Changing this comment triggers a re-run!\n    return [x * 2 for x in data]\n</code></pre>"},{"location":"advanced/caching/#2-cacheinput_hash-fastest","title":"2. <code>cache=\"input_hash\"</code> (Fastest)","text":"<p>Ignores code changes. Only re-runs if the data passed to it changes.</p> <pre><code>@step(cache=\"input_hash\")\ndef train_model(dataset, epochs):\n    # You can refactor this code without triggering a 4-hour training run\n    # It only re-runs if 'dataset' or 'epochs' change\n    return model.fit(dataset, epochs=epochs)\n</code></pre> <p>[!WARNING] Use <code>input_hash</code> carefully! If you change the logic (e.g., fix a bug) but inputs stay the same, flowyml won't know to re-run it. You'll get the old, buggy result.</p>"},{"location":"advanced/caching/#3-cachefalse-side-effects","title":"3. <code>cache=False</code> (Side Effects)","text":"<p>For steps that must always run.</p> <pre><code>@step(cache=False)\ndef send_slack_alert(metrics):\n    # Always send the alert, even if metrics haven't changed\n    slack.send(f\"Accuracy: {metrics['accuracy']}\")\n</code></pre>"},{"location":"advanced/caching/#practical-examples","title":"Practical Examples","text":""},{"location":"advanced/caching/#example-1-data-pipeline-with-mixed-caching","title":"Example 1: Data Pipeline with Mixed Caching","text":"<pre><code>from flowyml import Pipeline, step\n\n@step(cache=\"code_hash\", outputs=[\"raw\"])\ndef load_data(file_path: str):\n    \"\"\"Cache invalidated if code OR file_path changes.\"\"\"\n    return pd.read_csv(file_path)\n\n@step(inputs=[\"raw\"], cache=\"input_hash\", outputs=[\"clean\"])\ndef clean_data(raw):\n    \"\"\"Cache invalidated only if raw data changes.\"\"\"\n    return raw.dropna().reset_index(drop=True)\n\n@step(inputs=[\"clean\"], cache=False, outputs=[\"result\"])\ndef upload_to_db(clean):\n    \"\"\"Never cached - always uploads.\"\"\"\n    database.save(clean)\n    return {\"uploaded\": len(clean)}\n\npipeline = Pipeline(\"etl\", enable_cache=True)\npipeline.add_step(load_data)\npipeline.add_step(clean_data)\npipeline.add_step(upload_to_db)\n</code></pre>"},{"location":"advanced/caching/#example-2-machine-learning-training","title":"Example 2: Machine Learning Training","text":"<pre><code>@step(cache=\"code_hash\", outputs=[\"dataset\"])\ndef prepare_dataset(data_path: str, test_split: float):\n    \"\"\"Recompute if code or parameters change.\"\"\"\n    return load_and_split(data_path, test_split)\n\n@step(inputs=[\"dataset\"], cache=\"input_hash\", outputs=[\"model\"])\ndef train_model(dataset, learning_rate: float, epochs: int):\n    \"\"\"Expensive training - cache based on inputs only.\"\"\"\n    # Training logic doesn't change often, but we try different hyperparameters\n    model = Model()\n    model.fit(dataset, lr=learning_rate, epochs=epochs)\n    return model\n\n@step(inputs=[\"model\"], cache=False, outputs=[\"metrics\"])\ndef evaluate_fresh(model):\n    \"\"\"Always evaluate to get latest metrics.\"\"\"\n    return model.evaluate(test_data)\n</code></pre>"},{"location":"advanced/caching/#example-3-debugging","title":"Example 3: Debugging","text":"<pre><code># During development\npipeline = Pipeline(\"debug_pipeline\", enable_cache=False)\n# All steps run fresh every time\n\n# After development\npipeline = Pipeline(\"prod_pipeline\", enable_cache=True)\n# Benefit from caching\n</code></pre>"},{"location":"advanced/caching/#cache-performance","title":"Cache Performance \ud83d\udcca","text":""},{"location":"advanced/caching/#cache-hits-save-time","title":"Cache Hits Save Time","text":"<pre><code>import time\n\n@step(cache=\"code_hash\")\ndef expensive_step():\n    time.sleep(5)  # Simulate expensive computation\n    return \"result\"\n\npipeline = Pipeline(\"perf_test\", enable_cache=True)\npipeline.add_step(expensive_step)\n\n# First run: 5+ seconds\nresult1 = pipeline.run()  # \u23f1\ufe0f 5.2s\n\n# Second run: milliseconds!\nresult2 = pipeline.run()  # \u23f1\ufe0f 0.05s \u26a1 (cached)\n</code></pre>"},{"location":"advanced/caching/#monitoring-cache-usage","title":"Monitoring Cache Usage","text":"<p>Check which steps were cached:</p> <pre><code>result = pipeline.run()\n\nfor step_name, step_result in result.step_results.items():\n    status = \"CACHED\" if step_result.cached else \"EXECUTED\"\n    print(f\"{step_name}: {status} ({step_result.duration_seconds:.2f}s)\")\n\n# Output:\n# load_data: CACHED (0.01s)\n# process: EXECUTED (2.34s)\n# train: CACHED (0.02s)\n</code></pre>"},{"location":"advanced/caching/#cache-storage","title":"Cache Storage \ud83d\udcbe","text":""},{"location":"advanced/caching/#default-location","title":"Default Location","text":"<p>By default, cache is stored in <code>.flowyml/cache</code>:</p> <pre><code>.flowyml/\n\u251c\u2500\u2500 cache/\n\u2502   \u251c\u2500\u2500 cache.db       # SQLite database tracking cache entries\n\u2502   \u2514\u2500\u2500 objects/       # Pickled cached objects\n\u2502       \u251c\u2500\u2500 abc123.pkl\n\u2502       \u2514\u2500\u2500 def456.pkl\n</code></pre>"},{"location":"advanced/caching/#custom-cache-directory","title":"Custom Cache Directory","text":"<pre><code>pipeline = Pipeline(\n    \"my_pipeline\",\n    enable_cache=True,\n    cache_dir=\"/path/to/custom/cache\"\n)\n</code></pre>"},{"location":"advanced/caching/#cache-entry-structure","title":"Cache Entry Structure","text":"<p>Each cache entry contains: - Key: Hash of code + inputs + step name - Value: Serialized step output - Metadata: Timestamp, step name, code hash - TTL: Optional time-to-live</p>"},{"location":"advanced/caching/#managing-the-cache","title":"Managing the Cache \ud83e\uddf9","text":""},{"location":"advanced/caching/#clear-cache-via-code","title":"Clear Cache via Code","text":"<pre><code>from flowyml.core.cache import CacheStore\n\ncache = CacheStore(\".flowyml/cache\")\n\n# Clear all cache\ncache.clear()\n\n# Clear specific step\ncache.clear_step(\"my_step_name\")\n</code></pre>"},{"location":"advanced/caching/#cache-invalidation-patterns","title":"Cache Invalidation Patterns","text":""},{"location":"advanced/caching/#manual-invalidation","title":"Manual Invalidation","text":"<pre><code># Before running, clear cache for specific step\ncache.clear_step(\"data_loading\")\n\n# Then run pipeline\nresult = pipeline.run()\n</code></pre>"},{"location":"advanced/caching/#automatic-invalidation","title":"Automatic Invalidation","text":"<p>Cache is automatically invalidated when: - Function code changes (for <code>code_hash</code> strategy) - Input values change - Dependencies are updated</p>"},{"location":"advanced/caching/#time-based-invalidation","title":"Time-Based Invalidation","text":"<p>Implement custom TTL:</p> <pre><code>from datetime import datetime, timedelta\n\n@step\ndef load_with_ttl():\n    # Custom logic to check cache age\n    cache_age = get_cache_age(\"load_with_ttl\")\n    if cache_age &gt; timedelta(hours=24):\n        # Force re-execution\n        return fetch_fresh_data()\n</code></pre>"},{"location":"advanced/caching/#debugging-cache-issues","title":"Debugging Cache Issues \ud83d\udd27","text":""},{"location":"advanced/caching/#verify-cache-is-working","title":"Verify Cache is Working","text":"<pre><code># Run with debug output\nresult = pipeline.run(debug=True)\n\n# Check cache status for each step\nfor step_name, step_result in result.step_results.items():\n    if step_result.cached:\n        print(f\"\u2713 {step_name} was cached\")\n    else:\n        print(f\"\u26a0 {step_name} was executed (not cached)\")\n</code></pre>"},{"location":"advanced/caching/#common-cache-issues","title":"Common Cache Issues","text":""},{"location":"advanced/caching/#issue-cache-not-hitting-when-expected","title":"Issue: Cache Not Hitting When Expected","text":"<p>Causes: - Input values changed slightly (floating point precision) - Code changed (even whitespace with <code>code_hash</code>) - Cache was cleared - Different function object (re-imported module)</p> <p>Solutions: <pre><code># Use input_hash if code changes shouldn't matter\n@step(cache=\"input_hash\")\n\n# Round inputs to avoid floating point issues\n@step\ndef process(value: float):\n    value = round(value, 6)  # Normalize precision\n    ...\n</code></pre></p>"},{"location":"advanced/caching/#issue-stale-cache","title":"Issue: Stale Cache","text":"<p>Cause: Using <code>input_hash</code> but logic changed</p> <p>Solution: <pre><code># Temporarily disable cache to force refresh\n@step(cache=False)  # Remove after verified\ndef updated_logic(data):\n    # New implementation\n    ...\n</code></pre></p> <p>Or manually clear: <pre><code>cache.clear_step(\"updated_logic\")\n</code></pre></p>"},{"location":"advanced/caching/#when-to-use-each-strategy","title":"When to Use Each Strategy","text":"Strategy Development Production Stable Logic Changing Logic Side Effects <code>code_hash</code> (default) \u2705 Best \u2705 Safe \u2705 Good \u2705 Best \u26a0\ufe0f Use with care <code>input_hash</code> \u26a0\ufe0f Risky \u2705 Efficient \u2705 Best \u274c Dangerous \u26a0\ufe0f Use with care <code>False</code> \u2705 Debugging \u2705 Real-time \u274c Wasteful \u2705 Fine \u2705 Required"},{"location":"advanced/caching/#best-practices","title":"Best Practices \ud83d\udca1","text":""},{"location":"advanced/caching/#1-enable-caching-in-development","title":"1. Enable Caching in Development","text":"<pre><code># \u2705 Good - iterates faster\npipeline = Pipeline(\"dev\", enable_cache=True)\n\n# Changes to unchanged steps don't re-run\n</code></pre>"},{"location":"advanced/caching/#2-be-careful-with-side-effects","title":"2. Be Careful with Side Effects","text":"<pre><code># \u274c Bad - side effect might be cached!\n@step\ndef send_email(results):\n    email_service.send(results)  # Might not run if cached!\n    return \"sent\"\n\n# \u2705 Good - disable cache for side effects\n@step(cache=False)\ndef send_email(results):\n    email_service.send(results)  # Always runs\n    return \"sent\"\n</code></pre>"},{"location":"advanced/caching/#3-use-input-hash-for-stable-expensive-operations","title":"3. Use Input Hash for Stable, Expensive Operations","text":"<pre><code># \u2705 Good - training is expensive and logic is stable\n@step(cache=\"input_hash\")\ndef train_model(data, hyperparams):\n    # 2 hours of training\n    model = expensive_training(data, hyperparams)\n    return model\n</code></pre>"},{"location":"advanced/caching/#4-clear-cache-between-major-changes","title":"4. Clear Cache Between Major Changes","text":"<pre><code># When refactoring or major updates\ncache_store.clear()\n\n# Or via CLI\n# flowyml cache clear\n</code></pre>"},{"location":"advanced/caching/#5-monitor-cache-hit-rates","title":"5. Monitor Cache Hit Rates","text":"<pre><code>result = pipeline.run()\n\ntotal_steps = len(result.step_results)\ncached_steps = sum(1 for r in result.step_results.values() if r.cached)\nhit_rate = cached_steps / total_steps * 100\n\nprint(f\"Cache hit rate: {hit_rate:.1f}%\")\n# Target: &gt; 50% in development, 20-40% in production\n</code></pre>"},{"location":"advanced/caching/#advanced-custom-cache-keys","title":"Advanced: Custom Cache Keys","text":"<p>For complete control, provide a custom cache key function:</p> <pre><code>def my_cache_key(step, inputs, context):\n    # Custom logic to generate cache key\n    return f\"{step.name}:{hash(str(inputs))}\"\n\n@step(cache=my_cache_key)\ndef custom_cached_step(data):\n    ...\n</code></pre>"},{"location":"advanced/caching/#next-steps","title":"Next Steps \ud83d\udcda","text":"<ul> <li>Steps: Configure step caching strategies</li> <li>Pipelines: Enable caching at pipeline level</li> <li>Configuration: Set cache directory in config</li> </ul>"},{"location":"advanced/checkpointing/","title":"Checkpointing &amp; Experiment Tracking \ud83d\udcbe","text":"<p>flowyml ensures you never lose progress. Save pipeline state automatically and track every experiment detail.</p> <p>[!NOTE] What you'll learn: How to resume failed pipelines and track model performance over time</p> <p>Key insight: Long-running pipelines will fail eventually. Checkpointing turns a catastrophe into a minor annoyance.</p>"},{"location":"advanced/checkpointing/#why-checkpointing-matters","title":"Why Checkpointing Matters","text":"<p>Without checkpointing: - Lost time: A crash at hour 9 of a 10-hour job means restarting from hour 0 - Wasted compute: Re-computing expensive intermediate steps - Frustration: \"It worked on my machine, why did it fail now?\"</p> <p>With flowyml checkpointing: - Resume instantly: Restart exactly where it failed - Inspect state: Load the checkpoint to debug what went wrong - Skip redundant work: Re-use successful steps</p>"},{"location":"advanced/checkpointing/#checkpointing","title":"\ud83d\udcbe Checkpointing","text":"<p>Checkpointing allows you to save the intermediate results of your pipeline steps. This is crucial for long-running pipelines, as it enables you to resume execution from the point of failure or to skip expensive steps that have already been computed.</p>"},{"location":"advanced/checkpointing/#real-world-pattern-the-resume-workflow","title":"Real-World Pattern: The \"Resume\" Workflow","text":"<p>Crash-proof your long-running pipelines.</p> <pre><code>from flowyml import Pipeline, checkpoint_enabled_pipeline\n\npipeline = Pipeline(\"heavy_processing\")\n\n# 1. Try to run\ntry:\n    pipeline.run()\nexcept Exception:\n    print(\"Pipeline crashed! Fix the bug and re-run.\")\n\n# 2. Resume later (e.g., in a new script or after fix)\n# flowyml detects the previous run state and resumes\npipeline = checkpoint_enabled_pipeline(pipeline, run_id=\"run_2023_10_27\")\npipeline.run()\n</code></pre> <p>[!TIP] Pro Tip: Always enable checkpointing for pipelines that take longer than 10 minutes. The storage cost is negligible compared to the compute time saved.</p>"},{"location":"advanced/checkpointing/#manual-checkpointing","title":"Manual Checkpointing","text":"<p>For finer control, you can use the <code>PipelineCheckpoint</code> object within your steps.</p> <pre><code>from flowyml import PipelineCheckpoint, step\n\n@step\ndef train_large_model(data):\n    checkpoint = PipelineCheckpoint()\n\n    # Check if we have a saved state\n    if checkpoint.exists(\"model_epoch_5\"):\n        model = checkpoint.load(\"model_epoch_5\")\n        start_epoch = 5\n    else:\n        model = init_model()\n        start_epoch = 0\n\n    for epoch in range(start_epoch, 10):\n        train_one_epoch(model, data)\n        # Save state\n        checkpoint.save(f\"model_epoch_{epoch+1}\", model)\n\n    return model\n</code></pre>"},{"location":"advanced/checkpointing/#experiment-tracking","title":"\ud83e\uddea Experiment Tracking","text":"<p>flowyml automatically tracks every pipeline run, capturing parameters, metrics, and artifacts. This allows you to compare experiments and reproduce results.</p>"},{"location":"advanced/checkpointing/#tracking-metrics","title":"Tracking Metrics","text":"<p>Use the <code>Metrics</code> asset to log performance indicators.</p> <pre><code>from flowyml import step, Metrics\n\n@step(outputs=[\"metrics\"])\ndef evaluate(model, test_data):\n    accuracy = model.score(test_data)\n    f1 = f1_score(model, test_data)\n\n    # Create a Metrics object\n    return Metrics.create(\n        accuracy=accuracy,\n        f1_score=f1,\n        epoch=10\n    )\n</code></pre>"},{"location":"advanced/checkpointing/#comparing-experiments","title":"Comparing Experiments","text":"<p>You can compare runs using the CLI or the Python API.</p> <p>CLI: <pre><code>flowyml experiment compare &lt;run_id_1&gt; &lt;run_id_2&gt;\n</code></pre></p> <p>Python: <pre><code>from flowyml import compare_runs\n\ndiff = compare_runs([\"run_1\", \"run_2\"])\nprint(diff)\n</code></pre></p>"},{"location":"advanced/checkpointing/#visualizing-experiments","title":"Visualizing Experiments","text":"<p>The flowyml UI provides a dedicated Experiments view where you can: - View a table of all runs. - Filter by parameters or metrics. - Plot metric trends over time. - Compare side-by-side details of selected runs.</p>"},{"location":"advanced/conditional/","title":"Conditional Execution \ud83d\udd00","text":"<p>flowyml supports dynamic pipeline execution paths based on runtime conditions. Build smart workflows that adapt to data quality, model performance, or external factors.</p> <p>[!NOTE] What you'll learn: How to build adaptive pipelines that change behavior based on data</p> <p>Key insight: Real-world pipelines aren't linear. They need to make decisions (e.g., \"Is this model good enough to deploy?\").</p>"},{"location":"advanced/conditional/#why-conditional-logic-matters","title":"Why Conditional Logic Matters","text":"<p>Without conditional logic: - Manual intervention: Stopping pipelines manually if accuracy is low - Rigid workflows: One size fits all, regardless of data volume or quality - Separate pipelines: Maintaining \"Training\" and \"Deployment\" pipelines separately</p> <p>With flowyml conditional logic: - Automated decisions: \"If accuracy &gt; 90%, deploy. Else, retrain.\" - Adaptive behavior: \"If data &gt; 1GB, use Spark. Else, use Pandas.\" - Unified workflows: Handle edge cases within the same pipeline</p>"},{"location":"advanced/conditional/#conditional-patterns","title":"Conditional Patterns","text":""},{"location":"advanced/conditional/#conditional-steps","title":"\ud83d\udd00 Conditional Steps","text":"<p>You can use the <code>conditional</code> decorator or utility to define branching logic.</p>"},{"location":"advanced/conditional/#using-if-condition","title":"Using <code>If</code> Condition","text":"<pre><code>from flowyml import Pipeline, step, If\n\n@step(outputs=[\"accuracy\"])\ndef evaluate_model():\n    return 0.95\n\n@step\ndef deploy_model():\n    print(\"Deploying model...\")\n\n@step\ndef retrain_model():\n    print(\"Retraining model...\")\n\npipeline = Pipeline(\"conditional_deploy\")\n\npipeline.add_step(evaluate_model)\n\n# Define conditional logic\n# If accuracy &gt; 0.9, run deploy_model, else run retrain_model\npipeline.add_control_flow(\n    If(\n        condition=lambda ctx: ctx.steps['evaluate_model'].outputs['accuracy'] &gt; 0.9,\n        then_step=deploy_model,\n        else_step=retrain_model\n    )\n)\n</code></pre>"},{"location":"advanced/conditional/#skipping-steps","title":"Skipping Steps","text":"<p>You can also conditionally skip steps.</p> <pre><code>@step(skip_if=lambda ctx: ctx.params['dry_run'] is True)\ndef upload_to_s3(data):\n    # This will be skipped if dry_run is True\n    s3.upload(data)\n</code></pre>"},{"location":"advanced/conditional/#decision-guide-control-flow","title":"Decision Guide: Control Flow","text":"Pattern Use When Example <code>If / Switch</code> Branching logic: Choose between different paths Deploy vs. Retrain <code>skip_if</code> Optional steps: Skip a step based on a flag Skip upload in <code>dry_run</code> Dynamic DAG Complex routing: Structure depends on data Route A for images, Route B for text"},{"location":"advanced/conditional/#pattern-1-the-deployment-gate","title":"Pattern 1: The Deployment Gate","text":"<p>The most common pattern: only deploy if the model meets a threshold.</p> <pre><code>from flowyml import If\n\npipeline.add_control_flow(\n    If(condition=lambda ctx: ctx[\"accuracy\"] &gt; 0.95)\n    .then(deploy_to_prod)\n    .else_(notify_slack_failure)\n)\n</code></pre>"},{"location":"advanced/conditional/#pattern-2-the-dry-run","title":"Pattern 2: The Dry Run","text":"<p>Skip side-effects when testing.</p> <pre><code>@step(skip_if=lambda ctx: ctx.params.get(\"dry_run\", False))\ndef upload_to_s3(data):\n    # This won't run if dry_run=True\n    s3.upload(data)\n</code></pre> <p>[!TIP] Best Practice: Keep conditions simple. If your logic is complex, move it into a dedicated <code>@step</code> that outputs a boolean flag, then check that flag in the <code>If</code> condition.</p>"},{"location":"advanced/data-drift/","title":"Data Drift Detection \ud83d\udcc9","text":"<p>flowyml ensures your models don't rot in production by detecting when live data diverges from training data.</p> <p>[!NOTE] What you'll learn: How to catch \"silent failures\" where models degrade because the world changed</p> <p>Key insight: A model is only as good as the data it sees. If data changes, predictions fail.</p>"},{"location":"advanced/data-drift/#why-drift-detection-matters","title":"Why Drift Detection Matters","text":"<p>Without drift detection: - Silent degradation: Model accuracy drops, but no errors are thrown - Reactive debugging: Users complain about bad predictions weeks later - Blind retraining: Retraining on schedule regardless of need</p> <p>With flowyml drift detection: - Proactive alerts: Know immediately when data distribution shifts - Targeted retraining: Retrain only when necessary - Root cause analysis: See exactly which features drifted (e.g., \"Age distribution shifted older\")</p>"},{"location":"advanced/data-drift/#concept","title":"\ud83d\udcc9 Concept","text":"<p>Data drift occurs when the statistical properties of the target variable or input features change over time. flowyml uses the Population Stability Index (PSI) to quantify this shift.</p>"},{"location":"advanced/data-drift/#concept_1","title":"\ud83d\udcc9 Concept","text":"<p>Data drift occurs when the statistical properties of the target variable or input features change over time. flowyml uses the Population Stability Index (PSI) to quantify this shift.</p>"},{"location":"advanced/data-drift/#detecting-drift","title":"\ud83d\udd75\ufe0f Detecting Drift","text":"<p>Use the <code>detect_drift</code> function to compare two datasets.</p> <pre><code>from flowyml.monitoring.data import detect_drift\nimport numpy as np\n\n# Reference data (e.g., training set)\ntrain_data = np.random.normal(0, 1, 1000)\n\n# Current data (e.g., production traffic)\nprod_data = np.random.normal(0.5, 1, 1000)  # Shifted mean\n\n# Check for drift\nresult = detect_drift(\n    reference_data=train_data,\n    current_data=prod_data,\n    threshold=0.1  # PSI threshold (default: 0.1)\n)\n\nif result['drift_detected']:\n    print(f\"\u26a0\ufe0f Drift detected! PSI: {result['psi']:.4f}\")\n    print(f\"Reference Mean: {result['reference_stats']['mean']:.2f}\")\n    print(f\"Current Mean: {result['current_stats']['mean']:.2f}\")\nelse:\n    print(\"\u2705 Data is stable.\")\n</code></pre>"},{"location":"advanced/data-drift/#computing-statistics","title":"\ud83d\udcca Computing Statistics","text":"<p>You can also compute basic statistics for any dataset using <code>compute_stats</code>.</p> <pre><code>from flowyml.monitoring.data import compute_stats\n\nstats = compute_stats(prod_data)\nprint(stats)\n# Output: {'count': 1000.0, 'mean': 0.48, 'std': 1.01, ...}\n</code></pre>"},{"location":"advanced/data-drift/#real-world-pattern-automated-quality-gate","title":"Real-World Pattern: Automated Quality Gate","text":"<p>Stop a pipeline if drift is detected, preventing bad models from being deployed or bad predictions from being served.</p> <pre><code>from flowyml import step, get_notifier, If\n\n@step(outputs=[\"drift_result\"])\ndef check_drift(new_batch):\n    reference = load_reference_data()\n    result = detect_drift(reference, new_batch)\n    return result\n\n@step\ndef alert_team(result):\n    get_notifier().send_slack(f\"\ud83d\udea8 Drift detected! PSI: {result['psi']}\")\n\n@step\ndef process_data(data):\n    # Continue processing\n    pass\n\n# Build pipeline with a quality gate\npipeline.add_step(check_drift)\npipeline.add_control_flow(\n    If(condition=lambda ctx: ctx[\"drift_result\"][\"drift_detected\"])\n    .then(alert_team)\n    .else_(process_data)\n)\n</code></pre> <p>[!TIP] Thresholds: A PSI &lt; 0.1 is usually safe. PSI &gt; 0.2 indicates significant drift requiring investigation.</p>"},{"location":"advanced/debugging/","title":"Debugging Tools \ud83d\udc1b","text":"<p>flowyml provides interactive debugging tools that let you pause pipelines, inspect state, and fix issues without restarting from scratch.</p> <p>[!NOTE] What you'll learn: How to debug pipelines like standard Python code</p> <p>Key insight: Distributed pipelines are notoriously hard to debug. flowyml brings the \"IDE experience\" to pipelines.</p>"},{"location":"advanced/debugging/#why-interactive-debugging-matters","title":"Why Interactive Debugging Matters","text":"<p>The old way (Airflow/Kubeflow): 1. Push code 2. Wait 10 mins for container build 3. Wait 20 mins for pipeline to fail 4. Read logs: <code>KeyError: 'x'</code> 5. Add print statement, repeat</p> <p>The flowyml way: 1. Set a breakpoint: <code>debugger.add_breakpoint(...)</code> 2. Run pipeline locally 3. Execution pauses at the error 4. Inspect variables, fix code, resume</p>"},{"location":"advanced/debugging/#debugging-strategies","title":"Debugging Strategies","text":"Strategy Tool Use When Interactive <code>StepDebugger</code> You need to inspect variables mid-execution Tracing <code>PipelineDebugger</code> You need to see the sequence of events and timing Post-Mortem <code>analyze_errors</code> A run already failed and you want to know why"},{"location":"advanced/debugging/#overview-i","title":"Overview \u2139\ufe0f","text":"<p>flowyml provides comprehensive debugging tools: - StepDebugger: Debug individual steps with breakpoints and inspection - PipelineDebugger: Debug entire pipelines with execution tracing - Utility Functions: Quick debugging helpers</p>"},{"location":"advanced/debugging/#step-debugging","title":"Step Debugging \ud83d\udc1e","text":""},{"location":"advanced/debugging/#basic-usage","title":"Basic Usage","text":"<pre><code>from flowyml import StepDebugger, step\n\n@step(outputs=[\"result\"])\ndef process_data(data):\n    return [x * 2 for x in data]\n\n# Create debugger\ndebugger = StepDebugger(process_data)\n\n# Set breakpoint\ndebugger.add_breakpoint(\n    condition=lambda inputs: len(inputs['data']) &gt; 100,\n    action=lambda step, inputs: print(f\"Large dataset: {len(inputs['data'])} items\")\n)\n\n### Real-World Pattern: Conditional Breakpoint\n\nStop execution only when data looks weird.\n\n```python\nfrom flowyml import StepDebugger, step\n\n@step\ndef process_data(batch):\n    # ... complex logic ...\n    return result\n\n# Debug specific edge case\ndebugger = StepDebugger(process_data)\n\n# \"Stop if we get an empty batch\"\ndebugger.add_breakpoint(\n    condition=lambda inputs: len(inputs['batch']) == 0,\n    action=lambda step, inputs: print(f\"\u26a0\ufe0f Empty batch detected in {step.name}!\")\n)\n\n# Run pipeline - it will pause ONLY if the condition is met\npipeline.run(debugger=debugger)\n</code></pre>"},{"location":"advanced/debugging/#inspecting-inputs-and-outputs","title":"Inspecting Inputs and Outputs","text":"<pre><code># Inspect inputs before execution\ndebugger.inspect_inputs(data=[1, 2, 3])\n# Output: {'data': [1, 2, 3]}\n\n# Execute and inspect output\nresult = debugger.debug_execute(data=[1, 2, 3])\ndebugger.inspect_output()\n# Output: [2, 4, 6]\n</code></pre>"},{"location":"advanced/debugging/#breakpoints","title":"Breakpoints","text":"<pre><code># Conditional breakpoint\ndebugger.add_breakpoint(\n    condition=lambda inputs: inputs.get('threshold', 0) &gt; 10,\n    action=lambda step, inputs: print(f\"\u26a0\ufe0f High threshold: {inputs['threshold']}\")\n)\n\n# Always-trigger breakpoint\ndebugger.add_breakpoint(\n    action=lambda step, inputs: print(f\"Executing {step.name}\")\n)\n\n# Remove breakpoints\ndebugger.clear_breakpoints()\n</code></pre>"},{"location":"advanced/debugging/#exception-debugging","title":"Exception Debugging","text":"<pre><code>@step(outputs=[\"result\"])\ndef failing_step(data):\n    if len(data) == 0:\n        raise ValueError(\"Empty data\")\n    return data[0]\n\ndebugger = StepDebugger(failing_step)\n\ntry:\n    result = debugger.debug_execute(data=[])\nexcept Exception as e:\n    # Get detailed traceback\n    debugger.debug_exception(e)\n    # Shows: exception type, message, inputs that caused it\n</code></pre>"},{"location":"advanced/debugging/#pipeline-debugging","title":"Pipeline Debugging \ud83d\udd78\ufe0f","text":""},{"location":"advanced/debugging/#tracing-execution","title":"Tracing Execution","text":"<pre><code>from flowyml import PipelineDebugger, Pipeline\n\npipeline = Pipeline(\"my_pipeline\")\npipeline.add_step(load)\npipeline.add_step(transform)\npipeline.add_step(save)\n\n# Create debugger\ndebugger = PipelineDebugger(pipeline)\n\n# Enable tracing\ndebugger.enable_tracing()\n\n# Run with tracing\nresult = pipeline.run()\n\n# View execution trace\ntrace = debugger.get_trace()\nfor entry in trace:\n    print(f\"{entry['timestamp']}: {entry['step_name']} - {entry['event']}\")\n</code></pre>"},{"location":"advanced/debugging/#profiling","title":"Profiling","text":"<pre><code># Profile pipeline execution\ndebugger.enable_profiling()\nresult = pipeline.run()\n\n# Get profile report\nprofile = debugger.get_profile()\nfor step_name, metrics in profile.items():\n    print(f\"{step_name}:\")\n    print(f\"  Duration: {metrics['duration_seconds']:.2f}s\")\n    print(f\"  Memory: {metrics['memory_mb']:.1f}MB\")\n    print(f\"  CPU: {metrics['cpu_percent']:.1f}%\")\n</code></pre>"},{"location":"advanced/debugging/#dag-visualization","title":"DAG Visualization","text":"<pre><code># Visualize pipeline structure\ndebugger.visualize_dag(output_path=\"pipeline_dag.png\")\n\n# Or get DOT format\ndot = debugger.get_dag_dot()\nprint(dot)\n</code></pre>"},{"location":"advanced/debugging/#error-analysis","title":"Error Analysis","text":"<pre><code># After failed run\nif not result.success:\n    analysis = debugger.analyze_errors(result)\n\n    print(f\"Failed steps: {analysis['failed_steps']}\")\n    print(f\"Error types: {analysis['error_types']}\")\n    print(f\"Common patterns: {analysis['patterns']}\")\n</code></pre>"},{"location":"advanced/debugging/#execution-replay","title":"Execution Replay","text":"<pre><code># Replay a previous run\ndebugger.replay_execution(run_id=\"abc123\")\n\n# Replay with modifications\ndebugger.replay_execution(\n    run_id=\"abc123\",\n    override_inputs={\"step1\": {\"new_param\": \"value\"}}\n)\n</code></pre>"},{"location":"advanced/debugging/#quick-debugging-utilities","title":"Quick Debugging Utilities \ud83d\udee0\ufe0f","text":""},{"location":"advanced/debugging/#debug_step","title":"debug_step","text":"<pre><code>from flowyml.utils.debug import debug_step\n\n@step(outputs=[\"result\"])\ndef my_step(data):\n    return process(data)\n\n# Quick debug - prints inputs/outputs\nresult = debug_step(my_step, data=[1, 2, 3])\n# Output:\n# \ud83d\udd0d Debugging: my_step\n# \ud83d\udce5 Inputs: {'data': [1, 2, 3]}\n# \ud83d\udce4 Output: [2, 4, 6]\n# \u23f1\ufe0f Duration: 0.001s\n</code></pre>"},{"location":"advanced/debugging/#trace_step","title":"trace_step","text":"<pre><code>from flowyml.utils.debug import trace_step\n\n# Trace step execution\n@trace_step\n@step(outputs=[\"result\"])\ndef traced_step(data):\n    return process(data)\n\n# Automatically prints:\n# \u2192 Entering traced_step\n# \u2190 Exiting traced_step (0.001s)\n</code></pre>"},{"location":"advanced/debugging/#profile_step","title":"profile_step","text":"<pre><code>from flowyml.utils.debug import profile_step\n\n# Profile step performance\nstats = profile_step(my_step, data=[1, 2, 3])\nprint(f\"Execution time: {stats['time']:.3f}s\")\nprint(f\"Memory used: {stats['memory_mb']:.1f}MB\")\n</code></pre>"},{"location":"advanced/debugging/#best-practices","title":"Best Practices \ud83d\udca1","text":""},{"location":"advanced/debugging/#1-use-conditional-breakpoints","title":"1. Use Conditional Breakpoints","text":"<pre><code># Only break on interesting cases\ndebugger.add_breakpoint(\n    condition=lambda inputs: inputs['value'] &lt; 0,\n    action=lambda step, inputs: print(f\"\u26a0\ufe0f Negative value: {inputs['value']}\")\n)\n</code></pre>"},{"location":"advanced/debugging/#2-enable-tracing-for-production-issues","title":"2. Enable Tracing for Production Issues","text":"<pre><code># Enable tracing when issues occur\nif environment == \"production\":\n    if detect_anomaly():\n        debugger.enable_tracing()\n        result = pipeline.run()\n        trace = debugger.get_trace()\n        send_to_monitoring(trace)\n</code></pre>"},{"location":"advanced/debugging/#3-profile-before-optimization","title":"3. Profile Before Optimization","text":"<pre><code># Identify bottlenecks\ndebugger.enable_profiling()\nresult = pipeline.run()\n\nprofile = debugger.get_profile()\nslowest = max(profile.items(), key=lambda x: x[1]['duration_seconds'])\nprint(f\"Bottleneck: {slowest[0]} ({slowest[1]['duration_seconds']:.2f}s)\")\n</code></pre>"},{"location":"advanced/debugging/#4-combine-with-logging","title":"4. Combine with Logging","text":"<pre><code>import logging\n\nlogger = logging.getLogger(__name__)\n\ndebugger.add_breakpoint(\n    action=lambda step, inputs: logger.info(\n        f\"Executing {step.name} with {len(inputs)} inputs\"\n    )\n)\n</code></pre>"},{"location":"advanced/debugging/#advanced-usage","title":"Advanced Usage \u26a1","text":""},{"location":"advanced/debugging/#custom-breakpoint-actions","title":"Custom Breakpoint Actions","text":"<pre><code>def save_snapshot(step, inputs):\n    \"\"\"Save inputs to file for later analysis\"\"\"\n    import pickle\n    with open(f\"snapshot_{step.name}.pkl\", \"wb\") as f:\n        pickle.dump(inputs, f)\n    print(f\"\ud83d\udcbe Saved snapshot for {step.name}\")\n\ndebugger.add_breakpoint(\n    condition=lambda inputs: should_save(inputs),\n    action=save_snapshot\n)\n</code></pre>"},{"location":"advanced/debugging/#programmatic-error-handling","title":"Programmatic Error Handling","text":"<pre><code>class DebugHandler:\n    def __init__(self):\n        self.errors = []\n\n    def handle_error(self, step, inputs, exception):\n        self.errors.append({\n            'step': step.name,\n            'inputs': inputs,\n            'error': str(exception)\n        })\n        # Send alert\n        alert_team(f\"Error in {step.name}: {exception}\")\n\nhandler = DebugHandler()\n\ndebugger.add_breakpoint(\n    action=lambda step, inputs: handler.handle_error(step, inputs, None)\n)\n</code></pre>"},{"location":"advanced/debugging/#api-reference","title":"API Reference \ud83d\udcda","text":""},{"location":"advanced/debugging/#stepdebugger","title":"StepDebugger","text":"<pre><code>StepDebugger(step: Step)\n</code></pre> <p>Methods: - <code>debug_execute(**inputs) -&gt; Any</code> - Execute with debugging - <code>inspect_inputs(**inputs) -&gt; Dict</code> - Inspect inputs - <code>inspect_output() -&gt; Any</code> - Inspect last output - <code>add_breakpoint(action, condition=None)</code> - Add breakpoint - <code>clear_breakpoints()</code> - Remove all breakpoints - <code>debug_exception(exception)</code> - Analyze exception</p>"},{"location":"advanced/debugging/#pipelinedebugger","title":"PipelineDebugger","text":"<pre><code>PipelineDebugger(pipeline: Pipeline)\n</code></pre> <p>Methods: - <code>enable_tracing()</code> - Enable execution tracing - <code>enable_profiling()</code> - Enable performance profiling - <code>get_trace() -&gt; List[Dict]</code> - Get execution trace - <code>get_profile() -&gt; Dict</code> - Get performance profile - <code>visualize_dag(output_path: str)</code> - Visualize pipeline - <code>analyze_errors(result) -&gt; Dict</code> - Analyze failed run - <code>replay_execution(run_id, override_inputs=None)</code> - Replay run</p>"},{"location":"advanced/debugging/#utility-functions","title":"Utility Functions","text":"<p>debug_step(step, inputs)**: - Executes step with detailed logging - Returns: step output</p> <p>trace_step(func): - Decorator for automatic tracing - Prints entry/exit with timing</p> <p>profile_step(step, inputs)**: - Profiles step execution - Returns: Dict with timing and memory stats</p>"},{"location":"advanced/error-handling/","title":"Error Handling &amp; Retries \ud83d\udee1\ufe0f","text":"<p>Build self-healing pipelines that recover from failures automatically.</p> <p>[!NOTE] What you'll learn: How to make pipelines resilient to network blips, API timeouts, and transient errors</p> <p>Key insight: In distributed systems, failure is inevitable. Your pipeline should handle it, not crash.</p>"},{"location":"advanced/error-handling/#why-robustness-matters","title":"Why Robustness Matters","text":"<p>Without error handling: - Fragile pipelines: One network timeout kills the whole job - Manual restarts: Waking up at 3 AM to click \"retry\" - Data loss: Partial failures leave data in inconsistent states</p> <p>With flowyml resilience: - Self-healing: Transient errors are retried automatically - Fail-fast: Circuit breakers stop cascading failures - Graceful degradation: Fallbacks provide default values when services fail</p>"},{"location":"advanced/error-handling/#decision-guide-resilience-patterns","title":"Decision Guide: Resilience Patterns","text":"Pattern Use When Example Retry Transient errors: Network blips, rate limits API timeout, 503 error Circuit Breaker System outages: Service is down hard Database down, API 500 loop Fallback Critical path: Must continue even if step fails Use cached data if live API fails"},{"location":"advanced/error-handling/#retries","title":"\ud83d\udd04 Retries","text":"<p>Automatically retry failed steps with configurable backoff strategies.</p>"},{"location":"advanced/error-handling/#real-world-pattern-the-flaky-api","title":"Real-World Pattern: The Flaky API","text":"<p>Handle APIs that randomly fail or rate limit you.</p> <pre><code>from flowyml import step, retry, ExponentialBackoff\n\n@step(\n    retry=retry(\n        max_attempts=5,\n        backoff=ExponentialBackoff(initial=1.0, multiplier=2.0),\n        on=[NetworkError, TimeoutError, RateLimitError]\n    )\n)\ndef fetch_data():\n    # Attempt 1: Fail\n    # Wait 1s...\n    # Attempt 2: Fail\n    # Wait 2s...\n    # Attempt 3: Success!\n    return api.get_data()\n</code></pre>"},{"location":"advanced/error-handling/#circuit-breakers","title":"\ud83d\udd0c Circuit Breakers","text":"<p>Prevent cascading failures by \"opening the circuit\" when a service is down, failing fast instead of waiting for timeouts.</p> <pre><code>from flowyml import step, CircuitBreaker\n\n@step(\n    circuit_breaker=CircuitBreaker(\n        failure_threshold=3,\n        timeout=60  # Wait 60s before trying again\n    )\n)\ndef call_unstable_api():\n    return external_service.call()\n</code></pre>"},{"location":"advanced/error-handling/#fallbacks","title":"\ud83d\udee1\ufe0f Fallbacks","text":"<p>Define a fallback function to execute when a step fails, ensuring the pipeline can continue.</p> <pre><code>def load_cached_data():\n    return cache.get(\"latest_data\")\n\n@step(\n    fallback=load_cached_data,\n    fallback_on=[TimeoutError]\n)\ndef fetch_live_data():\n    return api.get_live_data()\n</code></pre>"},{"location":"advanced/error-handling/#failure-handlers","title":"\ud83d\udea8 Failure Handlers","text":"<p>Configure actions to take when a step fails (e.g., send alerts).</p> <pre><code>from flowyml import step, on_failure\n\n@step(\n    on_failure=on_failure(\n        action=\"slack\",\n        recipients=[\"#ml-alerts\"],\n        include_logs=True\n    )\n)\ndef critical_step():\n    # ...\n</code></pre>"},{"location":"advanced/human-in-the-loop/","title":"Human-in-the-Loop &amp; Approvals \u270b","text":"<p>Inject human intelligence into automated workflows. Pause pipelines for manual review, safety checks, or compliance approvals.</p> <p>[!NOTE] What you'll learn: How to stop automation when a human decision is required</p> <p>Key insight: Not everything should be automated. Deployment to production often needs a human \"thumbs up.\"</p>"},{"location":"advanced/human-in-the-loop/#why-human-review-matters","title":"Why Human Review Matters","text":"<p>Use cases: - Safety: \"Does this model output look safe?\" - Compliance: \"Has Legal approved this dataset?\" - Quality Assurance: \"Is the generated image high quality?\" - Cost Control: \"Approve spending $500 on this training run?\"</p>"},{"location":"advanced/human-in-the-loop/#approval-steps","title":"\u270b Approval Steps","text":"<p>You can insert an <code>approval</code> step anywhere in your pipeline.</p>"},{"location":"advanced/human-in-the-loop/#real-world-pattern-the-deployment-gate","title":"Real-World Pattern: The Deployment Gate","text":"<p>The classic MLOps pattern: Train automatically, deploy manually.</p> <pre><code>from flowyml import Pipeline, step, approval\n\npipeline = Pipeline(\"deployment_pipeline\")\n\n@step\ndef train():\n    return model\n\n# Pause here!\napprove_deploy = approval(\n    name=\"approve_deploy\",\n    approver=\"lead-data-scientist\",\n    timeout_seconds=86400  # 24 hours to approve\n)\n\n@step\ndef deploy(model):\n    # Only runs if approved\n    production.deploy(model)\n\npipeline.add_step(train)\npipeline.add_step(approve_deploy)\npipeline.add_step(deploy)\n</code></pre>"},{"location":"advanced/human-in-the-loop/#interactive-approval","title":"Interactive Approval","text":"<p>When running locally (CLI), the pipeline will pause and prompt the user:</p> <pre><code>$ flowyml run deployment_pipeline\n\n...\n[INFO] Step 'train' completed.\n[WARN] \u270b Step 'approve_deploy' requires approval.\n       Waiting for approval from: ml-team\n       Timeout: 3600s\n       Approve execution? [y/N]:\n</code></pre>"},{"location":"advanced/human-in-the-loop/#auto-approval-logic","title":"Auto-Approval Logic","text":"<p>You can define conditions for automatic approval, useful for CI/CD environments.</p> <pre><code>approve_deploy = approval(\n    name=\"approve_deploy\",\n    approver=\"ml-team\",\n    auto_approve_if=lambda: os.getenv(\"ENVIRONMENT\") == \"staging\"\n)\n</code></pre>"},{"location":"advanced/human-in-the-loop/#notifications","title":"\ud83d\udce7 Notifications","text":"<p>Approvers can be notified via configured channels (Email, Slack) when their attention is required. See Notifications for setup.</p>"},{"location":"advanced/llm-tracing/","title":"GenAI &amp; LLM Tracing \ud83d\udd75\ufe0f","text":"<p>flowyml provides built-in observability for Large Language Models (LLMs), giving you X-ray vision into your GenAI applications.</p> <p>[!NOTE] What you'll learn: How to track token usage, costs, and latency for every LLM call</p> <p>Key insight: LLMs are black boxes. Tracing turns them into transparent, measurable components.</p>"},{"location":"advanced/llm-tracing/#why-tracing-matters","title":"Why Tracing Matters","text":"<p>Without tracing: - Hidden costs: \"Why is our OpenAI bill $500 this month?\" - Latency spikes: \"Why is the chatbot taking 10 seconds?\" - Quality issues: \"What exact prompt caused this hallucination?\"</p> <p>With flowyml tracing: - Cost transparency: See cost per call, per user, or per pipeline - Performance metrics: Pinpoint slow steps in your RAG chain - Full context: See the exact prompt and completion for every interaction</p>"},{"location":"advanced/llm-tracing/#llm-call-tracing","title":"\ud83d\udd75\ufe0f LLM Call Tracing","text":"<p>You can trace any function as an LLM call or a chain of calls using the <code>@trace_llm</code> decorator. flowyml automatically captures inputs, outputs, and metadata.</p>"},{"location":"advanced/llm-tracing/#llm-call-tracing_1","title":"\ud83d\udd75\ufe0f LLM Call Tracing","text":"<p>You can trace any function as an LLM call or a chain of calls using the <code>@trace_llm</code> decorator.</p>"},{"location":"advanced/llm-tracing/#basic-usage","title":"Basic Usage","text":"<pre><code>from flowyml import trace_llm\nimport openai\n\n@trace_llm(name=\"text_generation\")\ndef generate_text(prompt: str):\n    response = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return response.choices[0].message.content\n\n# This call will be automatically traced and logged\nresult = generate_text(\"Write a haiku about ML pipelines\")\n</code></pre>"},{"location":"advanced/llm-tracing/#real-world-pattern-rag-pipeline","title":"Real-World Pattern: RAG Pipeline","text":"<p>Trace a complete Retrieval Augmented Generation (RAG) workflow to see where time is spent.</p> <pre><code>from flowyml import trace_llm\n\n@trace_llm(name=\"rag_chain\", event_type=\"chain\")\ndef rag_pipeline(query: str):\n    # 1. Retrieve context (Tool)\n    context = retrieve_context(query)\n\n    # 2. Generate answer (LLM)\n    answer = generate_answer(query, context)\n    return answer\n\n@trace_llm(name=\"retrieval\", event_type=\"tool\")\ndef retrieve_context(query: str):\n    # Simulate vector DB lookup\n    return \"flowyml documentation...\"\n\n@trace_llm(name=\"generation\", event_type=\"llm\", model=\"gpt-4\")\ndef generate_answer(query: str, context: str):\n    # This call's tokens and cost will be tracked\n    return openai.ChatCompletion.create(\n        model=\"gpt-4\",\n        messages=[\n            {\"role\": \"system\", \"content\": f\"Context: {context}\"},\n            {\"role\": \"user\", \"content\": query}\n        ]\n    )\n</code></pre> <p>[!TIP] Pro Tip: Use <code>event_type=\"chain\"</code> for the parent function and <code>\"llm\"</code> or <code>\"tool\"</code> for children. This creates a beautiful nested waterfall view in the UI.</p>"},{"location":"advanced/llm-tracing/#viewing-traces","title":"\ud83d\udcca Viewing Traces","text":"<p>Traces are automatically persisted to the metadata store and can be visualized in the flowyml UI.</p>"},{"location":"advanced/llm-tracing/#in-the-ui","title":"In the UI","text":"<p>Navigate to the Traces tab in the flowyml Dashboard (<code>http://localhost:8080/traces</code>). You will see:</p> <ul> <li>Timeline View: A waterfall chart of your traces.</li> <li>Latency &amp; Cost: Aggregated metrics for each trace.</li> <li>Inputs &amp; Outputs: Full inspection of prompts and completions.</li> <li>Token Usage: Detailed breakdown of prompt vs. completion tokens.</li> </ul>"},{"location":"advanced/llm-tracing/#programmatic-access","title":"Programmatic Access","text":"<p>You can also retrieve traces via the Python API for analysis.</p> <pre><code>from flowyml.storage.metadata import SQLiteMetadataStore\n\nstore = SQLiteMetadataStore()\ntrace = store.get_trace(trace_id=\"&lt;trace_id&gt;\")\n\nprint(f\"Latency: {trace.latency}ms\")\nprint(f\"Tokens: {trace.total_tokens}\")\n</code></pre>"},{"location":"advanced/llm-tracing/#trace-attributes","title":"\ud83c\udff7\ufe0f Trace Attributes","text":"<p>You can add custom attributes to your traces for better filtering and analysis.</p> <pre><code>@trace_llm(name=\"categorize\", attributes={\"model_version\": \"v2\", \"temperature\": 0.7})\ndef categorize_text(text):\n    # ...\n</code></pre>"},{"location":"advanced/materializers/","title":"Materializers \ud83d\udce6","text":"<p>Teach flowyml how to save and load your custom objects.</p> <p>[!NOTE] What you'll learn: How to make any Python object persistable and trackable</p> <p>Key insight: If you can't save it, you can't cache it, version it, or inspect it. Materializers bridge the gap between memory and storage.</p>"},{"location":"advanced/materializers/#why-custom-serialization-matters","title":"Why Custom Serialization Matters","text":"<p>Without materializers: - Pickle hell: Relying on <code>pickle</code> for everything (brittle, insecure) - Lost metadata: Saving a model as bytes loses its hyperparameters - No visualization: The UI can't show a preview of a custom object</p> <p>With flowyml materializers: - Optimized storage: Save large arrays as Parquet/Numpy, not JSON - Rich visualization: Tell the UI how to display your object - Cross-language support: Save as standard formats (ONNX, CSV) usable by other tools</p>"},{"location":"advanced/materializers/#built-in-materializers","title":"\ud83d\udce6 Built-in Materializers","text":"<p>flowyml automatically selects the appropriate materializer based on the type hint or object type.</p> <ul> <li>PandasMaterializer: Parquet or CSV.</li> <li>NumpyMaterializer: <code>.npy</code> files.</li> <li>JsonMaterializer: JSON files.</li> <li>PickleMaterializer: Fallback for arbitrary Python objects.</li> </ul>"},{"location":"advanced/materializers/#custom-materializers","title":"\ud83d\udee0 Custom Materializers","text":"<p>To support a custom type, subclass <code>BaseMaterializer</code>.</p>"},{"location":"advanced/materializers/#real-world-pattern-pytorch-model-wrapper","title":"Real-World Pattern: PyTorch Model Wrapper","text":"<p>Save PyTorch models with their metadata in a clean, versioned way.</p> <pre><code>import torch\nfrom flowyml.io import BaseMaterializer\n\nclass PyTorchMaterializer(BaseMaterializer):\n    ASSOCIATED_TYPES = (torch.nn.Module,)\n\n    def handle_input(self, data_type):\n        # Load model state dict\n        with open(self.artifact.uri, 'rb') as f:\n            return torch.load(f)\n\n    def handle_return(self, model):\n        # Save model state dict\n        with open(self.artifact.uri, 'wb') as f:\n            torch.save(model, f)\n\n# Register it once\nfrom flowyml import materializer_registry\nmaterializer_registry.register(PyTorchMaterializer)\n</code></pre>"},{"location":"advanced/materializers/#usage","title":"\ud83c\udfaf Usage","text":"<p>Once registered, flowyml will automatically use your materializer when a step returns a <code>CustomGraph</code> object.</p> <pre><code>@step\ndef build_graph() -&gt; CustomGraph:\n    return CustomGraph(...)\n</code></pre>"},{"location":"advanced/model-leaderboard/","title":"Model Leaderboard \ud83c\udfc6","text":"<p>flowyml automatically tracks and ranks your models, so you always know which one performs best.</p> <p>[!NOTE] What you'll learn: How to compare models across experiments and automatically pick the winner</p> <p>Key insight: Don't track model performance in spreadsheets. Let the framework do it.</p>"},{"location":"advanced/model-leaderboard/#why-leaderboards-matter","title":"Why Leaderboards Matter","text":"<p>Without a leaderboard: - Manual tracking: \"Was run_42 better than run_38?\" - Lost history: \"What hyperparameters did we use for the best model last month?\" - Subjective choices: Picking models based on gut feeling rather than metrics</p> <p>With flowyml leaderboard: - Automated ranking: Always know the SOTA model for your task - Metric-driven: Compare on accuracy, F1, latency, or custom metrics - Full lineage: Click any score to see the exact code and data that produced it</p>"},{"location":"advanced/model-leaderboard/#using-the-leaderboard","title":"\ud83c\udfc6 Using the Leaderboard","text":"<p>The <code>ModelLeaderboard</code> class allows you to rank models based on specific metrics.</p> <pre><code>from flowyml.tracking import ModelLeaderboard\n\n# Initialize leaderboard for 'accuracy'\nleaderboard = ModelLeaderboard(metric=\"accuracy\", higher_is_better=True)\n\n# Add a score from a run\nleaderboard.add_score(\n    model_name=\"resnet50\",\n    run_id=\"run_123\",\n    score=0.95,\n    metadata={\"epochs\": 10}\n)\n\n## Real-World Pattern: Auto-Promotion\n\nAutomatically fetch the best model from history to use as a baseline or for deployment.\n\n```python\nfrom flowyml.tracking import ModelLeaderboard\n\n# 1. Get the current champion\nleaderboard = ModelLeaderboard(metric=\"accuracy\")\nbest_run = leaderboard.get_best_run()\n\nprint(f\"Current Champion: {best_run.run_id} (Acc: {best_run.score:.4f})\")\n\n# 2. Compare with new candidate\nif new_model_score &gt; best_run.score:\n    print(\"\ud83d\ude80 New Champion! Promoting to production...\")\n    deploy(new_model)\nelse:\n    print(\"\u274c Failed to beat baseline.\")\n</code></pre>"},{"location":"advanced/model-leaderboard/#comparison","title":"\ud83d\udcca Comparison","text":"<p>You can also compare specific runs side-by-side.</p> <pre><code>from flowyml.tracking import compare_runs\n\ndiff = compare_runs([\"run_1\", \"run_2\"])\nprint(diff)\n</code></pre>"},{"location":"advanced/model-leaderboard/#ui-view","title":"\ud83d\udda5\ufe0f UI View","text":"<p>The flowyml Dashboard provides a rich interactive leaderboard where you can: - Sort by any metric. - Filter by tags or date. - Click to see detailed run configuration.</p>"},{"location":"advanced/notifications/","title":"Notifications &amp; Alerts \ud83d\udd14","text":"<p>Know immediately when pipelines succeed, fail, or detect issues.</p> <p>[!NOTE] What you'll learn: How to set up proactive alerts so you don't have to watch dashboards all day</p> <p>Key insight: The faster you know about a failure, the faster you can fix it.</p>"},{"location":"advanced/notifications/#why-alerts-matter","title":"Why Alerts Matter","text":"<p>Without alerts: - Silent failures: A nightly job fails, and no one notices until the dashboard is empty in the morning - Dashboard fatigue: Constantly refreshing the UI to check status - Delayed response: Critical production issues persist for hours</p> <p>With flowyml alerts: - Instant notification: Slack ping the moment an exception is thrown - Contextual info: \"Pipeline X failed at step Y with error Z\" - Multi-channel: Email for summaries, Slack for urgent issues</p>"},{"location":"advanced/notifications/#configuration","title":"\ud83d\udd14 Configuration","text":"<p>Configure notifications globally or per-pipeline.</p> <pre><code>from flowyml import configure_notifications\n\nconfigure_notifications(\n    console=True,\n    slack_webhook=\"https://hooks.slack.com/services/...\",\n    email_config={\n        'smtp_host': 'smtp.gmail.com',\n        'username': 'user@example.com',\n        'password': 'app-password',\n        'to_addrs': ['team@example.com']\n    }\n)\n</code></pre>"},{"location":"advanced/notifications/#sending-notifications","title":"\ud83d\udce8 Sending Notifications","text":"<p>You can send manual notifications from any step.</p>"},{"location":"advanced/notifications/#real-world-pattern-the-success-ping","title":"Real-World Pattern: The \"Success\" Ping","text":"<p>Notify the team when a long training run finishes successfully.</p> <pre><code>from flowyml import step, get_notifier\n\n@step\ndef notify_success(metrics):\n    notifier = get_notifier()\n\n    # Send a rich message to Slack\n    notifier.notify(\n        title=\"\ud83d\ude80 Model Training Complete\",\n        message=f\"New model ready!\\nAccuracy: {metrics['acc']:.2%}\\nF1: {metrics['f1']:.2f}\",\n        level=\"success\",\n        channels=[\"slack\"]\n    )\n</code></pre>"},{"location":"advanced/notifications/#event-hooks","title":"\ud83d\udea8 Event Hooks","text":"<p>Automatically trigger notifications on specific events.</p> <pre><code>notifier.on_pipeline_failure(\"training_pipeline\", run_id, error=\"OOM\")\nnotifier.on_drift_detected(\"feature_x\", psi=0.45)\n</code></pre>"},{"location":"advanced/notifications/#custom-channels","title":"\ud83d\udee0 Custom Channels","text":"<p>Implement <code>NotificationChannel</code> to support other services (Discord, PagerDuty, etc.).</p> <pre><code>from flowyml.monitoring.notifications import NotificationChannel, Notification\n\nclass DiscordChannel(NotificationChannel):\n    def send(self, notification: Notification) -&gt; bool:\n        # Send to Discord webhook\n        return True\n</code></pre>"},{"location":"advanced/parallel/","title":"Parallel Execution \ud83d\ude80","text":"<p>flowyml allows you to execute independent steps concurrently, slashing pipeline runtime by running tasks in parallel.</p> <p>[!NOTE] What you'll learn: How to run multiple steps at once to speed up execution</p> <p>Key insight: Most ML pipelines have independent branches (e.g., processing different datasets). Running them sequentially is a waste of time.</p>"},{"location":"advanced/parallel/#why-parallelism-matters","title":"Why Parallelism Matters","text":"<p>Without parallelism: - Slow execution: Steps run one after another (A \u2192 B \u2192 C) - Idle resources: CPU cores sit idle while one core works - Long feedback loops: Waiting hours for independent tasks</p> <p>With flowyml parallelism: - Faster results: Run A, B, and C at the same time - Resource efficiency: Utilize all CPU cores - Scalability: Process 10x data in the same amount of time</p>"},{"location":"advanced/parallel/#enabling-parallelism","title":"Enabling Parallelism","text":"<p>You can enable parallel execution by using the <code>ParallelExecutor</code>. It automatically detects independent steps in your DAG and runs them concurrently.</p>"},{"location":"advanced/parallel/#basic-usage","title":"Basic Usage","text":"<pre><code>from flowyml import Pipeline, ParallelExecutor, step\n\n@step\ndef process_chunk(chunk_id):\n    # Simulate work\n    return f\"processed_{chunk_id}\"\n\npipeline = Pipeline(\"parallel_processing\")\n\n# Add multiple independent steps\nfor i in range(5):\n    pipeline.add_step(process_chunk, name=f\"chunk_{i}\", params={\"chunk_id\": i})\n\n# Configure executor\nexecutor = ParallelExecutor(max_workers=4)\n\n# Run pipeline with executor\npipeline.run(executor=executor)\n</code></pre>"},{"location":"advanced/parallel/#in-step-parallelism","title":"In-Step Parallelism","text":"<p>You can also parallelize work within a single step using standard Python libraries or flowyml's utilities.</p> <pre><code>from flowyml.utils.parallel import parallel_map\n\n@step\ndef batch_process(items):\n    # Process items in parallel using a thread pool\n    results = parallel_map(process_item, items, max_workers=8)\n    return results\n</code></pre>"},{"location":"advanced/parallel/#decision-guide-execution-backends","title":"Decision Guide: Execution Backends","text":"Backend Best For Why <code>process</code> (Default) CPU-bound tasks Bypasses Python's GIL. Good for data processing, training. <code>thread</code> I/O-bound tasks Lightweight. Good for API calls, downloading files, DB queries."},{"location":"advanced/parallel/#when-to-use-process-cpu","title":"When to use <code>process</code> (CPU)","text":"<ul> <li>Data transformation (pandas, numpy)</li> <li>Image processing</li> <li>Model training (sklearn)</li> </ul>"},{"location":"advanced/parallel/#when-to-use-thread-io","title":"When to use <code>thread</code> (I/O)","text":"<ul> <li>Fetching data from APIs</li> <li>Uploading files to S3</li> <li>Querying databases</li> </ul>"},{"location":"advanced/parallel/#real-world-pattern-batch-processing","title":"Real-World Pattern: Batch Processing","text":"<p>Process a large dataset by splitting it into chunks and processing them in parallel.</p> <pre><code>from flowyml import Pipeline, ParallelExecutor, step\n\n@step\ndef process_chunk(chunk_path):\n    # Heavy CPU work\n    df = pd.read_csv(chunk_path)\n    return df.mean()\n\npipeline = Pipeline(\"batch_processor\")\n\n# Add 10 parallel steps\nfor i in range(10):\n    pipeline.add_step(\n        process_chunk,\n        name=f\"chunk_{i}\",\n        params={\"chunk_path\": f\"data/part_{i}.csv\"}\n    )\n\n# Run with 4 processes\npipeline.run(executor=ParallelExecutor(max_workers=4, backend=\"process\"))\n</code></pre> <p>[!TIP] Performance Tip: Set <code>max_workers</code> to <code>cpu_count() - 1</code> to keep one core free for the system.</p>"},{"location":"advanced/step-grouping/","title":"Step Grouping","text":"<p>Efficiently execute multiple steps in the same environment/container</p>"},{"location":"advanced/step-grouping/#overview","title":"Overview","text":"<p>Step grouping allows you to run multiple pipeline steps within the same Docker container or remote executor, significantly reducing overhead and improving efficiency. This is particularly useful when you have several logically related steps that would benefit from sharing an execution environment.</p>"},{"location":"advanced/step-grouping/#quick-start","title":"Quick Start","text":"<pre><code>from flowyml import step, Pipeline\nfrom flowyml.core.resources import ResourceRequirements, GPUConfig\n\n# Define steps with the same execution_group\n@step(outputs=[\"raw_data\"], execution_group=\"preprocessing\")\ndef load_data():\n    return {\"records\": 1000}\n\n@step(inputs=[\"raw_data\"], outputs=[\"clean_data\"], execution_group=\"preprocessing\")\ndef clean_data(raw_data: dict):\n    return {**raw_data, \"status\": \"cleaned\"}\n\n@step(inputs=[\"clean_data\"], outputs=[\"features\"], execution_group=\"preprocessing\")\ndef extract_features(clean_data: dict):\n    return {**clean_data, \"features\": [\"f1\", \"f2\", \"f3\"]}\n\n# Create and run pipeline\npipeline = Pipeline(\"data_pipeline\")\npipeline.add_step(load_data)\npipeline.add_step(clean_data)\npipeline.add_step(extract_features)\n\n# All three steps run in the same container\nresult = pipeline.run()\n</code></pre>"},{"location":"advanced/step-grouping/#how-it-works","title":"How It Works","text":""},{"location":"advanced/step-grouping/#1-define-groups","title":"1. Define Groups","text":"<p>Add the <code>execution_group</code> parameter to steps you want to group together:</p> <pre><code>@step(outputs=[\"data\"], execution_group=\"my_group\")\ndef step_1():\n    return \"data\"\n\n@step(inputs=[\"data\"], outputs=[\"result\"], execution_group=\"my_group\")\ndef step_2(data: str):\n    return f\"{data}_processed\"\n</code></pre>"},{"location":"advanced/step-grouping/#2-automatic-analysis","title":"2. Automatic Analysis","text":"<p>flowyml automatically: - \u2705 Analyzes the pipeline DAG - \u2705 Groups consecutive steps with the same <code>execution_group</code> - \u2705 Splits non-consecutive steps into subgroups - \u2705 Aggregates resource requirements</p>"},{"location":"advanced/step-grouping/#3-efficient-execution","title":"3. Efficient Execution","text":"<p>Grouped steps: - Run sequentially in the same process/container - Share the same environment and dependencies - Pass outputs directly without serialization overhead - Use aggregated resources (max CPU, memory, GPU)</p>"},{"location":"advanced/step-grouping/#resource-aggregation","title":"Resource Aggregation","text":"<p>When steps are grouped, their resource requirements are intelligently aggregated:</p> <pre><code>@step(\n    outputs=[\"data\"],\n    execution_group=\"training\",\n    resources=ResourceRequirements(cpu=\"2\", memory=\"4Gi\")\n)\ndef prepare_data():\n    return \"data\"\n\n@step(\n    inputs=[\"data\"],\n    outputs=[\"model\"],\n    execution_group=\"training\",\n    resources=ResourceRequirements(\n        cpu=\"8\",\n        memory=\"16Gi\",\n        gpu=GPUConfig(gpu_type=\"nvidia-v100\", count=2)\n    )\n)\ndef train_model(data: str):\n    return \"model\"\n\n# Group resources: cpu=\"8\", memory=\"16Gi\", gpu=V100 x2\n</code></pre> <p>Aggregation Strategy: - CPU: Maximum across all steps - Memory: Maximum across all steps - Storage: Maximum across all steps - GPU:   - Count: Maximum   - Type: Best GPU (A100 &gt; V100 &gt; T4)   - Memory: Maximum - Node Affinity: Union of constraints</p>"},{"location":"advanced/step-grouping/#sequential-analysis","title":"Sequential Analysis","text":""},{"location":"advanced/step-grouping/#consecutive-steps-grouped","title":"Consecutive Steps (Grouped)","text":"<pre><code># A \u2192 B \u2192 C (all in same group) \u2705 Grouped together\n@step(outputs=[\"a\"], execution_group=\"proc\")\ndef step_a(): return 1\n\n@step(inputs=[\"a\"], outputs=[\"b\"], execution_group=\"proc\")\ndef step_b(a): return a + 1\n\n@step(inputs=[\"b\"], outputs=[\"c\"], execution_group=\"proc\")\ndef step_c(b): return b + 1\n</code></pre>"},{"location":"advanced/step-grouping/#non-consecutive-steps-split","title":"Non-Consecutive Steps (Split)","text":"<pre><code># A (group1) \u2192 X (no group) \u2192 B (group1)\n# Result: A and B run separately (not consecutive)\n\n@step(outputs=[\"a\"], execution_group=\"group1\")\ndef step_a(): return 1\n\n@step(outputs=[\"x\"])  # Different group\ndef step_x(): return 100\n\n@step(inputs=[\"x\"], outputs=[\"b\"], execution_group=\"group1\")\ndef step_b(x): return x + 1\n</code></pre>"},{"location":"advanced/step-grouping/#advanced-features","title":"Advanced Features","text":""},{"location":"advanced/step-grouping/#mixed-grouped-and-ungrouped-steps","title":"Mixed Grouped and Ungrouped Steps","text":"<pre><code># Some steps grouped, others standalone\n@step(outputs=[\"config\"])  # Standalone\ndef load_config():\n    return {\"batch_size\": 32}\n\n@step(inputs=[\"config\"], outputs=[\"data\"], execution_group=\"prep\")\ndef fetch_data(config):\n    return \"data\"\n\n@step(inputs=[\"data\"], outputs=[\"processed\"], execution_group=\"prep\")\ndef process_data(data):\n    return \"processed\"\n\n@step(inputs=[\"processed\"], outputs=[\"deployed\"])  # Standalone\ndef deploy(processed):\n    return \"deployed\"\n</code></pre>"},{"location":"advanced/step-grouping/#compatibility-with-all-features","title":"Compatibility with All Features","text":"<p>Step grouping works seamlessly with:</p> <ul> <li>\u2705 Caching (all strategies: <code>code_hash</code>, <code>input_hash</code>, <code>False</code>)</li> <li>\u2705 Retries (per-step retry configuration)</li> <li>\u2705 Conditional execution (condition functions)</li> <li>\u2705 Versioned pipelines (<code>VersionedPipeline</code>)</li> <li>\u2705 Resource specifications (dict or <code>ResourceRequirements</code>)</li> <li>\u2705 GPU scheduling</li> <li>\u2705 Node affinity and tolerations</li> </ul>"},{"location":"advanced/step-grouping/#best-practices","title":"Best Practices","text":""},{"location":"advanced/step-grouping/#do-group-related-steps","title":"\u2705 DO: Group Related Steps","text":"<pre><code># Good: Logically related preprocessing steps\n@step(outputs=[\"raw\"], execution_group=\"preprocessing\")\ndef load(): ...\n\n@step(inputs=[\"raw\"], outputs=[\"clean\"], execution_group=\"preprocessing\")\ndef clean(): ...\n\n@step(inputs=[\"clean\"], outputs=[\"features\"], execution_group=\"preprocessing\")\ndef extract(): ...\n</code></pre>"},{"location":"advanced/step-grouping/#dont-group-unrelated-steps","title":"\u274c DON'T: Group Unrelated Steps","text":"<pre><code># Bad: Unrelated steps that could run in parallel\n@step(outputs=[\"data1\"], execution_group=\"bad_group\")\ndef load_dataset_a(): ...\n\n@step(outputs=[\"data2\"], execution_group=\"bad_group\")  # Independent!\ndef load_dataset_b(): ...\n</code></pre>"},{"location":"advanced/step-grouping/#do-use-for-cost-optimization","title":"\u2705 DO: Use for Cost Optimization","text":"<pre><code># Good: Multiple lightweight steps share expensive GPU instance\n@step(outputs=[\"data\"], execution_group=\"gpu_tasks\",\n      resources=ResourceRequirements(gpu=GPUConfig(\"nvidia-a100\", count=1)))\ndef prepare_on_gpu(): ...\n\n@step(inputs=[\"data\"], outputs=[\"result\"], execution_group=\"gpu_tasks\")\ndef inference_on_gpu(data): ...\n\n# Both steps reuse the same A100 instance\n</code></pre>"},{"location":"advanced/step-grouping/#inspecting-groups","title":"Inspecting Groups","text":"<p>After building a pipeline, you can inspect the created groups:</p> <pre><code>pipeline = Pipeline(\"my_pipeline\")\npipeline.add_step(step_a)\npipeline.add_step(step_b)\npipeline.add_step(step_c)\npipeline.build()\n\n# Check groups\nprint(f\"Number of groups: {len(pipeline.step_groups)}\")\n\nfor group in pipeline.step_groups:\n    print(f\"\\nGroup: {group.group_name}\")\n    print(f\"  Steps: {[s.name for s in group.steps]}\")\n    print(f\"  Execution order: {group.execution_order}\")\n    if group.aggregated_resources:\n        print(f\"  CPU: {group.aggregated_resources.cpu}\")\n        print(f\"  Memory: {group.aggregated_resources.memory}\")\n        if group.aggregated_resources.gpu:\n            print(f\"  GPU: {group.aggregated_resources.gpu.count}x {group.aggregated_resources.gpu.gpu_type}\")\n</code></pre>"},{"location":"advanced/step-grouping/#examples","title":"Examples","text":""},{"location":"advanced/step-grouping/#example-1-data-pipeline","title":"Example 1: Data Pipeline","text":"<pre><code>@step(outputs=[\"raw\"], execution_group=\"etl\")\ndef extract():\n    return fetch_from_source()\n\n@step(inputs=[\"raw\"], outputs=[\"transformed\"], execution_group=\"etl\")\ndef transform(raw):\n    return apply_transformations(raw)\n\n@step(inputs=[\"transformed\"], outputs=[\"loaded\"], execution_group=\"etl\")\ndef load(transformed):\n    save_to_warehouse(transformed)\n    return \"success\"\n</code></pre>"},{"location":"advanced/step-grouping/#example-2-ml-training-pipeline","title":"Example 2: ML Training Pipeline","text":"<pre><code>@step(\n    outputs=[\"dataset\"],\n    execution_group=\"training\",\n    resources=ResourceRequirements(cpu=\"4\", memory=\"8Gi\")\n)\ndef prepare_dataset():\n    return load_and_preprocess()\n\n@step(\n    inputs=[\"dataset\"],\n    outputs=[\"model\"],\n    execution_group=\"training\",\n    resources=ResourceRequirements(\n        cpu=\"8\",\n        memory=\"32Gi\",\n        gpu=GPUConfig(\"nvidia-a100\", count=2)\n    )\n)\ndef train_model(dataset):\n    return train(dataset, epochs=100)\n\n@step(\n    inputs=[\"model\"],\n    outputs=[\"metrics\"],\n    execution_group=\"training\",\n    resources=ResourceRequirements(cpu=\"2\", memory=\"4Gi\")\n)\ndef evaluate_model(model):\n    return evaluate(model)\n\n# Entire training workflow runs in one A100 instance\n# Resources: cpu=\"8\", memory=\"32Gi\", gpu=2x A100\n</code></pre>"},{"location":"advanced/step-grouping/#see-also","title":"See Also","text":"<ul> <li>Resource Specification - Define resource requirements</li> <li>Pipeline Execution - Understand pipeline execution</li> <li>Caching - Optimize with caching strategies</li> <li>Examples - Full working examples</li> </ul>"},{"location":"advanced/templates/","title":"Pipeline Templates \ud83d\udccb","text":"<p>Standardize your ML workflows with reusable templates. Define the \"Golden Path\" for your team.</p> <p>[!NOTE] What you'll learn: How to create reusable pipeline blueprints that enforce best practices</p> <p>Key insight: Don't copy-paste code. Use templates to ensure every project starts with the right structure, logging, and error handling.</p>"},{"location":"advanced/templates/#why-templates-matter","title":"Why Templates Matter","text":"<p>Without templates: - Inconsistency: Team A uses <code>pandas</code>, Team B uses <code>polars</code>, Team C writes raw SQL - Boilerplate: Rewriting the same setup code for every new project - Maintenance nightmare: Updating a best practice requires editing 50 repos</p> <p>With flowyml templates: - Standardization: \"Use the <code>training_template</code>\" ensures everyone logs metrics the same way - Speed: Start a new project in seconds, not hours - Governance: Bake compliance checks into the base template</p>"},{"location":"advanced/templates/#using-built-in-templates","title":"\ud83d\udccb Using Built-in Templates","text":"<p>flowyml comes with several built-in templates for common ML patterns.</p> <pre><code>from flowyml import create_from_template\n\n# Create a standard training pipeline\npipeline = create_from_template(\n    \"ml_training\",\n    name=\"my_model_training\",\n    data_loader=my_loader,\n    trainer=my_trainer,\n    evaluator=my_evaluator\n)\n</code></pre>"},{"location":"advanced/templates/#creating-custom-templates","title":"\ud83d\udee0 Creating Custom Templates","text":"<p>You can define your own templates by creating a function that returns a <code>Pipeline</code> object.</p>"},{"location":"advanced/templates/#real-world-pattern-the-golden-path-template","title":"Real-World Pattern: The \"Golden Path\" Template","text":"<p>Create a standard ETL pipeline that enforces your team's best practices (e.g., always validating data).</p> <pre><code>from flowyml import Pipeline, step\n\ndef create_standard_etl(name, source_config, dest_config):\n    \"\"\"\n    A template that enforces:\n    1. Extraction\n    2. Validation (Mandatory!)\n    3. Loading\n    \"\"\"\n    pipeline = Pipeline(name)\n\n    @step\n    def extract():\n        return connect(source_config).read()\n\n    @step\n    def validate(data):\n        # Enforce quality checks\n        if data.null_count &gt; 0:\n            raise ValueError(\"Data quality check failed!\")\n        return data\n\n    @step\n    def load(data):\n        connect(dest_config).write(data)\n\n    pipeline.add_step(extract)\n    pipeline.add_step(validate)\n    pipeline.add_step(load)\n\n    return pipeline\n</code></pre>"},{"location":"advanced/templates/#sharing-templates","title":"\ud83d\udce6 Sharing Templates","text":"<p>Templates can be shared as Python packages or modules. This is excellent for platform teams who want to provide \"golden paths\" for data scientists.</p> <pre><code># In your internal library\nfrom my_company.templates import standard_training_pipeline\n\npipeline = standard_training_pipeline(\n    model_type=\"xgboost\",\n    target=\"churn\"\n)\n</code></pre>"},{"location":"api/artifact_stores/","title":"Artifact Stores API \ud83d\udce6","text":"<p>Artifact Stores manage the storage and retrieval of step outputs.</p>"},{"location":"api/artifact_stores/#base-artifact-store","title":"Base Artifact Store","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for artifact storage backends.</p>"},{"location":"api/artifact_stores/#flowyml.storage.artifacts.ArtifactStore-functions","title":"Functions","text":""},{"location":"api/artifact_stores/#flowyml.storage.artifacts.ArtifactStore.delete","title":"<code>delete(path: str) -&gt; None</code>  <code>abstractmethod</code>","text":"<p>Delete artifact at path.</p> Source code in <code>flowyml/storage/artifacts.py</code> <pre><code>@abstractmethod\ndef delete(self, path: str) -&gt; None:\n    \"\"\"Delete artifact at path.\"\"\"\n    pass\n</code></pre>"},{"location":"api/artifact_stores/#flowyml.storage.artifacts.ArtifactStore.exists","title":"<code>exists(path: str) -&gt; bool</code>  <code>abstractmethod</code>","text":"<p>Check if artifact exists at path.</p> Source code in <code>flowyml/storage/artifacts.py</code> <pre><code>@abstractmethod\ndef exists(self, path: str) -&gt; bool:\n    \"\"\"Check if artifact exists at path.\"\"\"\n    pass\n</code></pre>"},{"location":"api/artifact_stores/#flowyml.storage.artifacts.ArtifactStore.list_artifacts","title":"<code>list_artifacts(prefix: str = '') -&gt; list[str]</code>  <code>abstractmethod</code>","text":"<p>List all artifacts with optional prefix filter.</p> Source code in <code>flowyml/storage/artifacts.py</code> <pre><code>@abstractmethod\ndef list_artifacts(self, prefix: str = \"\") -&gt; list[str]:\n    \"\"\"List all artifacts with optional prefix filter.\"\"\"\n    pass\n</code></pre>"},{"location":"api/artifact_stores/#flowyml.storage.artifacts.ArtifactStore.load","title":"<code>load(path: str) -&gt; Any</code>  <code>abstractmethod</code>","text":"<p>Load an artifact from storage.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Storage path of the artifact</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The loaded artifact</p> Source code in <code>flowyml/storage/artifacts.py</code> <pre><code>@abstractmethod\ndef load(self, path: str) -&gt; Any:\n    \"\"\"Load an artifact from storage.\n\n    Args:\n        path: Storage path of the artifact\n\n    Returns:\n        The loaded artifact\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/artifact_stores/#flowyml.storage.artifacts.ArtifactStore.materialize","title":"<code>materialize(obj: Any, name: str, run_id: str, step_name: str, project_name: str = 'default') -&gt; str</code>","text":"<p>Materialize artifact to structured storage.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>Object to materialize</p> required <code>name</code> <code>str</code> <p>Name of the artifact</p> required <code>run_id</code> <code>str</code> <p>ID of the current run</p> required <code>step_name</code> <code>str</code> <p>Name of the step producing the artifact</p> required <code>project_name</code> <code>str</code> <p>Name of the project</p> <code>'default'</code> <p>Returns:</p> Type Description <code>str</code> <p>Path where artifact was saved</p> Source code in <code>flowyml/storage/artifacts.py</code> <pre><code>def materialize(self, obj: Any, name: str, run_id: str, step_name: str, project_name: str = \"default\") -&gt; str:\n    \"\"\"Materialize artifact to structured storage.\n\n    Args:\n        obj: Object to materialize\n        name: Name of the artifact\n        run_id: ID of the current run\n        step_name: Name of the step producing the artifact\n        project_name: Name of the project\n\n    Returns:\n        Path where artifact was saved\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/artifact_stores/#flowyml.storage.artifacts.ArtifactStore.save","title":"<code>save(artifact: Any, path: str, metadata: dict | None = None) -&gt; str</code>  <code>abstractmethod</code>","text":"<p>Save an artifact to storage.</p> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>Any</code> <p>The artifact to save</p> required <code>path</code> <code>str</code> <p>Storage path for the artifact</p> required <code>metadata</code> <code>dict | None</code> <p>Optional metadata dictionary</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Full path where artifact was saved</p> Source code in <code>flowyml/storage/artifacts.py</code> <pre><code>@abstractmethod\ndef save(self, artifact: Any, path: str, metadata: dict | None = None) -&gt; str:\n    \"\"\"Save an artifact to storage.\n\n    Args:\n        artifact: The artifact to save\n        path: Storage path for the artifact\n        metadata: Optional metadata dictionary\n\n    Returns:\n        Full path where artifact was saved\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/artifact_stores/#local-artifact-store","title":"Local Artifact Store","text":"<p>               Bases: <code>ArtifactStore</code></p> <p>Local filesystem artifact storage.</p> <p>Initialize local artifact store.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>str</code> <p>Base directory for storing artifacts</p> <code>'.flowyml/artifacts'</code> Source code in <code>flowyml/storage/artifacts.py</code> <pre><code>def __init__(self, base_path: str = \".flowyml/artifacts\"):\n    \"\"\"Initialize local artifact store.\n\n    Args:\n        base_path: Base directory for storing artifacts\n    \"\"\"\n    self.base_path = Path(base_path)\n    self.base_path.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"api/artifact_stores/#flowyml.storage.artifacts.LocalArtifactStore-functions","title":"Functions","text":""},{"location":"api/artifact_stores/#flowyml.storage.artifacts.LocalArtifactStore.delete","title":"<code>delete(path: str) -&gt; None</code>","text":"<p>Delete artifact from filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Relative path to delete</p> required Source code in <code>flowyml/storage/artifacts.py</code> <pre><code>def delete(self, path: str) -&gt; None:\n    \"\"\"Delete artifact from filesystem.\n\n    Args:\n        path: Relative path to delete\n    \"\"\"\n    full_path = self.base_path / path\n    if full_path.exists():\n        if full_path.is_dir():\n            shutil.rmtree(full_path)\n        else:\n            full_path.unlink()\n\n        # Also delete metadata if exists\n        metadata_path = full_path.with_suffix(\".meta.json\")\n        if metadata_path.exists():\n            metadata_path.unlink()\n</code></pre>"},{"location":"api/artifact_stores/#flowyml.storage.artifacts.LocalArtifactStore.exists","title":"<code>exists(path: str) -&gt; bool</code>","text":"<p>Check if artifact exists.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Relative path to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if artifact exists, False otherwise</p> Source code in <code>flowyml/storage/artifacts.py</code> <pre><code>def exists(self, path: str) -&gt; bool:\n    \"\"\"Check if artifact exists.\n\n    Args:\n        path: Relative path to check\n\n    Returns:\n        True if artifact exists, False otherwise\n    \"\"\"\n    full_path = self.base_path / path\n    return full_path.exists()\n</code></pre>"},{"location":"api/artifact_stores/#flowyml.storage.artifacts.LocalArtifactStore.get_metadata","title":"<code>get_metadata(path: str) -&gt; dict | None</code>","text":"<p>Get metadata for an artifact.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Relative path to the artifact</p> required <p>Returns:</p> Type Description <code>dict | None</code> <p>Metadata dictionary or None if no metadata exists</p> Source code in <code>flowyml/storage/artifacts.py</code> <pre><code>def get_metadata(self, path: str) -&gt; dict | None:\n    \"\"\"Get metadata for an artifact.\n\n    Args:\n        path: Relative path to the artifact\n\n    Returns:\n        Metadata dictionary or None if no metadata exists\n    \"\"\"\n    full_path = self.base_path / path\n    metadata_path = full_path.with_suffix(\".meta.json\")\n\n    if not metadata_path.exists():\n        return None\n\n    import json\n\n    with open(metadata_path) as f:\n        return json.load(f)\n</code></pre>"},{"location":"api/artifact_stores/#flowyml.storage.artifacts.LocalArtifactStore.list_artifacts","title":"<code>list_artifacts(prefix: str = '') -&gt; list[str]</code>","text":"<p>List all artifacts with optional prefix.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>Optional prefix filter</p> <code>''</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of artifact paths</p> Source code in <code>flowyml/storage/artifacts.py</code> <pre><code>def list_artifacts(self, prefix: str = \"\") -&gt; list[str]:\n    \"\"\"List all artifacts with optional prefix.\n\n    Args:\n        prefix: Optional prefix filter\n\n    Returns:\n        List of artifact paths\n    \"\"\"\n    search_path = self.base_path / prefix if prefix else self.base_path\n\n    if not search_path.exists():\n        return []\n\n    artifacts = []\n    for item in search_path.rglob(\"*\"):\n        if item.is_file() and not item.name.endswith(\".meta.json\"):\n            rel_path = item.relative_to(self.base_path)\n            artifacts.append(str(rel_path))\n\n    return sorted(artifacts)\n</code></pre>"},{"location":"api/artifact_stores/#flowyml.storage.artifacts.LocalArtifactStore.load","title":"<code>load(path: str) -&gt; Any</code>","text":"<p>Load artifact from local filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Relative or absolute path to the artifact</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The loaded artifact</p> Source code in <code>flowyml/storage/artifacts.py</code> <pre><code>def load(self, path: str) -&gt; Any:\n    \"\"\"Load artifact from local filesystem.\n\n    Args:\n        path: Relative or absolute path to the artifact\n\n    Returns:\n        The loaded artifact\n    \"\"\"\n    full_path = Path(path) if Path(path).is_absolute() else self.base_path / path\n\n    if not full_path.exists():\n        raise FileNotFoundError(f\"Artifact not found at {full_path}\")\n\n    with open(full_path, \"rb\") as f:\n        return pickle.load(f)\n</code></pre>"},{"location":"api/artifact_stores/#flowyml.storage.artifacts.LocalArtifactStore.materialize","title":"<code>materialize(obj: Any, name: str, run_id: str, step_name: str, project_name: str = 'default') -&gt; str</code>","text":"<p>Materialize artifact to structured storage.</p> Source code in <code>flowyml/storage/artifacts.py</code> <pre><code>def materialize(self, obj: Any, name: str, run_id: str, step_name: str, project_name: str = \"default\") -&gt; str:\n    \"\"\"Materialize artifact to structured storage.\"\"\"\n    from datetime import datetime\n    from flowyml.storage.materializers.base import get_materializer\n    import shutil\n    import pickle\n    import json\n\n    date_str = datetime.now().strftime(\"%Y-%m-%d\")\n    # Structure: project / date / run_id / data / step / name\n    rel_path = Path(project_name) / date_str / run_id / \"data\" / step_name / name\n    full_path = self.base_path / rel_path\n\n    # Clean up if exists\n    if full_path.exists():\n        if full_path.is_dir():\n            shutil.rmtree(full_path)\n        else:\n            full_path.unlink()\n\n    full_path.mkdir(parents=True, exist_ok=True)\n\n    materializer = get_materializer(obj)\n    if materializer:\n        materializer.save(obj, full_path)\n    else:\n        # Fallback to pickle\n        with open(full_path / \"data.pkl\", \"wb\") as f:\n            pickle.dump(obj, f)\n        # Save metadata\n        with open(full_path / \"metadata.json\", \"w\") as f:\n            json.dump({\"type\": \"pickle\", \"format\": \"pickle\"}, f, indent=2)\n\n    return str(full_path)\n</code></pre>"},{"location":"api/artifact_stores/#flowyml.storage.artifacts.LocalArtifactStore.save","title":"<code>save(artifact: Any, path: str, metadata: dict | None = None) -&gt; str</code>","text":"<p>Save artifact to local filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>Any</code> <p>The artifact to save</p> required <code>path</code> <code>str</code> <p>Relative path for the artifact</p> required <code>metadata</code> <code>dict | None</code> <p>Optional metadata dictionary</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Full path where artifact was saved</p> Source code in <code>flowyml/storage/artifacts.py</code> <pre><code>def save(self, artifact: Any, path: str, metadata: dict | None = None) -&gt; str:\n    \"\"\"Save artifact to local filesystem.\n\n    Args:\n        artifact: The artifact to save\n        path: Relative path for the artifact\n        metadata: Optional metadata dictionary\n\n    Returns:\n        Full path where artifact was saved\n    \"\"\"\n    full_path = self.base_path / path\n    full_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Save artifact using pickle by default\n    with open(full_path, \"wb\") as f:\n        pickle.dump(artifact, f)\n\n    # Save metadata if provided\n    if metadata:\n        metadata_path = full_path.with_suffix(\".meta.json\")\n        import json\n\n        with open(metadata_path, \"w\") as f:\n            json.dump(metadata, f, indent=2)\n\n    return str(full_path)\n</code></pre>"},{"location":"api/artifact_stores/#flowyml.storage.artifacts.LocalArtifactStore.size","title":"<code>size(path: str) -&gt; int</code>","text":"<p>Get size of artifact in bytes.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Relative path to the artifact</p> required <p>Returns:</p> Type Description <code>int</code> <p>Size in bytes</p> Source code in <code>flowyml/storage/artifacts.py</code> <pre><code>def size(self, path: str) -&gt; int:\n    \"\"\"Get size of artifact in bytes.\n\n    Args:\n        path: Relative path to the artifact\n\n    Returns:\n        Size in bytes\n    \"\"\"\n    full_path = self.base_path / path\n    if full_path.exists():\n        return full_path.stat().st_size\n    return 0\n</code></pre>"},{"location":"api/artifact_stores/#gcs-artifact-store","title":"GCS Artifact Store","text":"<p>               Bases: <code>ArtifactStore</code></p> <p>Google Cloud Storage artifact store.</p> <p>Stores pipeline artifacts in Google Cloud Storage buckets.</p> Example <pre><code>from flowyml.stacks.gcp import GCSArtifactStore\n\nartifact_store = GCSArtifactStore(bucket_name=\"my-flowyml-artifacts\", project_id=\"my-gcp-project\")\n</code></pre> <p>Initialize GCS artifact store.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the artifact store</p> <code>'gcs'</code> <code>bucket_name</code> <code>str | None</code> <p>GCS bucket name</p> <code>None</code> <code>project_id</code> <code>str | None</code> <p>GCP project ID</p> <code>None</code> <code>prefix</code> <code>str</code> <p>Prefix for all artifacts in bucket</p> <code>'flowyml'</code> Source code in <code>flowyml/stacks/gcp.py</code> <pre><code>def __init__(\n    self,\n    name: str = \"gcs\",\n    bucket_name: str | None = None,\n    project_id: str | None = None,\n    prefix: str = \"flowyml\",\n):\n    \"\"\"Initialize GCS artifact store.\n\n    Args:\n        name: Name of the artifact store\n        bucket_name: GCS bucket name\n        project_id: GCP project ID\n        prefix: Prefix for all artifacts in bucket\n    \"\"\"\n    super().__init__(name)\n    self.bucket_name = bucket_name\n    self.project_id = project_id\n    self.prefix = prefix\n</code></pre>"},{"location":"api/artifact_stores/#flowyml.stacks.gcp.GCSArtifactStore-functions","title":"Functions","text":""},{"location":"api/artifact_stores/#flowyml.stacks.gcp.GCSArtifactStore.exists","title":"<code>exists(path: str) -&gt; bool</code>","text":"<p>Check if artifact exists in GCS.</p> Source code in <code>flowyml/stacks/gcp.py</code> <pre><code>def exists(self, path: str) -&gt; bool:\n    \"\"\"Check if artifact exists in GCS.\"\"\"\n    from google.cloud import storage\n\n    client = storage.Client(project=self.project_id)\n    bucket = client.bucket(self.bucket_name)\n\n    full_path = f\"{self.prefix}/{path}\"\n    blob = bucket.blob(full_path)\n\n    return blob.exists()\n</code></pre>"},{"location":"api/artifact_stores/#flowyml.stacks.gcp.GCSArtifactStore.load","title":"<code>load(path: str) -&gt; Any</code>","text":"<p>Load artifact from GCS.</p> Source code in <code>flowyml/stacks/gcp.py</code> <pre><code>def load(self, path: str) -&gt; Any:\n    \"\"\"Load artifact from GCS.\"\"\"\n    from google.cloud import storage\n    import pickle\n\n    client = storage.Client(project=self.project_id)\n    bucket = client.bucket(self.bucket_name)\n\n    # Handle both full gs:// URIs and relative paths\n    if path.startswith(\"gs://\"):\n        # Extract bucket and path from URI\n        parts = path.replace(\"gs://\", \"\").split(\"/\", 1)\n        blob_path = parts[1] if len(parts) &gt; 1 else \"\"\n    else:\n        blob_path = f\"{self.prefix}/{path}\"\n\n    blob = bucket.blob(blob_path)\n    data = blob.download_as_bytes()\n\n    return pickle.loads(data)\n</code></pre>"},{"location":"api/artifact_stores/#flowyml.stacks.gcp.GCSArtifactStore.save","title":"<code>save(artifact: Any, path: str) -&gt; str</code>","text":"<p>Save artifact to GCS.</p> Source code in <code>flowyml/stacks/gcp.py</code> <pre><code>def save(self, artifact: Any, path: str) -&gt; str:\n    \"\"\"Save artifact to GCS.\"\"\"\n    from google.cloud import storage\n    import pickle\n\n    client = storage.Client(project=self.project_id)\n    bucket = client.bucket(self.bucket_name)\n\n    # Full path with prefix\n    full_path = f\"{self.prefix}/{path}\"\n    blob = bucket.blob(full_path)\n\n    # Serialize and upload\n    data = pickle.dumps(artifact)\n    blob.upload_from_string(data)\n\n    return f\"gs://{self.bucket_name}/{full_path}\"\n</code></pre>"},{"location":"api/artifact_stores/#flowyml.stacks.gcp.GCSArtifactStore.to_dict","title":"<code>to_dict() -&gt; dict[str, Any]</code>","text":"<p>Convert to dictionary.</p> Source code in <code>flowyml/stacks/gcp.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert to dictionary.\"\"\"\n    return {\n        \"name\": self.name,\n        \"type\": \"gcs\",\n        \"bucket_name\": self.bucket_name,\n        \"project_id\": self.project_id,\n        \"prefix\": self.prefix,\n    }\n</code></pre>"},{"location":"api/artifact_stores/#flowyml.stacks.gcp.GCSArtifactStore.validate","title":"<code>validate() -&gt; bool</code>","text":"<p>Validate GCS configuration.</p> Source code in <code>flowyml/stacks/gcp.py</code> <pre><code>def validate(self) -&gt; bool:\n    \"\"\"Validate GCS configuration.\"\"\"\n    if not self.bucket_name:\n        raise ValueError(\"bucket_name is required for GCSArtifactStore\")\n\n    # Check if google-cloud-storage is installed\n    import importlib.util\n\n    if importlib.util.find_spec(\"google.cloud.storage\") is not None:\n        return True\n    raise ImportError(\n        \"google-cloud-storage is required for GCSArtifactStore. Install with: pip install google-cloud-storage\",\n    )\n</code></pre>"},{"location":"api/assets/","title":"Assets API \ud83d\udce6","text":"<p>First-class citizens for data and models.</p>"},{"location":"api/assets/#usage","title":"Usage","text":"<pre><code>from flowyml import Dataset, Model\n\nds = Dataset.create(data, name=\"my_data\")\nmodel = Model.create(clf, name=\"my_model\")\n</code></pre>"},{"location":"api/assets/#class-asset","title":"Class <code>Asset</code>","text":"<p>Base class for all ML assets (datasets, models, features, etc).</p> <p>Assets are first-class objects in flowyml pipelines with full lineage tracking.</p> Source code in <code>flowyml/assets/base.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    version: str | None = None,\n    data: Any = None,\n    parent: Optional[\"Asset\"] = None,\n    tags: dict[str, str] | None = None,\n    properties: dict[str, Any] | None = None,\n):\n    self.name = name\n    self.version = version or \"v1.0.0\"\n    self.data = data\n    self.asset_id = str(uuid4())\n\n    # Metadata\n    self.metadata = AssetMetadata(\n        asset_id=self.asset_id,\n        name=name,\n        version=self.version,\n        asset_type=self.__class__.__name__,\n        created_at=datetime.now(),\n        created_by=\"flowyml\",\n        parent_ids=[parent.asset_id] if parent else [],\n        tags=tags or {},\n        properties=properties or {},\n    )\n\n    # Lineage tracking\n    self.parents: list[Asset] = [parent] if parent else []\n    self.children: list[Asset] = []\n\n    if parent:\n        parent.children.append(self)\n</code></pre>"},{"location":"api/assets/#flowyml.assets.base.Asset-functions","title":"Functions","text":""},{"location":"api/assets/#flowyml.assets.base.Asset.add_property","title":"<code>add_property(key: str, value: Any) -&gt; None</code>","text":"<p>Add a property to the asset.</p> Source code in <code>flowyml/assets/base.py</code> <pre><code>def add_property(self, key: str, value: Any) -&gt; None:\n    \"\"\"Add a property to the asset.\"\"\"\n    self.metadata.properties[key] = value\n</code></pre>"},{"location":"api/assets/#flowyml.assets.base.Asset.add_tag","title":"<code>add_tag(key: str, value: str) -&gt; None</code>","text":"<p>Add a tag to the asset.</p> Source code in <code>flowyml/assets/base.py</code> <pre><code>def add_tag(self, key: str, value: str) -&gt; None:\n    \"\"\"Add a tag to the asset.\"\"\"\n    self.metadata.tags[key] = value\n</code></pre>"},{"location":"api/assets/#flowyml.assets.base.Asset.create","title":"<code>create(data: Any, name: str | None = None, version: str | None = None, parent: Optional[Asset] = None, **kwargs: Any) -&gt; Asset</code>  <code>classmethod</code>","text":"<p>Factory method to create an asset.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The actual data/object</p> required <code>name</code> <code>str | None</code> <p>Asset name</p> <code>None</code> <code>version</code> <code>str | None</code> <p>Asset version</p> <code>None</code> <code>parent</code> <code>Optional[Asset]</code> <p>Parent asset for lineage</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional metadata</p> <code>{}</code> <p>Returns:</p> Type Description <code>Asset</code> <p>New asset instance</p> Source code in <code>flowyml/assets/base.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    data: Any,\n    name: str | None = None,\n    version: str | None = None,\n    parent: Optional[\"Asset\"] = None,\n    **kwargs: Any,\n) -&gt; \"Asset\":\n    \"\"\"Factory method to create an asset.\n\n    Args:\n        data: The actual data/object\n        name: Asset name\n        version: Asset version\n        parent: Parent asset for lineage\n        **kwargs: Additional metadata\n\n    Returns:\n        New asset instance\n    \"\"\"\n    asset_name = name or f\"{cls.__name__}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n\n    # Extract tags and properties if passed explicitly\n    tags = kwargs.pop(\"tags\", {})\n    props = kwargs.pop(\"properties\", {})\n    # Merge remaining kwargs into properties\n    props.update(kwargs)\n\n    return cls(\n        name=asset_name,\n        version=version,\n        data=data,\n        parent=parent,\n        tags=tags,\n        properties=props,\n    )\n</code></pre>"},{"location":"api/assets/#flowyml.assets.base.Asset.get_all_ancestors","title":"<code>get_all_ancestors() -&gt; set[Asset]</code>","text":"<p>Get all ancestor assets.</p> Source code in <code>flowyml/assets/base.py</code> <pre><code>def get_all_ancestors(self) -&gt; set[\"Asset\"]:\n    \"\"\"Get all ancestor assets.\"\"\"\n    ancestors = set()\n\n    def traverse(asset) -&gt; None:\n        for parent in asset.parents:\n            if parent not in ancestors:\n                ancestors.add(parent)\n                traverse(parent)\n\n    traverse(self)\n    return ancestors\n</code></pre>"},{"location":"api/assets/#flowyml.assets.base.Asset.get_all_descendants","title":"<code>get_all_descendants() -&gt; set[Asset]</code>","text":"<p>Get all descendant assets.</p> Source code in <code>flowyml/assets/base.py</code> <pre><code>def get_all_descendants(self) -&gt; set[\"Asset\"]:\n    \"\"\"Get all descendant assets.\"\"\"\n    descendants = set()\n\n    def traverse(asset) -&gt; None:\n        for child in asset.children:\n            if child not in descendants:\n                descendants.add(child)\n                traverse(child)\n\n    traverse(self)\n    return descendants\n</code></pre>"},{"location":"api/assets/#flowyml.assets.base.Asset.get_hash","title":"<code>get_hash() -&gt; str</code>","text":"<p>Generate hash of asset for caching/versioning.</p> Source code in <code>flowyml/assets/base.py</code> <pre><code>def get_hash(self) -&gt; str:\n    \"\"\"Generate hash of asset for caching/versioning.\"\"\"\n    content = json.dumps(\n        {\n            \"name\": self.name,\n            \"version\": self.version,\n            \"type\": self.metadata.asset_type,\n            \"created_at\": self.metadata.created_at.isoformat(),\n        },\n        sort_keys=True,\n    )\n    return hashlib.sha256(content.encode()).hexdigest()[:16]\n</code></pre>"},{"location":"api/assets/#flowyml.assets.base.Asset.get_lineage","title":"<code>get_lineage(depth: int = -1) -&gt; dict[str, Any]</code>","text":"<p>Get asset lineage.</p> <p>Parameters:</p> Name Type Description Default <code>depth</code> <code>int</code> <p>How many levels to traverse (-1 for all)</p> <code>-1</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Lineage tree as nested dict</p> Source code in <code>flowyml/assets/base.py</code> <pre><code>def get_lineage(self, depth: int = -1) -&gt; dict[str, Any]:\n    \"\"\"Get asset lineage.\n\n    Args:\n        depth: How many levels to traverse (-1 for all)\n\n    Returns:\n        Lineage tree as nested dict\n    \"\"\"\n    lineage = {\n        \"asset\": {\n            \"asset_id\": self.asset_id,\n            \"name\": self.name,\n            \"type\": self.metadata.asset_type,\n            \"version\": self.version,\n        },\n        \"parents\": [],\n        \"children\": [],\n    }\n\n    if depth != 0:\n        next_depth = depth - 1 if depth &gt; 0 else -1\n        lineage[\"parents\"] = [p.get_lineage(next_depth) for p in self.parents]\n        lineage[\"children\"] = [c.get_lineage(next_depth) for c in self.children]\n\n    return lineage\n</code></pre>"},{"location":"api/assets/#flowyml.assets.base.Asset.to_dict","title":"<code>to_dict() -&gt; dict[str, Any]</code>","text":"<p>Convert asset to dictionary.</p> Source code in <code>flowyml/assets/base.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert asset to dictionary.\"\"\"\n    return {\n        \"metadata\": self.metadata.to_dict(),\n        \"lineage\": {\n            \"parents\": [p.asset_id for p in self.parents],\n            \"children\": [c.asset_id for c in self.children],\n        },\n    }\n</code></pre>"},{"location":"api/assets/#class-dataset","title":"Class <code>Dataset</code>","text":"<p>               Bases: <code>Asset</code></p> <p>Dataset asset with schema and lineage tracking.</p> Example <p>raw_data = Dataset( ...     name=\"imagenet_train\", ...     version=\"v2.0\", ...     data=train_dataset, ...     properties={\"size\": \"150GB\", \"samples\": 1_281_167}, ... )</p> Source code in <code>flowyml/assets/dataset.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    version: str | None = None,\n    data: Any = None,\n    schema: Any | None = None,\n    location: str | None = None,\n    parent: Asset | None = None,\n    tags: dict[str, str] | None = None,\n    properties: dict[str, Any] | None = None,\n):\n    super().__init__(\n        name=name,\n        version=version,\n        data=data,\n        parent=parent,\n        tags=tags,\n        properties=properties,\n    )\n\n    self.schema = schema\n    self.location = location\n\n    # Add dataset-specific properties\n    if schema:\n        self.metadata.properties[\"schema\"] = str(schema)\n    if location:\n        self.metadata.properties[\"location\"] = location\n</code></pre>"},{"location":"api/assets/#flowyml.assets.dataset.Dataset-attributes","title":"Attributes","text":""},{"location":"api/assets/#flowyml.assets.dataset.Dataset.num_samples","title":"<code>num_samples: int | None</code>  <code>property</code>","text":"<p>Get number of samples if available.</p>"},{"location":"api/assets/#flowyml.assets.dataset.Dataset.size","title":"<code>size: int | None</code>  <code>property</code>","text":"<p>Get dataset size if available.</p>"},{"location":"api/assets/#flowyml.assets.dataset.Dataset-functions","title":"Functions","text":""},{"location":"api/assets/#flowyml.assets.dataset.Dataset.split","title":"<code>split(train_ratio: float = 0.8, name_prefix: str | None = None) -&gt; tuple[Dataset, Dataset]</code>","text":"<p>Split dataset into train/test.</p> <p>Parameters:</p> Name Type Description Default <code>train_ratio</code> <code>float</code> <p>Ratio for training split</p> <code>0.8</code> <code>name_prefix</code> <code>str | None</code> <p>Prefix for split dataset names</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Dataset, Dataset]</code> <p>Tuple of (train_dataset, test_dataset)</p> Source code in <code>flowyml/assets/dataset.py</code> <pre><code>def split(self, train_ratio: float = 0.8, name_prefix: str | None = None) -&gt; tuple[\"Dataset\", \"Dataset\"]:\n    \"\"\"Split dataset into train/test.\n\n    Args:\n        train_ratio: Ratio for training split\n        name_prefix: Prefix for split dataset names\n\n    Returns:\n        Tuple of (train_dataset, test_dataset)\n    \"\"\"\n    prefix = name_prefix or self.name\n\n    # Placeholder - actual splitting logic would depend on data type\n    _ = train_ratio  # Unused in placeholder\n    train_data = self.data  # Would actually split the data\n    test_data = self.data\n\n    train_dataset = Dataset(\n        name=f\"{prefix}_train\",\n        version=self.version,\n        data=train_data,\n        schema=self.schema,\n        parent=self,\n        tags={**self.metadata.tags, \"split\": \"train\"},\n    )\n\n    test_dataset = Dataset(\n        name=f\"{prefix}_test\",\n        version=self.version,\n        data=test_data,\n        schema=self.schema,\n        parent=self,\n        tags={**self.metadata.tags, \"split\": \"test\"},\n    )\n\n    return train_dataset, test_dataset\n</code></pre>"},{"location":"api/assets/#flowyml.assets.dataset.Dataset.validate_schema","title":"<code>validate_schema() -&gt; bool</code>","text":"<p>Validate data against schema (placeholder).</p> Source code in <code>flowyml/assets/dataset.py</code> <pre><code>def validate_schema(self) -&gt; bool:\n    \"\"\"Validate data against schema (placeholder).\"\"\"\n    if self.schema is None or self.data is None:\n        return True\n    # Schema validation would go here\n    return True\n</code></pre>"},{"location":"api/assets/#class-model","title":"Class <code>Model</code>","text":"<p>               Bases: <code>Asset</code></p> <p>Model asset with training metadata and lineage.</p> Example <p>model = Model( ...     name=\"resnet50_v1\", ...     version=\"v1.0.0\", ...     data=trained_model, ...     architecture=\"resnet50\", ...     framework=\"pytorch\", ...     properties={\"params\": 25_557_032}, ... )</p> Source code in <code>flowyml/assets/model.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    version: str | None = None,\n    data: Any = None,\n    architecture: str | None = None,\n    framework: str | None = None,\n    input_shape: tuple | None = None,\n    output_shape: tuple | None = None,\n    trained_on: Asset | None = None,\n    parent: Asset | None = None,\n    tags: dict[str, str] | None = None,\n    properties: dict[str, Any] | None = None,\n):\n    super().__init__(\n        name=name,\n        version=version,\n        data=data,\n        parent=parent,\n        tags=tags,\n        properties=properties,\n    )\n\n    self.architecture = architecture\n    self.framework = framework\n    self.input_shape = input_shape\n    self.output_shape = output_shape\n\n    # Track training dataset\n    if trained_on:\n        self.parents.append(trained_on)\n        trained_on.children.append(self)\n\n    # Add model-specific properties\n    if architecture:\n        self.metadata.properties[\"architecture\"] = architecture\n    if framework:\n        self.metadata.properties[\"framework\"] = framework\n    if input_shape:\n        self.metadata.properties[\"input_shape\"] = input_shape\n    if output_shape:\n        self.metadata.properties[\"output_shape\"] = output_shape\n</code></pre>"},{"location":"api/assets/#flowyml.assets.model.Model-functions","title":"Functions","text":""},{"location":"api/assets/#flowyml.assets.model.Model.get_architecture_info","title":"<code>get_architecture_info() -&gt; dict[str, Any]</code>","text":"<p>Get architecture information.</p> Source code in <code>flowyml/assets/model.py</code> <pre><code>def get_architecture_info(self) -&gt; dict[str, Any]:\n    \"\"\"Get architecture information.\"\"\"\n    return {\n        \"architecture\": self.architecture,\n        \"framework\": self.framework,\n        \"input_shape\": self.input_shape,\n        \"output_shape\": self.output_shape,\n        \"parameters\": self.get_parameters_count(),\n    }\n</code></pre>"},{"location":"api/assets/#flowyml.assets.model.Model.get_parameters_count","title":"<code>get_parameters_count() -&gt; int | None</code>","text":"<p>Get number of model parameters if available.</p> Source code in <code>flowyml/assets/model.py</code> <pre><code>def get_parameters_count(self) -&gt; int | None:\n    \"\"\"Get number of model parameters if available.\"\"\"\n    return self.metadata.properties.get(\"params\") or self.metadata.properties.get(\"parameters\")\n</code></pre>"},{"location":"api/assets/#flowyml.assets.model.Model.get_training_datasets","title":"<code>get_training_datasets()</code>","text":"<p>Get all datasets this model was trained on.</p> Source code in <code>flowyml/assets/model.py</code> <pre><code>def get_training_datasets(self):\n    \"\"\"Get all datasets this model was trained on.\"\"\"\n    from flowyml.assets.dataset import Dataset\n\n    return [p for p in self.parents if isinstance(p, Dataset)]\n</code></pre>"},{"location":"api/cli/","title":"CLI Reference \ud83d\udcbb","text":"<p>Command-line interface for flowyml.</p>"},{"location":"api/cli/#usage","title":"Usage","text":"<pre><code>flowyml [COMMAND] [OPTIONS]\n</code></pre>"},{"location":"api/cli/#commands","title":"Commands","text":""},{"location":"api/cli/#init","title":"<code>init</code>","text":"<p>Initialize a new flowyml project.</p> <pre><code>flowyml init [PROJECT_NAME]\n</code></pre>"},{"location":"api/cli/#run","title":"<code>run</code>","text":"<p>Run a pipeline.</p> <pre><code>flowyml run [PIPELINE_FILE]\n</code></pre>"},{"location":"api/cli/#stack","title":"<code>stack</code>","text":"<p>Manage infrastructure stacks.</p> <pre><code>flowyml stack list\nflowyml stack register [NAME]\nflowyml stack set [NAME]\n</code></pre>"},{"location":"api/cli/#ui","title":"<code>ui</code>","text":"<p>Start the dashboard.</p> <pre><code>flowyml ui --port 8080\n</code></pre>"},{"location":"api/config/","title":"Configuration API \u2699\ufe0f","text":"<p>Reference for <code>flowyml.yaml</code> and environment variables.</p>"},{"location":"api/config/#stack-configuration","title":"Stack Configuration","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for execution stacks.</p>"},{"location":"api/config/#general-configuration","title":"General Configuration","text":"<p>Configuration management for flowyml.</p>"},{"location":"api/config/#flowyml.utils.config-classes","title":"Classes","text":""},{"location":"api/config/#flowyml.utils.config.FlowymlConfig","title":"<code>FlowymlConfig(flowyml_home: Path = (lambda: Path.home() / '.flowyml')(), artifacts_dir: Path = (lambda: Path('.flowyml/artifacts'))(), metadata_db: Path = (lambda: Path('.flowyml/metadata.db'))(), cache_dir: Path = (lambda: Path('.flowyml/cache'))(), runs_dir: Path = (lambda: Path('.flowyml/runs'))(), experiments_dir: Path = (lambda: Path('.flowyml/experiments'))(), default_stack: str = 'local', execution_mode: str = 'local', remote_server_url: str = '', remote_ui_url: str = '', enable_caching: bool = True, enable_logging: bool = True, log_level: str = 'INFO', max_cache_size_mb: int = 10000, ui_host: str = 'localhost', ui_port: int = 8080, enable_ui: bool = False, auto_log_params: bool = True, auto_log_metrics: bool = True, auto_log_artifacts: bool = True, track_git: bool = True, track_environment: bool = True, max_parallel_steps: int = 4, step_timeout_seconds: int = 3600, retry_max_attempts: int = 3, debug_mode: bool = False, strict_validation: bool = True, allow_pickle: bool = True)</code>  <code>dataclass</code>","text":"<p>Global flowyml configuration.</p>"},{"location":"api/config/#flowyml.utils.config.FlowymlConfig-functions","title":"Functions","text":""},{"location":"api/config/#flowyml.utils.config.FlowymlConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Ensure all paths are Path objects.</p> Source code in <code>flowyml/utils/config.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Ensure all paths are Path objects.\"\"\"\n    for field_name in [\n        \"flowyml_home\",\n        \"artifacts_dir\",\n        \"metadata_db\",\n        \"cache_dir\",\n        \"runs_dir\",\n        \"experiments_dir\",\n    ]:\n        value = getattr(self, field_name)\n        if not isinstance(value, Path):\n            setattr(self, field_name, Path(value))\n</code></pre>"},{"location":"api/config/#flowyml.utils.config.FlowymlConfig.create_directories","title":"<code>create_directories() -&gt; None</code>","text":"<p>Create necessary directories.</p> Source code in <code>flowyml/utils/config.py</code> <pre><code>def create_directories(self) -&gt; None:\n    \"\"\"Create necessary directories.\"\"\"\n    self.flowyml_home.mkdir(parents=True, exist_ok=True)\n    self.artifacts_dir.mkdir(parents=True, exist_ok=True)\n    self.cache_dir.mkdir(parents=True, exist_ok=True)\n    self.runs_dir.mkdir(parents=True, exist_ok=True)\n    self.experiments_dir.mkdir(parents=True, exist_ok=True)\n\n    # Create metadata db parent dir\n    self.metadata_db.parent.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"api/config/#flowyml.utils.config.FlowymlConfig.from_dict","title":"<code>from_dict(data: dict[str, Any]) -&gt; FlowymlConfig</code>  <code>classmethod</code>","text":"<p>Create config from dictionary.</p> Source code in <code>flowyml/utils/config.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; \"FlowymlConfig\":\n    \"\"\"Create config from dictionary.\"\"\"\n    return cls(**data)\n</code></pre>"},{"location":"api/config/#flowyml.utils.config.FlowymlConfig.load","title":"<code>load(path: Path | None = None) -&gt; FlowymlConfig</code>  <code>classmethod</code>","text":"<p>Load config from file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | None</code> <p>Path to load config from (defaults to ~/.flowyml/config.yaml)</p> <code>None</code> <p>Returns:</p> Type Description <code>FlowymlConfig</code> <p>Loaded FlowymlConfig instance</p> Source code in <code>flowyml/utils/config.py</code> <pre><code>@classmethod\ndef load(cls, path: Path | None = None) -&gt; \"FlowymlConfig\":\n    \"\"\"Load config from file.\n\n    Args:\n        path: Path to load config from (defaults to ~/.flowyml/config.yaml)\n\n    Returns:\n        Loaded FlowymlConfig instance\n    \"\"\"\n    if path is None:\n        path = Path.home() / \".flowyml\" / \"config.yaml\"\n\n    if not path.exists():\n        # Return default config\n        return cls()\n\n    with open(path) as f:\n        data = yaml.safe_load(f)\n\n    return cls.from_dict(data)\n</code></pre>"},{"location":"api/config/#flowyml.utils.config.FlowymlConfig.save","title":"<code>save(path: Path | None = None) -&gt; None</code>","text":"<p>Save config to file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | None</code> <p>Path to save config (defaults to ~/.flowyml/config.yaml)</p> <code>None</code> Source code in <code>flowyml/utils/config.py</code> <pre><code>def save(self, path: Path | None = None) -&gt; None:\n    \"\"\"Save config to file.\n\n    Args:\n        path: Path to save config (defaults to ~/.flowyml/config.yaml)\n    \"\"\"\n    if path is None:\n        path = self.flowyml_home / \"config.yaml\"\n\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    with open(path, \"w\") as f:\n        yaml.dump(self.to_dict(), f, default_flow_style=False)\n</code></pre>"},{"location":"api/config/#flowyml.utils.config.FlowymlConfig.to_dict","title":"<code>to_dict() -&gt; dict[str, Any]</code>","text":"<p>Convert config to dictionary.</p> Source code in <code>flowyml/utils/config.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert config to dictionary.\"\"\"\n    return {\n        \"flowyml_home\": str(self.flowyml_home),\n        \"artifacts_dir\": str(self.artifacts_dir),\n        \"metadata_db\": str(self.metadata_db),\n        \"cache_dir\": str(self.cache_dir),\n        \"runs_dir\": str(self.runs_dir),\n        \"experiments_dir\": str(self.experiments_dir),\n        \"default_stack\": self.default_stack,\n        \"execution_mode\": self.execution_mode,\n        \"remote_server_url\": self.remote_server_url,\n        \"remote_ui_url\": self.remote_ui_url,\n        \"enable_caching\": self.enable_caching,\n        \"enable_logging\": self.enable_logging,\n        \"log_level\": self.log_level,\n        \"max_cache_size_mb\": self.max_cache_size_mb,\n        \"ui_host\": self.ui_host,\n        \"ui_port\": self.ui_port,\n        \"enable_ui\": self.enable_ui,\n        \"auto_log_params\": self.auto_log_params,\n        \"auto_log_metrics\": self.auto_log_metrics,\n        \"auto_log_artifacts\": self.auto_log_artifacts,\n        \"track_git\": self.track_git,\n        \"track_environment\": self.track_environment,\n        \"max_parallel_steps\": self.max_parallel_steps,\n        \"step_timeout_seconds\": self.step_timeout_seconds,\n        \"retry_max_attempts\": self.retry_max_attempts,\n        \"debug_mode\": self.debug_mode,\n        \"strict_validation\": self.strict_validation,\n        \"allow_pickle\": self.allow_pickle,\n    }\n</code></pre>"},{"location":"api/config/#flowyml.utils.config-functions","title":"Functions","text":""},{"location":"api/config/#flowyml.utils.config.get_config","title":"<code>get_config() -&gt; FlowymlConfig</code>","text":"<p>Get global flowyml configuration.</p> <p>Returns:</p> Type Description <code>FlowymlConfig</code> <p>Global FlowymlConfig instance</p> Source code in <code>flowyml/utils/config.py</code> <pre><code>def get_config() -&gt; FlowymlConfig:\n    \"\"\"Get global flowyml configuration.\n\n    Returns:\n        Global FlowymlConfig instance\n    \"\"\"\n    global _global_config\n\n    if _global_config is None:\n        # Try to load from environment variable\n        config_path = os.environ.get(\"FLOWYML_CONFIG\")\n        if config_path:\n            _global_config = FlowymlConfig.load(Path(config_path))\n        else:\n            # Load from default location\n            _global_config = FlowymlConfig.load()\n\n        # Create necessary directories\n        _global_config.create_directories()\n\n    return _global_config\n</code></pre>"},{"location":"api/config/#flowyml.utils.config.get_env_config","title":"<code>get_env_config() -&gt; dict[str, Any]</code>","text":"<p>Get configuration from environment variables.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of configuration values from environment</p> Source code in <code>flowyml/utils/config.py</code> <pre><code>def get_env_config() -&gt; dict[str, Any]:\n    \"\"\"Get configuration from environment variables.\n\n    Returns:\n        Dictionary of configuration values from environment\n    \"\"\"\n    env_config = {}\n\n    # Map environment variables to config fields\n    env_mappings = {\n        \"flowyml_HOME\": \"flowyml_home\",\n        \"flowyml_ARTIFACTS_DIR\": \"artifacts_dir\",\n        \"flowyml_METADATA_DB\": \"metadata_db\",\n        \"flowyml_CACHE_DIR\": \"cache_dir\",\n        \"flowyml_DEFAULT_STACK\": \"default_stack\",\n        \"flowyml_EXECUTION_MODE\": \"execution_mode\",\n        \"flowyml_REMOTE_SERVER_URL\": \"remote_server_url\",\n        \"flowyml_REMOTE_UI_URL\": \"remote_ui_url\",\n        \"flowyml_ENABLE_CACHING\": \"enable_caching\",\n        \"flowyml_LOG_LEVEL\": \"log_level\",\n        \"flowyml_UI_HOST\": \"ui_host\",\n        \"flowyml_UI_PORT\": \"ui_port\",\n        \"flowyml_DEBUG\": \"debug_mode\",\n    }\n\n    for env_var, config_key in env_mappings.items():\n        value = os.environ.get(env_var)\n        if value is not None:\n            # Convert booleans\n            if value.lower() in (\"true\", \"1\", \"yes\"):\n                value = True\n            elif value.lower() in (\"false\", \"0\", \"no\"):\n                value = False\n            # Convert integers\n            elif value.isdigit():\n                value = int(value)\n\n            env_config[config_key] = value\n\n    return env_config\n</code></pre>"},{"location":"api/config/#flowyml.utils.config.init_config_from_env","title":"<code>init_config_from_env() -&gt; FlowymlConfig</code>","text":"<p>Initialize configuration from environment variables.</p> <p>Returns:</p> Type Description <code>FlowymlConfig</code> <p>FlowymlConfig instance with values from environment</p> Source code in <code>flowyml/utils/config.py</code> <pre><code>def init_config_from_env() -&gt; FlowymlConfig:\n    \"\"\"Initialize configuration from environment variables.\n\n    Returns:\n        FlowymlConfig instance with values from environment\n    \"\"\"\n    env_config = get_env_config()\n    config = FlowymlConfig()\n\n    for key, value in env_config.items():\n        if hasattr(config, key):\n            setattr(config, key, value)\n\n    return config\n</code></pre>"},{"location":"api/config/#flowyml.utils.config.load_project_config","title":"<code>load_project_config(project_dir: Path | None = None) -&gt; dict[str, Any]</code>","text":"<p>Load project-specific configuration.</p> <p>Parameters:</p> Name Type Description Default <code>project_dir</code> <code>Path | None</code> <p>Project directory (defaults to current directory)</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Project configuration dictionary</p> Source code in <code>flowyml/utils/config.py</code> <pre><code>def load_project_config(project_dir: Path | None = None) -&gt; dict[str, Any]:\n    \"\"\"Load project-specific configuration.\n\n    Args:\n        project_dir: Project directory (defaults to current directory)\n\n    Returns:\n        Project configuration dictionary\n    \"\"\"\n    if project_dir is None:\n        project_dir = Path.cwd()\n\n    config_path = project_dir / \"flowyml.yaml\"\n    if not config_path.exists():\n        config_path = project_dir / \"flowyml.yml\"\n\n    if not config_path.exists():\n        return {}\n\n    with open(config_path) as f:\n        return yaml.safe_load(f) or {}\n</code></pre>"},{"location":"api/config/#flowyml.utils.config.reset_config","title":"<code>reset_config() -&gt; None</code>","text":"<p>Reset global configuration to defaults.</p> Source code in <code>flowyml/utils/config.py</code> <pre><code>def reset_config() -&gt; None:\n    \"\"\"Reset global configuration to defaults.\"\"\"\n    global _global_config\n    _global_config = FlowymlConfig()\n    _global_config.create_directories()\n</code></pre>"},{"location":"api/config/#flowyml.utils.config.save_project_config","title":"<code>save_project_config(config: dict[str, Any], project_dir: Path | None = None) -&gt; None</code>","text":"<p>Save project-specific configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>Configuration dictionary</p> required <code>project_dir</code> <code>Path | None</code> <p>Project directory (defaults to current directory)</p> <code>None</code> Source code in <code>flowyml/utils/config.py</code> <pre><code>def save_project_config(config: dict[str, Any], project_dir: Path | None = None) -&gt; None:\n    \"\"\"Save project-specific configuration.\n\n    Args:\n        config: Configuration dictionary\n        project_dir: Project directory (defaults to current directory)\n    \"\"\"\n    if project_dir is None:\n        project_dir = Path.cwd()\n\n    config_path = project_dir / \"flowyml.yaml\"\n\n    with open(config_path, \"w\") as f:\n        yaml.dump(config, f, default_flow_style=False)\n</code></pre>"},{"location":"api/config/#flowyml.utils.config.set_config","title":"<code>set_config(config: FlowymlConfig) -&gt; None</code>","text":"<p>Set global flowyml configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>FlowymlConfig</code> <p>FlowymlConfig instance to set as global</p> required Source code in <code>flowyml/utils/config.py</code> <pre><code>def set_config(config: FlowymlConfig) -&gt; None:\n    \"\"\"Set global flowyml configuration.\n\n    Args:\n        config: FlowymlConfig instance to set as global\n    \"\"\"\n    global _global_config\n    _global_config = config\n    _global_config.create_directories()\n</code></pre>"},{"location":"api/config/#flowyml.utils.config.update_config","title":"<code>update_config(**kwargs) -&gt; None</code>","text":"<p>Update global configuration with new values.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Configuration values to update</p> <code>{}</code> Source code in <code>flowyml/utils/config.py</code> <pre><code>def update_config(**kwargs) -&gt; None:\n    \"\"\"Update global configuration with new values.\n\n    Args:\n        **kwargs: Configuration values to update\n    \"\"\"\n    config = get_config()\n    for key, value in kwargs.items():\n        if hasattr(config, key):\n            setattr(config, key, value)\n</code></pre>"},{"location":"api/context/","title":"Context API \ud83e\udde0","text":"<p>Access runtime information, parameters, and configuration within a step.</p>"},{"location":"api/context/#usage","title":"Usage","text":"<pre><code>from flowyml import step, get_context\n\n@step\ndef my_step():\n    ctx = get_context()\n    print(f\"Running in pipeline: {ctx.pipeline_name}\")\n</code></pre>"},{"location":"api/context/#class-context","title":"Class <code>Context</code>","text":"<p>Pipeline context with automatic parameter injection.</p> Example <p>ctx = Context(learning_rate=0.001, epochs=10, batch_size=32, device=\"cuda\")</p> Source code in <code>flowyml/core/context.py</code> <pre><code>def __init__(self, **kwargs):\n    self._params = kwargs\n    self._parent = None\n    self._metadata = {}\n</code></pre>"},{"location":"api/context/#flowyml.core.context.Context-functions","title":"Functions","text":""},{"location":"api/context/#flowyml.core.context.Context.__getattr__","title":"<code>__getattr__(name: str) -&gt; Any</code>","text":"<p>Allow dot notation access to parameters.</p> Source code in <code>flowyml/core/context.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Any:\n    \"\"\"Allow dot notation access to parameters.\"\"\"\n    if name.startswith(\"_\"):\n        return super().__getattribute__(name)\n\n    if name in self._params:\n        return self._params[name]\n\n    if self._parent and name in self._parent._params:\n        return self._parent._params[name]\n\n    raise AttributeError(f\"Context has no parameter '{name}'\")\n</code></pre>"},{"location":"api/context/#flowyml.core.context.Context.__getitem__","title":"<code>__getitem__(key: str) -&gt; Any</code>","text":"<p>Allow dict-style access to parameters.</p> Source code in <code>flowyml/core/context.py</code> <pre><code>def __getitem__(self, key: str) -&gt; Any:\n    \"\"\"Allow dict-style access to parameters.\"\"\"\n    if key in self._params:\n        return self._params[key]\n\n    if self._parent and key in self._parent._params:\n        return self._parent._params[key]\n\n    raise KeyError(f\"Context has no parameter '{key}'\")\n</code></pre>"},{"location":"api/context/#flowyml.core.context.Context.get","title":"<code>get(key: str, default: Any = None) -&gt; Any</code>","text":"<p>Get parameter with default value.</p> Source code in <code>flowyml/core/context.py</code> <pre><code>def get(self, key: str, default: Any = None) -&gt; Any:\n    \"\"\"Get parameter with default value.\"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default\n</code></pre>"},{"location":"api/context/#flowyml.core.context.Context.inherit","title":"<code>inherit(**overrides) -&gt; Context</code>","text":"<p>Create child context with inheritance.</p> Source code in <code>flowyml/core/context.py</code> <pre><code>def inherit(self, **overrides) -&gt; \"Context\":\n    \"\"\"Create child context with inheritance.\"\"\"\n    child = Context(**overrides)\n    child._parent = self\n    return child\n</code></pre>"},{"location":"api/context/#flowyml.core.context.Context.inject_params","title":"<code>inject_params(func: callable) -&gt; dict[str, Any]</code>","text":"<p>Automatically inject parameters based on function signature.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>callable</code> <p>Function to analyze and inject parameters for</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of parameters to inject</p> Source code in <code>flowyml/core/context.py</code> <pre><code>def inject_params(self, func: callable) -&gt; dict[str, Any]:\n    \"\"\"Automatically inject parameters based on function signature.\n\n    Args:\n        func: Function to analyze and inject parameters for\n\n    Returns:\n        Dictionary of parameters to inject\n    \"\"\"\n    sig = inspect.signature(func)\n    injected = {}\n\n    for param_name, param in sig.parameters.items():\n        # Skip self, cls, args, kwargs\n        if param_name in (\"self\", \"cls\"):\n            continue\n        if param.kind in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD):\n            continue\n\n        # Check if parameter exists in context\n        if param_name in self.keys():\n            injected[param_name] = self[param_name]\n\n    return injected\n</code></pre>"},{"location":"api/context/#flowyml.core.context.Context.items","title":"<code>items() -&gt; list[tuple]</code>","text":"<p>Return all parameter items.</p> Source code in <code>flowyml/core/context.py</code> <pre><code>def items(self) -&gt; list[tuple]:\n    \"\"\"Return all parameter items.\"\"\"\n    result = {}\n    if self._parent:\n        result.update(dict(self._parent.items()))\n    result.update(self._params)\n    return list(result.items())\n</code></pre>"},{"location":"api/context/#flowyml.core.context.Context.keys","title":"<code>keys() -&gt; set[str]</code>","text":"<p>Return all parameter keys.</p> Source code in <code>flowyml/core/context.py</code> <pre><code>def keys(self) -&gt; set[str]:\n    \"\"\"Return all parameter keys.\"\"\"\n    keys = set(self._params.keys())\n    if self._parent:\n        keys.update(self._parent.keys())\n    return keys\n</code></pre>"},{"location":"api/context/#flowyml.core.context.Context.to_dict","title":"<code>to_dict() -&gt; dict[str, Any]</code>","text":"<p>Convert context to dictionary.</p> Source code in <code>flowyml/core/context.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert context to dictionary.\"\"\"\n    result = {}\n    if self._parent:\n        result.update(self._parent.to_dict())\n    result.update(self._params)\n    return result\n</code></pre>"},{"location":"api/context/#flowyml.core.context.Context.update","title":"<code>update(data: dict[str, Any]) -&gt; None</code>","text":"<p>Update context with new data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>Dictionary of key-value pairs to add to context</p> required Source code in <code>flowyml/core/context.py</code> <pre><code>def update(self, data: dict[str, Any]) -&gt; None:\n    \"\"\"Update context with new data.\n\n    Args:\n        data: Dictionary of key-value pairs to add to context\n    \"\"\"\n    self._params.update(data)\n</code></pre>"},{"location":"api/context/#flowyml.core.context.Context.validate_for_step","title":"<code>validate_for_step(step_func: callable, exclude: list[str] = None) -&gt; list[str]</code>","text":"<p>Validate that all required parameters are available.</p> <p>Parameters:</p> Name Type Description Default <code>step_func</code> <code>callable</code> <p>Step function to validate</p> required <code>exclude</code> <code>list[str]</code> <p>List of parameter names to exclude from validation (e.g. inputs)</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of missing required parameters</p> Source code in <code>flowyml/core/context.py</code> <pre><code>def validate_for_step(self, step_func: callable, exclude: list[str] = None) -&gt; list[str]:\n    \"\"\"Validate that all required parameters are available.\n\n    Args:\n        step_func: Step function to validate\n        exclude: List of parameter names to exclude from validation (e.g. inputs)\n\n    Returns:\n        List of missing required parameters\n    \"\"\"\n    sig = inspect.signature(step_func)\n    missing = []\n    exclude = exclude or []\n\n    for param_name, param in sig.parameters.items():\n        # Skip optional parameters\n        if param_name in (\"self\", \"cls\"):\n            continue\n        if param_name in exclude:\n            continue\n        if param.default != inspect.Parameter.empty:\n            continue\n        if param.kind in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD):\n            continue\n\n        # Check if required param is missing\n        if param_name not in self.keys():\n            missing.append(param_name)\n\n    return missing\n</code></pre>"},{"location":"api/core/","title":"Core API Reference","text":""},{"location":"api/core/#pipeline","title":"Pipeline","text":""},{"location":"api/core/#flowyml.core.pipeline.Pipeline","title":"<code>flowyml.core.pipeline.Pipeline(name: str, context: Context | None = None, executor: Executor | None = None, enable_cache: bool = True, cache_dir: str | None = None, stack: Any | None = None, project: str | None = None)</code>","text":"<p>Main pipeline class for orchestrating ML workflows.</p> Example <p>from flowyml import Pipeline, step, context ctx = context(learning_rate=0.001, epochs=10) @step(outputs=[\"model/trained\"]) ... def train(learning_rate: float, epochs: int): ...     return train_model(learning_rate, epochs) pipeline = Pipeline(\"my_pipeline\", context=ctx) pipeline.add_step(train) result = pipeline.run()</p> <p>Initialize pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the pipeline</p> required <code>context</code> <code>Context | None</code> <p>Optional context for parameter injection</p> <code>None</code> <code>executor</code> <code>Executor | None</code> <p>Optional executor (defaults to LocalExecutor)</p> <code>None</code> <code>enable_cache</code> <code>bool</code> <p>Whether to enable caching</p> <code>True</code> <code>cache_dir</code> <code>str | None</code> <p>Optional directory for cache</p> <code>None</code> <code>stack</code> <code>Any | None</code> <p>Optional stack instance to run on</p> <code>None</code> <code>project</code> <code>str | None</code> <p>Optional project name to attach this pipeline to.</p> <code>None</code> Source code in <code>flowyml/core/pipeline.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    context: Context | None = None,\n    executor: Executor | None = None,\n    enable_cache: bool = True,\n    cache_dir: str | None = None,\n    stack: Any | None = None,  # Stack instance\n    project: str | None = None,  # Project name to attach to\n):\n    \"\"\"Initialize pipeline.\n\n    Args:\n        name: Name of the pipeline\n        context: Optional context for parameter injection\n        executor: Optional executor (defaults to LocalExecutor)\n        enable_cache: Whether to enable caching\n        cache_dir: Optional directory for cache\n        stack: Optional stack instance to run on\n        project: Optional project name to attach this pipeline to.\n    \"\"\"\n    self.name = name\n    self.context = context or Context()\n    self.enable_cache = enable_cache\n    self.stack = stack  # Store stack instance\n\n    self.steps: list[Step] = []\n    self.dag = DAG()\n\n    # Storage\n    if cache_dir is None:\n        from flowyml.utils.config import get_config\n\n        cache_dir = str(get_config().cache_dir)\n\n    self.cache_store = CacheStore(cache_dir) if enable_cache else None\n\n    from flowyml.utils.config import get_config\n\n    self.runs_dir = get_config().runs_dir\n    self.runs_dir.mkdir(parents=True, exist_ok=True)\n\n    # Initialize components from stack or defaults\n    if self.stack:\n        self.executor = executor or self.stack.executor\n        self.metadata_store = self.stack.metadata_store\n    else:\n        self.executor = executor or LocalExecutor()\n        # Metadata store for UI integration\n        from flowyml.storage.metadata import SQLiteMetadataStore\n\n        self.metadata_store = SQLiteMetadataStore()\n\n    # Handle Project Attachment\n    if project:\n        from flowyml.core.project import ProjectManager\n\n        manager = ProjectManager()\n        # Get or create project\n        proj = manager.get_project(project)\n        if not proj:\n            proj = manager.create_project(project)\n\n        # Configure pipeline with project settings\n        self.runs_dir = proj.runs_dir\n        self.metadata_store = proj.metadata_store\n\n        # Register pipeline with project\n        if name not in proj.metadata[\"pipelines\"]:\n            proj.metadata[\"pipelines\"].append(name)\n            proj._save_metadata()\n\n    # State\n    self._built = False\n    self.step_groups: list[Any] = []  # Will hold StepGroup objects\n</code></pre>"},{"location":"api/core/#flowyml.core.pipeline.Pipeline-functions","title":"Functions","text":""},{"location":"api/core/#flowyml.core.pipeline.Pipeline.add_step","title":"<code>add_step(step: Step) -&gt; Pipeline</code>","text":"<p>Add a step to the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>Step</code> <p>Step to add</p> required <p>Returns:</p> Type Description <code>Pipeline</code> <p>Self for chaining</p> Source code in <code>flowyml/core/pipeline.py</code> <pre><code>def add_step(self, step: Step) -&gt; \"Pipeline\":\n    \"\"\"Add a step to the pipeline.\n\n    Args:\n        step: Step to add\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self.steps.append(step)\n    self._built = False\n    return self\n</code></pre>"},{"location":"api/core/#flowyml.core.pipeline.Pipeline.build","title":"<code>build() -&gt; None</code>","text":"<p>Build the execution DAG.</p> Source code in <code>flowyml/core/pipeline.py</code> <pre><code>def build(self) -&gt; None:\n    \"\"\"Build the execution DAG.\"\"\"\n    if self._built:\n        return\n\n    # Clear previous DAG\n    self.dag = DAG()\n\n    # Add nodes\n    for step in self.steps:\n        node = Node(\n            name=step.name,\n            step=step,\n            inputs=step.inputs,\n            outputs=step.outputs,\n        )\n        self.dag.add_node(node)\n\n    # Build edges\n    self.dag.build_edges()\n\n    # Validate\n    errors = self.dag.validate()\n    if errors:\n        raise ValueError(\"Pipeline validation failed:\\n\" + \"\\n\".join(errors))\n\n    # Analyze step groups\n    from flowyml.core.step_grouping import StepGroupAnalyzer\n\n    analyzer = StepGroupAnalyzer()\n    self.step_groups = analyzer.analyze_groups(self.dag, self.steps)\n\n    self._built = True\n</code></pre>"},{"location":"api/core/#flowyml.core.pipeline.Pipeline.cache_stats","title":"<code>cache_stats() -&gt; dict[str, Any]</code>","text":"<p>Get cache statistics.</p> Source code in <code>flowyml/core/pipeline.py</code> <pre><code>def cache_stats(self) -&gt; dict[str, Any]:\n    \"\"\"Get cache statistics.\"\"\"\n    if self.cache_store:\n        return self.cache_store.stats()\n    return {}\n</code></pre>"},{"location":"api/core/#flowyml.core.pipeline.Pipeline.from_definition","title":"<code>from_definition(definition: dict, context: Context | None = None) -&gt; Pipeline</code>  <code>classmethod</code>","text":"<p>Reconstruct pipeline from stored definition.</p> <p>This creates a \"ghost\" pipeline that can be executed but uses the stored step structure. Actual step logic must still be available in the codebase.</p> <p>Parameters:</p> Name Type Description Default <code>definition</code> <code>dict</code> <p>Pipeline definition from to_definition()</p> required <code>context</code> <code>Context | None</code> <p>Optional context for execution</p> <code>None</code> <p>Returns:</p> Type Description <code>Pipeline</code> <p>Reconstructed Pipeline instance</p> Source code in <code>flowyml/core/pipeline.py</code> <pre><code>@classmethod\ndef from_definition(cls, definition: dict, context: Context | None = None) -&gt; \"Pipeline\":\n    \"\"\"Reconstruct pipeline from stored definition.\n\n    This creates a \"ghost\" pipeline that can be executed but uses\n    the stored step structure. Actual step logic must still be\n    available in the codebase.\n\n    Args:\n        definition: Pipeline definition from to_definition()\n        context: Optional context for execution\n\n    Returns:\n        Reconstructed Pipeline instance\n    \"\"\"\n    from flowyml.core.step import step as step_decorator\n\n    # Create pipeline instance\n    pipeline = cls(\n        name=definition[\"name\"],\n        context=context or Context(),\n    )\n\n    # Reconstruct steps\n    for step_def in definition[\"steps\"]:\n        # Create a generic step function that can be called\n        # In a real implementation, we'd need to either:\n        # 1. Store serialized functions (using cloudpickle)\n        # 2. Import functions by name from codebase\n        # 3. Use placeholder functions\n\n        # For now, we'll create a placeholder that logs execution\n        def generic_step_func(*args, **kwargs):\n            \"\"\"Generic step function for reconstructed pipeline.\"\"\"\n            print(f\"Executing reconstructed step with args={args}, kwargs={kwargs}\")\n            return\n\n        # Apply step decorator with stored metadata\n        decorated = step_decorator(\n            name=step_def[\"name\"],\n            inputs=step_def[\"inputs\"],\n            outputs=step_def[\"outputs\"],\n            tags=step_def.get(\"tags\", []),\n        )(generic_step_func)\n\n        # Add to pipeline\n        pipeline.add_step(decorated)\n\n    return pipeline\n</code></pre>"},{"location":"api/core/#flowyml.core.pipeline.Pipeline.invalidate_cache","title":"<code>invalidate_cache(step: str | None = None, before: str | None = None) -&gt; None</code>","text":"<p>Invalidate cache entries.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>str | None</code> <p>Invalidate cache for specific step</p> <code>None</code> <code>before</code> <code>str | None</code> <p>Invalidate cache entries before date</p> <code>None</code> Source code in <code>flowyml/core/pipeline.py</code> <pre><code>def invalidate_cache(\n    self,\n    step: str | None = None,\n    before: str | None = None,\n) -&gt; None:\n    \"\"\"Invalidate cache entries.\n\n    Args:\n        step: Invalidate cache for specific step\n        before: Invalidate cache entries before date\n    \"\"\"\n    if self.cache_store:\n        if step:\n            self.cache_store.invalidate(step_name=step)\n        else:\n            self.cache_store.clear()\n</code></pre>"},{"location":"api/core/#flowyml.core.pipeline.Pipeline.run","title":"<code>run(inputs: dict[str, Any] | None = None, debug: bool = False, stack: Any | None = None, resources: Any | None = None, docker_config: Any | None = None, context: dict[str, Any] | None = None) -&gt; PipelineResult</code>","text":"<p>Execute the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any] | None</code> <p>Optional input data for the pipeline</p> <code>None</code> <code>debug</code> <code>bool</code> <p>Enable debug mode with detailed logging</p> <code>False</code> <code>stack</code> <code>Any | None</code> <p>Stack override (uses self.stack if not provided)</p> <code>None</code> <code>resources</code> <code>Any | None</code> <p>Resource configuration for execution</p> <code>None</code> <code>docker_config</code> <code>Any | None</code> <p>Docker configuration for containerized execution</p> <code>None</code> <code>context</code> <code>dict[str, Any] | None</code> <p>Context variables override</p> <code>None</code> <p>Returns:</p> Type Description <code>PipelineResult</code> <p>PipelineResult with outputs and execution info</p> Source code in <code>flowyml/core/pipeline.py</code> <pre><code>def run(\n    self,\n    inputs: dict[str, Any] | None = None,\n    debug: bool = False,\n    stack: Any | None = None,  # Stack override\n    resources: Any | None = None,  # ResourceConfig\n    docker_config: Any | None = None,  # DockerConfig\n    context: dict[str, Any] | None = None,  # Context vars override\n) -&gt; PipelineResult:\n    \"\"\"Execute the pipeline.\n\n    Args:\n        inputs: Optional input data for the pipeline\n        debug: Enable debug mode with detailed logging\n        stack: Stack override (uses self.stack if not provided)\n        resources: Resource configuration for execution\n        docker_config: Docker configuration for containerized execution\n        context: Context variables override\n\n    Returns:\n        PipelineResult with outputs and execution info\n    \"\"\"\n    import uuid\n\n    run_id = str(uuid.uuid4())\n\n    # Use provided stack or instance stack\n    if stack is not None:\n        self.stack = stack\n        # Update components from new stack\n        self.executor = self.stack.executor\n        self.metadata_store = self.stack.metadata_store\n\n    # Determine artifact store\n    artifact_store = None\n    if self.stack:\n        artifact_store = self.stack.artifact_store\n\n    # Update context with provided values\n    if context:\n        self.context.update(context)\n\n    # Build DAG if needed\n    if not self._built:\n        self.build()\n\n    # Initialize result\n    result = PipelineResult(run_id, self.name)\n    step_outputs = inputs or {}\n\n    # Map step names to step objects for easier lookup\n    self.steps_dict = {step.name: step for step in self.steps}\n    if debug:\n        pass\n    else:\n        # Always print the run URL for better UX\n        pass\n\n    # Get execution units (individual steps or groups)\n    from flowyml.core.step_grouping import get_execution_units\n\n    execution_units = get_execution_units(self.dag, self.steps)\n\n    # Execute steps/groups in order\n    for unit in execution_units:\n        # Check if unit is a group or individual step\n        from flowyml.core.step_grouping import StepGroup\n\n        if isinstance(unit, StepGroup):\n            # Execute entire group\n            if debug:\n                pass\n\n            # Get context parameters (use first step's function as representative)\n            first_step = unit.steps[0]\n            context_params = self.context.inject_params(first_step.func)\n\n            # Execute the group\n            group_results = self.executor.execute_step_group(\n                step_group=unit,\n                inputs=step_outputs,\n                context_params=context_params,\n                cache_store=self.cache_store,\n                artifact_store=artifact_store,\n                run_id=run_id,\n                project_name=self.name,\n            )\n\n            # Process each step result\n            for step_result in group_results:\n                result.add_step_result(step_result)\n\n                if debug:\n                    pass\n\n                # Handle failure\n                if not step_result.success and not step_result.skipped:\n                    result.finalize(success=False)\n                    self._save_run(result)\n                    return result\n\n                # Store outputs for next steps/groups\n                if step_result.output is not None:\n                    # Find step definition to get output names\n                    step_def = next((s for s in self.steps if s.name == step_result.step_name), None)\n                    if step_def:\n                        if len(step_def.outputs) == 1:\n                            step_outputs[step_def.outputs[0]] = step_result.output\n                            result.outputs[step_def.outputs[0]] = step_result.output\n                        elif isinstance(step_result.output, (list, tuple)) and len(step_result.output) == len(\n                            step_def.outputs,\n                        ):\n                            for name, val in zip(step_def.outputs, step_result.output, strict=False):\n                                step_outputs[name] = val\n                                result.outputs[name] = val\n                        elif isinstance(step_result.output, dict):\n                            for name in step_def.outputs:\n                                if name in step_result.output:\n                                    step_outputs[name] = step_result.output[name]\n                                    result.outputs[name] = step_result.output[name]\n                        else:\n                            if step_def.outputs:\n                                step_outputs[step_def.outputs[0]] = step_result.output\n                                result.outputs[step_def.outputs[0]] = step_result.output\n\n        else:\n            # Execute single ungrouped step\n            step = unit\n\n            if debug:\n                pass\n\n            # Prepare step inputs\n            step_inputs = {}\n\n            # Get function signature to map inputs to parameters\n            import inspect\n\n            sig = inspect.signature(step.func)\n            params = list(sig.parameters.values())\n\n            # Filter out self/cls\n            params = [p for p in params if p.name not in (\"self\", \"cls\")]\n\n            # Strategy:\n            # 1. Map inputs to parameters\n            #    - If input name matches param name, use it\n            #    - If not, use positional mapping (input[i] -&gt; param[i])\n\n            # Track which parameters have been assigned\n            assigned_params = set()\n\n            if step.inputs:\n                for i, input_name in enumerate(step.inputs):\n                    if input_name not in step_outputs:\n                        continue\n\n                    val = step_outputs[input_name]\n\n                    # Check if input name matches a parameter\n                    param_match = next((p for p in params if p.name == input_name), None)\n\n                    if param_match:\n                        step_inputs[param_match.name] = val\n                        assigned_params.add(param_match.name)\n                    elif i &lt; len(params):\n                        # Positional fallback\n                        # Only if this parameter hasn't been assigned yet\n                        target_param = params[i]\n                        if target_param.name not in assigned_params:\n                            step_inputs[target_param.name] = val\n                            assigned_params.add(target_param.name)\n\n            # Auto-map parameters from available outputs if they match function signature\n            # This allows passing inputs to run() without declaring them as asset dependencies\n            for param in params:\n                if param.name in step_outputs and param.name not in step_inputs:\n                    step_inputs[param.name] = step_outputs[param.name]\n                    assigned_params.add(param.name)\n\n            # Validate context parameters\n            # Exclude parameters that are already provided in step_inputs\n            exclude_params = list(step.inputs) + list(step_inputs.keys())\n            missing_params = self.context.validate_for_step(step.func, exclude=exclude_params)\n            if missing_params:\n                if debug:\n                    pass\n\n                error_msg = f\"Missing required parameters: {missing_params}\"\n                step_result = ExecutionResult(\n                    step_name=step.name,\n                    success=False,\n                    error=error_msg,\n                )\n                result.add_step_result(step_result)\n                result.finalize(success=False)\n                self._save_run(result)  # Save run before returning\n                self._save_pipeline_definition()  # Save definition even on failure\n                print(\"DEBUG: Pipeline failed at step execution\")\n                return result\n\n            # Get context parameters for this step\n            context_params = self.context.inject_params(step.func)\n\n            # Execute step\n            step_result = self.executor.execute_step(\n                step,\n                step_inputs,\n                context_params,\n                self.cache_store,\n                artifact_store=artifact_store,\n                run_id=run_id,\n                project_name=self.name,\n            )\n\n            result.add_step_result(step_result)\n\n            if debug:\n                pass\n\n            # Handle failure\n            if not step_result.success:\n                if debug and not step_result.error:\n                    pass\n                result.finalize(success=False)\n                self._save_run(result)\n                self._save_pipeline_definition()  # Save definition even on failure\n                print(\"DEBUG: Pipeline failed at step execution\")\n                return result\n\n            # Store outputs for next steps\n            if step_result.output is not None:\n                if len(step.outputs) == 1:\n                    step_outputs[step.outputs[0]] = step_result.output\n                    result.outputs[step.outputs[0]] = step_result.output\n                elif isinstance(step_result.output, (list, tuple)) and len(step_result.output) == len(step.outputs):\n                    for name, val in zip(step.outputs, step_result.output, strict=False):\n                        step_outputs[name] = val\n                        result.outputs[name] = val\n                elif isinstance(step_result.output, dict):\n                    for name in step.outputs:\n                        if name in step_result.output:\n                            step_outputs[name] = step_result.output[name]\n                            result.outputs[name] = step_result.output[name]\n                else:\n                    # Fallback: assign to first output if available\n                    if step.outputs:\n                        step_outputs[step.outputs[0]] = step_result.output\n                        result.outputs[step.outputs[0]] = step_result.output\n\n    # Success!\n    result.finalize(success=True)\n\n    if debug:\n        pass\n\n    self._save_run(result)\n    self._save_pipeline_definition()  # Save pipeline structure for scheduling\n    return result\n</code></pre>"},{"location":"api/core/#flowyml.core.pipeline.Pipeline.to_definition","title":"<code>to_definition() -&gt; dict</code>","text":"<p>Serialize pipeline to definition for storage and reconstruction.</p> Source code in <code>flowyml/core/pipeline.py</code> <pre><code>def to_definition(self) -&gt; dict:\n    \"\"\"Serialize pipeline to definition for storage and reconstruction.\"\"\"\n    if not self._built:\n        self.build()\n\n    return {\n        \"name\": self.name,\n        \"steps\": [\n            {\n                \"name\": step.name,\n                \"inputs\": step.inputs,\n                \"outputs\": step.outputs,\n                \"source_code\": step.source_code,\n                \"tags\": step.tags,\n            }\n            for step in self.steps\n        ],\n        \"dag\": {\n            \"nodes\": [\n                {\n                    \"name\": node.name,\n                    \"inputs\": node.inputs,\n                    \"outputs\": node.outputs,\n                }\n                for node in self.dag.nodes.values()\n            ],\n            \"edges\": [\n                {\"source\": dep, \"target\": node_name} for node_name, deps in self.dag.edges.items() for dep in deps\n            ],\n        },\n    }\n</code></pre>"},{"location":"api/core/#flowyml.core.pipeline.Pipeline.visualize","title":"<code>visualize() -&gt; str</code>","text":"<p>Generate pipeline visualization.</p> Source code in <code>flowyml/core/pipeline.py</code> <pre><code>def visualize(self) -&gt; str:\n    \"\"\"Generate pipeline visualization.\"\"\"\n    if not self._built:\n        self.build()\n    return self.dag.visualize()\n</code></pre>"},{"location":"api/core/#step","title":"Step","text":""},{"location":"api/core/#flowyml.core.step.Step","title":"<code>flowyml.core.step.Step(func: Callable, name: str | None = None, inputs: list[str] | None = None, outputs: list[str] | None = None, cache: bool | str | Callable = 'code_hash', retry: int = 0, timeout: int | None = None, resources: Union[dict[str, Any], ResourceRequirements, None] = None, tags: dict[str, str] | None = None, condition: Callable | None = None, execution_group: str | None = None)</code>","text":"<p>A pipeline step that can be executed with automatic context injection.</p> Source code in <code>flowyml/core/step.py</code> <pre><code>def __init__(\n    self,\n    func: Callable,\n    name: str | None = None,\n    inputs: list[str] | None = None,\n    outputs: list[str] | None = None,\n    cache: bool | str | Callable = \"code_hash\",\n    retry: int = 0,\n    timeout: int | None = None,\n    resources: Union[dict[str, Any], \"ResourceRequirements\", None] = None,\n    tags: dict[str, str] | None = None,\n    condition: Callable | None = None,\n    execution_group: str | None = None,\n):\n    self.func = func\n    self.name = name or func.__name__\n    self.inputs = inputs or []\n    self.outputs = outputs or []\n    self.cache = cache\n    self.retry = retry\n    self.timeout = timeout\n\n    # Store resources (accept both dict for backward compatibility and ResourceRequirements)\n    self.resources = resources\n\n    self.tags = tags or {}\n    self.condition = condition\n    self.execution_group = execution_group\n\n    # Capture source code for UI display\n    try:\n        self.source_code = inspect.getsource(func)\n    except (OSError, TypeError):\n        self.source_code = \"# Source code not available\"\n\n    self.config = StepConfig(\n        name=self.name,\n        func=func,\n        inputs=self.inputs,\n        outputs=self.outputs,\n        cache=self.cache,\n        retry=self.retry,\n        timeout=self.timeout,\n        resources=self.resources,\n        tags=self.tags,\n        condition=self.condition,\n        execution_group=self.execution_group,\n    )\n</code></pre>"},{"location":"api/core/#flowyml.core.step.Step-functions","title":"Functions","text":""},{"location":"api/core/#flowyml.core.step.Step.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Execute the step function.</p> Source code in <code>flowyml/core/step.py</code> <pre><code>def __call__(self, *args, **kwargs):\n    \"\"\"Execute the step function.\"\"\"\n    # Check condition if present\n    if self.condition:\n        # We might need to inject context into condition too,\n        # but for now assume it takes no args or same args as step?\n        # This is tricky without context injection logic here.\n        # The executor handles execution, so maybe we just store it here.\n        pass\n\n    return self.func(*args, **kwargs)\n</code></pre>"},{"location":"api/core/#flowyml.core.step.Step.get_cache_key","title":"<code>get_cache_key(inputs: dict[str, Any] | None = None) -&gt; str</code>","text":"<p>Generate cache key based on caching strategy.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any] | None</code> <p>Input data for the step</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Cache key string</p> Source code in <code>flowyml/core/step.py</code> <pre><code>def get_cache_key(self, inputs: dict[str, Any] | None = None) -&gt; str:\n    \"\"\"Generate cache key based on caching strategy.\n\n    Args:\n        inputs: Input data for the step\n\n    Returns:\n        Cache key string\n    \"\"\"\n    if self.cache == \"code_hash\":\n        return f\"{self.name}:{self.get_code_hash()}\"\n    elif self.cache == \"input_hash\" and inputs:\n        return f\"{self.name}:{self.get_input_hash(inputs)}\"\n    elif callable(self.cache) and inputs:\n        return self.cache(inputs, {})\n    else:\n        return f\"{self.name}:no-cache\"\n</code></pre>"},{"location":"api/core/#flowyml.core.step.Step.get_code_hash","title":"<code>get_code_hash() -&gt; str</code>","text":"<p>Compute hash of the step's source code.</p> Source code in <code>flowyml/core/step.py</code> <pre><code>def get_code_hash(self) -&gt; str:\n    \"\"\"Compute hash of the step's source code.\"\"\"\n    try:\n        source = inspect.getsource(self.func)\n        return hashlib.md5(source.encode()).hexdigest()\n    except (OSError, TypeError):\n        # Fallback for dynamically defined functions or when source is unavailable\n        return hashlib.md5(self.name.encode()).hexdigest()[:16]\n</code></pre>"},{"location":"api/core/#flowyml.core.step.Step.get_input_hash","title":"<code>get_input_hash(inputs: dict[str, Any]) -&gt; str</code>","text":"<p>Generate hash of inputs for caching.</p> Source code in <code>flowyml/core/step.py</code> <pre><code>def get_input_hash(self, inputs: dict[str, Any]) -&gt; str:\n    \"\"\"Generate hash of inputs for caching.\"\"\"\n    input_str = json.dumps(inputs, sort_keys=True, default=str)\n    return hashlib.sha256(input_str.encode()).hexdigest()[:16]\n</code></pre>"},{"location":"api/core/#context","title":"Context","text":""},{"location":"api/core/#flowyml.core.context.Context","title":"<code>flowyml.core.context.Context(**kwargs)</code>","text":"<p>Pipeline context with automatic parameter injection.</p> Example <p>ctx = Context(learning_rate=0.001, epochs=10, batch_size=32, device=\"cuda\")</p> Source code in <code>flowyml/core/context.py</code> <pre><code>def __init__(self, **kwargs):\n    self._params = kwargs\n    self._parent = None\n    self._metadata = {}\n</code></pre>"},{"location":"api/core/#flowyml.core.context.Context-functions","title":"Functions","text":""},{"location":"api/core/#flowyml.core.context.Context.__getattr__","title":"<code>__getattr__(name: str) -&gt; Any</code>","text":"<p>Allow dot notation access to parameters.</p> Source code in <code>flowyml/core/context.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Any:\n    \"\"\"Allow dot notation access to parameters.\"\"\"\n    if name.startswith(\"_\"):\n        return super().__getattribute__(name)\n\n    if name in self._params:\n        return self._params[name]\n\n    if self._parent and name in self._parent._params:\n        return self._parent._params[name]\n\n    raise AttributeError(f\"Context has no parameter '{name}'\")\n</code></pre>"},{"location":"api/core/#flowyml.core.context.Context.__getitem__","title":"<code>__getitem__(key: str) -&gt; Any</code>","text":"<p>Allow dict-style access to parameters.</p> Source code in <code>flowyml/core/context.py</code> <pre><code>def __getitem__(self, key: str) -&gt; Any:\n    \"\"\"Allow dict-style access to parameters.\"\"\"\n    if key in self._params:\n        return self._params[key]\n\n    if self._parent and key in self._parent._params:\n        return self._parent._params[key]\n\n    raise KeyError(f\"Context has no parameter '{key}'\")\n</code></pre>"},{"location":"api/core/#flowyml.core.context.Context.get","title":"<code>get(key: str, default: Any = None) -&gt; Any</code>","text":"<p>Get parameter with default value.</p> Source code in <code>flowyml/core/context.py</code> <pre><code>def get(self, key: str, default: Any = None) -&gt; Any:\n    \"\"\"Get parameter with default value.\"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default\n</code></pre>"},{"location":"api/core/#flowyml.core.context.Context.inherit","title":"<code>inherit(**overrides) -&gt; Context</code>","text":"<p>Create child context with inheritance.</p> Source code in <code>flowyml/core/context.py</code> <pre><code>def inherit(self, **overrides) -&gt; \"Context\":\n    \"\"\"Create child context with inheritance.\"\"\"\n    child = Context(**overrides)\n    child._parent = self\n    return child\n</code></pre>"},{"location":"api/core/#flowyml.core.context.Context.inject_params","title":"<code>inject_params(func: callable) -&gt; dict[str, Any]</code>","text":"<p>Automatically inject parameters based on function signature.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>callable</code> <p>Function to analyze and inject parameters for</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of parameters to inject</p> Source code in <code>flowyml/core/context.py</code> <pre><code>def inject_params(self, func: callable) -&gt; dict[str, Any]:\n    \"\"\"Automatically inject parameters based on function signature.\n\n    Args:\n        func: Function to analyze and inject parameters for\n\n    Returns:\n        Dictionary of parameters to inject\n    \"\"\"\n    sig = inspect.signature(func)\n    injected = {}\n\n    for param_name, param in sig.parameters.items():\n        # Skip self, cls, args, kwargs\n        if param_name in (\"self\", \"cls\"):\n            continue\n        if param.kind in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD):\n            continue\n\n        # Check if parameter exists in context\n        if param_name in self.keys():\n            injected[param_name] = self[param_name]\n\n    return injected\n</code></pre>"},{"location":"api/core/#flowyml.core.context.Context.items","title":"<code>items() -&gt; list[tuple]</code>","text":"<p>Return all parameter items.</p> Source code in <code>flowyml/core/context.py</code> <pre><code>def items(self) -&gt; list[tuple]:\n    \"\"\"Return all parameter items.\"\"\"\n    result = {}\n    if self._parent:\n        result.update(dict(self._parent.items()))\n    result.update(self._params)\n    return list(result.items())\n</code></pre>"},{"location":"api/core/#flowyml.core.context.Context.keys","title":"<code>keys() -&gt; set[str]</code>","text":"<p>Return all parameter keys.</p> Source code in <code>flowyml/core/context.py</code> <pre><code>def keys(self) -&gt; set[str]:\n    \"\"\"Return all parameter keys.\"\"\"\n    keys = set(self._params.keys())\n    if self._parent:\n        keys.update(self._parent.keys())\n    return keys\n</code></pre>"},{"location":"api/core/#flowyml.core.context.Context.to_dict","title":"<code>to_dict() -&gt; dict[str, Any]</code>","text":"<p>Convert context to dictionary.</p> Source code in <code>flowyml/core/context.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert context to dictionary.\"\"\"\n    result = {}\n    if self._parent:\n        result.update(self._parent.to_dict())\n    result.update(self._params)\n    return result\n</code></pre>"},{"location":"api/core/#flowyml.core.context.Context.update","title":"<code>update(data: dict[str, Any]) -&gt; None</code>","text":"<p>Update context with new data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>Dictionary of key-value pairs to add to context</p> required Source code in <code>flowyml/core/context.py</code> <pre><code>def update(self, data: dict[str, Any]) -&gt; None:\n    \"\"\"Update context with new data.\n\n    Args:\n        data: Dictionary of key-value pairs to add to context\n    \"\"\"\n    self._params.update(data)\n</code></pre>"},{"location":"api/core/#flowyml.core.context.Context.validate_for_step","title":"<code>validate_for_step(step_func: callable, exclude: list[str] = None) -&gt; list[str]</code>","text":"<p>Validate that all required parameters are available.</p> <p>Parameters:</p> Name Type Description Default <code>step_func</code> <code>callable</code> <p>Step function to validate</p> required <code>exclude</code> <code>list[str]</code> <p>List of parameter names to exclude from validation (e.g. inputs)</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of missing required parameters</p> Source code in <code>flowyml/core/context.py</code> <pre><code>def validate_for_step(self, step_func: callable, exclude: list[str] = None) -&gt; list[str]:\n    \"\"\"Validate that all required parameters are available.\n\n    Args:\n        step_func: Step function to validate\n        exclude: List of parameter names to exclude from validation (e.g. inputs)\n\n    Returns:\n        List of missing required parameters\n    \"\"\"\n    sig = inspect.signature(step_func)\n    missing = []\n    exclude = exclude or []\n\n    for param_name, param in sig.parameters.items():\n        # Skip optional parameters\n        if param_name in (\"self\", \"cls\"):\n            continue\n        if param_name in exclude:\n            continue\n        if param.default != inspect.Parameter.empty:\n            continue\n        if param.kind in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD):\n            continue\n\n        # Check if required param is missing\n        if param_name not in self.keys():\n            missing.append(param_name)\n\n    return missing\n</code></pre>"},{"location":"api/decorators/","title":"Decorators API \ud83c\udf80","text":""},{"location":"api/decorators/#step","title":"<code>@step</code>","text":"<p>Decorator to define a pipeline step with automatic context injection.</p> <p>Can be used as @step or @step(inputs=...)</p> <p>Parameters:</p> Name Type Description Default <code>_func</code> <code>Callable | None</code> <p>Function being decorated (when used as @step)</p> <code>None</code> <code>inputs</code> <code>list[str] | None</code> <p>List of input asset names</p> <code>None</code> <code>outputs</code> <code>list[str] | None</code> <p>List of output asset names</p> <code>None</code> <code>cache</code> <code>bool | str | Callable</code> <p>Caching strategy (\"code_hash\", \"input_hash\", callable, or False)</p> <code>'code_hash'</code> <code>retry</code> <code>int</code> <p>Number of retry attempts on failure</p> <code>0</code> <code>timeout</code> <code>int | None</code> <p>Maximum execution time in seconds</p> <code>None</code> <code>resources</code> <code>Union[dict[str, Any], ResourceRequirements, None]</code> <p>Resource requirements (ResourceRequirements object or dict for backward compat)</p> <code>None</code> <code>tags</code> <code>dict[str, str] | None</code> <p>Metadata tags for the step</p> <code>None</code> <code>name</code> <code>str | None</code> <p>Optional custom name for the step</p> <code>None</code> <code>condition</code> <code>Callable | None</code> <p>Optional callable that returns True if step should run</p> <code>None</code> <code>execution_group</code> <code>str | None</code> <p>Optional group name for executing multiple steps together</p> <code>None</code> Example <p>@step ... def simple_step(): ...     ... @step(inputs=[\"data/train\"], outputs=[\"model/trained\"]) ... def train_model(train_data): ...     ...</p> Source code in <code>flowyml/core/step.py</code> <pre><code>def step(\n    _func: Callable | None = None,\n    *,\n    inputs: list[str] | None = None,\n    outputs: list[str] | None = None,\n    cache: bool | str | Callable = \"code_hash\",\n    retry: int = 0,\n    timeout: int | None = None,\n    resources: Union[dict[str, Any], \"ResourceRequirements\", None] = None,\n    tags: dict[str, str] | None = None,\n    name: str | None = None,\n    condition: Callable | None = None,\n    execution_group: str | None = None,\n):\n    \"\"\"Decorator to define a pipeline step with automatic context injection.\n\n    Can be used as @step or @step(inputs=...)\n\n    Args:\n        _func: Function being decorated (when used as @step)\n        inputs: List of input asset names\n        outputs: List of output asset names\n        cache: Caching strategy (\"code_hash\", \"input_hash\", callable, or False)\n        retry: Number of retry attempts on failure\n        timeout: Maximum execution time in seconds\n        resources: Resource requirements (ResourceRequirements object or dict for backward compat)\n        tags: Metadata tags for the step\n        name: Optional custom name for the step\n        condition: Optional callable that returns True if step should run\n        execution_group: Optional group name for executing multiple steps together\n\n    Example:\n        &gt;&gt;&gt; @step\n        ... def simple_step():\n        ...     ...\n        &gt;&gt;&gt; @step(inputs=[\"data/train\"], outputs=[\"model/trained\"])\n        ... def train_model(train_data):\n        ...     ...\n        &gt;&gt;&gt; # With resource requirements\n        &gt;&gt;&gt; from flowyml.core.resources import ResourceRequirements, GPUConfig\n        &gt;&gt;&gt; @step(resources=ResourceRequirements(cpu=\"4\", memory=\"16Gi\", gpu=GPUConfig(gpu_type=\"nvidia-v100\", count=2)))\n        ... def gpu_train(data):\n        ...     ...\n    \"\"\"\n\n    def decorator(func: Callable) -&gt; Step:\n        return Step(\n            func=func,\n            name=name,\n            inputs=inputs,\n            outputs=outputs,\n            cache=cache,\n            retry=retry,\n            timeout=timeout,\n            resources=resources,\n            tags=tags,\n            condition=condition,\n            execution_group=execution_group,\n        )\n\n    if _func is None:\n        return decorator\n    else:\n        return decorator(_func)\n</code></pre>"},{"location":"api/decorators/#flowyml.core.step.step--with-resource-requirements","title":"With resource requirements","text":"<p>from flowyml.core.resources import ResourceRequirements, GPUConfig @step(resources=ResourceRequirements(cpu=\"4\", memory=\"16Gi\", gpu=GPUConfig(gpu_type=\"nvidia-v100\", count=2))) ... def gpu_train(data): ...     ...</p>"},{"location":"api/decorators/#trace_llm","title":"<code>@trace_llm</code>","text":"<p>Decorator to trace LLM calls.</p> Source code in <code>flowyml/monitoring/llm.py</code> <pre><code>def trace_llm(name: str = None, event_type: str = \"llm\"):\n    \"\"\"Decorator to trace LLM calls.\"\"\"\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            event_name = name or func.__name__\n\n            # Capture inputs\n            inputs = {\n                \"args\": [str(a) for a in args],\n                \"kwargs\": {k: str(v) for k, v in kwargs.items()},\n            }\n\n            tracer.start_event(event_name, event_type, inputs)\n\n            try:\n                result = func(*args, **kwargs)\n\n                # Try to extract metrics if result has them (e.g. OpenAI response)\n                metrics = {}\n                if hasattr(result, \"usage\"):  # OpenAI style\n                    metrics[\"prompt_tokens\"] = getattr(result.usage, \"prompt_tokens\", 0)\n                    metrics[\"completion_tokens\"] = getattr(result.usage, \"completion_tokens\", 0)\n                    metrics[\"total_tokens\"] = getattr(result.usage, \"total_tokens\", 0)\n\n                tracer.end_event(outputs={\"result\": str(result)}, metrics=metrics)\n                return result\n            except Exception as e:\n                tracer.end_event(error=str(e))\n                raise e\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"api/exceptions/","title":"Exceptions API \ud83d\udea8","text":"<p>Custom exceptions thrown by flowyml.</p>"},{"location":"api/exceptions/#error-handling-module","title":"Error Handling Module","text":"<p>Error handling utilities for robust pipeline execution.</p>"},{"location":"api/exceptions/#flowyml.core.error_handling-classes","title":"Classes","text":""},{"location":"api/exceptions/#flowyml.core.error_handling.CircuitBreaker","title":"<code>CircuitBreaker(failure_threshold: int = 5, timeout: float = 60, recovery_timeout: float = 300, expected_exceptions: list[type[Exception]] | None = None)</code>","text":"<p>Circuit breaker pattern implementation.</p> <p>Prevents cascading failures by failing fast when a service is down.</p> Example <pre><code>@step(circuit_breaker=CircuitBreaker(failure_threshold=3, timeout=60))\ndef call_api(url):\n    return requests.get(url).json()\n</code></pre> <p>Initialize circuit breaker.</p> <p>Parameters:</p> Name Type Description Default <code>failure_threshold</code> <code>int</code> <p>Number of failures before opening circuit</p> <code>5</code> <code>timeout</code> <code>float</code> <p>Seconds to wait before trying again</p> <code>60</code> <code>recovery_timeout</code> <code>float</code> <p>Seconds to wait before fully closing circuit</p> <code>300</code> <code>expected_exceptions</code> <code>list[type[Exception]] | None</code> <p>Exceptions that trigger the breaker</p> <code>None</code> Source code in <code>flowyml/core/error_handling.py</code> <pre><code>def __init__(\n    self,\n    failure_threshold: int = 5,\n    timeout: float = 60,\n    recovery_timeout: float = 300,\n    expected_exceptions: list[type[Exception]] | None = None,\n):\n    \"\"\"Initialize circuit breaker.\n\n    Args:\n        failure_threshold: Number of failures before opening circuit\n        timeout: Seconds to wait before trying again\n        recovery_timeout: Seconds to wait before fully closing circuit\n        expected_exceptions: Exceptions that trigger the breaker\n    \"\"\"\n    self.config = CircuitBreakerConfig(\n        failure_threshold=failure_threshold,\n        timeout=timeout,\n        recovery_timeout=recovery_timeout,\n        expected_exceptions=expected_exceptions or [Exception],\n    )\n\n    self.state = CircuitState.CLOSED\n    self.failure_count = 0\n    self.last_failure_time: datetime | None = None\n    self.success_count = 0\n</code></pre>"},{"location":"api/exceptions/#flowyml.core.error_handling.CircuitBreaker-functions","title":"Functions","text":""},{"location":"api/exceptions/#flowyml.core.error_handling.CircuitBreaker.call","title":"<code>call(func: Callable, *args, **kwargs) -&gt; Any</code>","text":"<p>Call function with circuit breaker protection.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>Function to call</p> required <code>*args</code> <p>Positional arguments</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Function result</p> <p>Raises:</p> Type Description <code>CircuitOpenError</code> <p>If circuit is open</p> Source code in <code>flowyml/core/error_handling.py</code> <pre><code>def call(self, func: Callable, *args, **kwargs) -&gt; Any:\n    \"\"\"Call function with circuit breaker protection.\n\n    Args:\n        func: Function to call\n        *args: Positional arguments\n        **kwargs: Keyword arguments\n\n    Returns:\n        Function result\n\n    Raises:\n        CircuitOpenError: If circuit is open\n    \"\"\"\n    if self.state == CircuitState.OPEN:\n        if self._should_attempt_reset():\n            self.state = CircuitState.HALF_OPEN\n        else:\n            raise CircuitOpenError(\n                f\"Circuit breaker is open. Wait {self.config.timeout}s before retry.\",\n            )\n\n    try:\n        result = func(*args, **kwargs)\n        self._on_success()\n        return result\n\n    except Exception as e:\n        if self._is_expected_exception(e):\n            self._on_failure()\n        raise\n</code></pre>"},{"location":"api/exceptions/#flowyml.core.error_handling.CircuitBreaker.reset","title":"<code>reset() -&gt; None</code>","text":"<p>Reset circuit breaker to closed state.</p> Source code in <code>flowyml/core/error_handling.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset circuit breaker to closed state.\"\"\"\n    self.state = CircuitState.CLOSED\n    self.failure_count = 0\n    self.success_count = 0\n    self.last_failure_time = None\n</code></pre>"},{"location":"api/exceptions/#flowyml.core.error_handling.CircuitBreakerConfig","title":"<code>CircuitBreakerConfig(failure_threshold: int = 5, timeout: float = 60, recovery_timeout: float = 300, expected_exceptions: list[type[Exception]] = (lambda: [Exception])())</code>  <code>dataclass</code>","text":"<p>Configuration for circuit breaker.</p>"},{"location":"api/exceptions/#flowyml.core.error_handling.CircuitBreakerConfig-attributes","title":"Attributes","text":""},{"location":"api/exceptions/#flowyml.core.error_handling.CircuitBreakerConfig.expected_exceptions","title":"<code>expected_exceptions: list[type[Exception]] = field(default_factory=(lambda: [Exception]))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Exceptions that trigger circuit breaker</p>"},{"location":"api/exceptions/#flowyml.core.error_handling.CircuitBreakerConfig.failure_threshold","title":"<code>failure_threshold: int = 5</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of failures before opening circuit</p>"},{"location":"api/exceptions/#flowyml.core.error_handling.CircuitBreakerConfig.recovery_timeout","title":"<code>recovery_timeout: float = 300</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Time to wait before fully closing circuit (seconds)</p>"},{"location":"api/exceptions/#flowyml.core.error_handling.CircuitBreakerConfig.timeout","title":"<code>timeout: float = 60</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Time to wait before trying again (seconds)</p>"},{"location":"api/exceptions/#flowyml.core.error_handling.CircuitOpenError","title":"<code>CircuitOpenError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when circuit breaker is open.</p>"},{"location":"api/exceptions/#flowyml.core.error_handling.CircuitState","title":"<code>CircuitState</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Circuit breaker states.</p>"},{"location":"api/exceptions/#flowyml.core.error_handling.ExponentialBackoff","title":"<code>ExponentialBackoff(initial: float = 1.0, max_delay: float = 60.0, multiplier: float = 2.0, jitter: bool = True)</code>","text":"<p>Exponential backoff retry strategy.</p> Example <pre><code>from flowyml import step, retry, ExponentialBackoff\n\n\n@step(\n    retry=retry(\n        max_attempts=5, backoff=ExponentialBackoff(initial=1, max=60, multiplier=2), on=[NetworkError, TimeoutError]\n    )\n)\ndef fetch_data():\n    return api.get_data()\n</code></pre> <p>Initialize exponential backoff.</p> <p>Parameters:</p> Name Type Description Default <code>initial</code> <code>float</code> <p>Initial delay in seconds</p> <code>1.0</code> <code>max_delay</code> <code>float</code> <p>Maximum delay in seconds</p> <code>60.0</code> <code>multiplier</code> <code>float</code> <p>Backoff multiplier</p> <code>2.0</code> <code>jitter</code> <code>bool</code> <p>Add random jitter to delays</p> <code>True</code> Source code in <code>flowyml/core/error_handling.py</code> <pre><code>def __init__(\n    self,\n    initial: float = 1.0,\n    max_delay: float = 60.0,\n    multiplier: float = 2.0,\n    jitter: bool = True,\n):\n    \"\"\"Initialize exponential backoff.\n\n    Args:\n        initial: Initial delay in seconds\n        max_delay: Maximum delay in seconds\n        multiplier: Backoff multiplier\n        jitter: Add random jitter to delays\n    \"\"\"\n    self.initial = initial\n    self.max_delay = max_delay\n    self.multiplier = multiplier\n    self.jitter = jitter\n    self.attempt = 0\n</code></pre>"},{"location":"api/exceptions/#flowyml.core.error_handling.ExponentialBackoff-functions","title":"Functions","text":""},{"location":"api/exceptions/#flowyml.core.error_handling.ExponentialBackoff.get_delay","title":"<code>get_delay() -&gt; float</code>","text":"<p>Get delay for current attempt.</p> <p>Returns:</p> Type Description <code>float</code> <p>Delay in seconds</p> Source code in <code>flowyml/core/error_handling.py</code> <pre><code>def get_delay(self) -&gt; float:\n    \"\"\"Get delay for current attempt.\n\n    Returns:\n        Delay in seconds\n    \"\"\"\n    delay = min(self.initial * (self.multiplier**self.attempt), self.max_delay)\n\n    if self.jitter:\n        import random\n\n        delay = delay * (0.5 + random.random())\n\n    self.attempt += 1\n    return delay\n</code></pre>"},{"location":"api/exceptions/#flowyml.core.error_handling.ExponentialBackoff.reset","title":"<code>reset() -&gt; None</code>","text":"<p>Reset attempt counter.</p> Source code in <code>flowyml/core/error_handling.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset attempt counter.\"\"\"\n    self.attempt = 0\n</code></pre>"},{"location":"api/exceptions/#flowyml.core.error_handling.FallbackConfig","title":"<code>FallbackConfig(fallback_func: Callable, fallback_on: list[type[Exception]] = (lambda: [Exception])(), max_fallback_attempts: int = 1)</code>  <code>dataclass</code>","text":"<p>Configuration for fallback handler.</p>"},{"location":"api/exceptions/#flowyml.core.error_handling.FallbackConfig-attributes","title":"Attributes","text":""},{"location":"api/exceptions/#flowyml.core.error_handling.FallbackConfig.fallback_func","title":"<code>fallback_func: Callable</code>  <code>instance-attribute</code>","text":"<p>Fallback function to call on error</p>"},{"location":"api/exceptions/#flowyml.core.error_handling.FallbackConfig.fallback_on","title":"<code>fallback_on: list[type[Exception]] = field(default_factory=(lambda: [Exception]))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Exceptions that trigger fallback</p>"},{"location":"api/exceptions/#flowyml.core.error_handling.FallbackConfig.max_fallback_attempts","title":"<code>max_fallback_attempts: int = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum number of fallback attempts</p>"},{"location":"api/exceptions/#flowyml.core.error_handling.FallbackHandler","title":"<code>FallbackHandler(fallback_func: Callable, fallback_on: list[type[Exception]] | None = None, max_attempts: int = 1)</code>","text":"<p>Fallback handler for graceful degradation.</p> Example <pre><code>@step(fallback=lambda: load_cached_data(), fallback_on=[TimeoutError])\ndef fetch_live_data():\n    return requests.get(url).json()\n</code></pre> <p>Initialize fallback handler.</p> <p>Parameters:</p> Name Type Description Default <code>fallback_func</code> <code>Callable</code> <p>Function to call as fallback</p> required <code>fallback_on</code> <code>list[type[Exception]] | None</code> <p>Exceptions that trigger fallback</p> <code>None</code> <code>max_attempts</code> <code>int</code> <p>Maximum fallback attempts</p> <code>1</code> Source code in <code>flowyml/core/error_handling.py</code> <pre><code>def __init__(\n    self,\n    fallback_func: Callable,\n    fallback_on: list[type[Exception]] | None = None,\n    max_attempts: int = 1,\n):\n    \"\"\"Initialize fallback handler.\n\n    Args:\n        fallback_func: Function to call as fallback\n        fallback_on: Exceptions that trigger fallback\n        max_attempts: Maximum fallback attempts\n    \"\"\"\n    self.config = FallbackConfig(\n        fallback_func=fallback_func,\n        fallback_on=fallback_on or [Exception],\n        max_fallback_attempts=max_attempts,\n    )\n    self.fallback_attempts = 0\n</code></pre>"},{"location":"api/exceptions/#flowyml.core.error_handling.FallbackHandler-functions","title":"Functions","text":""},{"location":"api/exceptions/#flowyml.core.error_handling.FallbackHandler.call","title":"<code>call(func: Callable, *args, **kwargs) -&gt; Any</code>","text":"<p>Call function with fallback protection.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>Primary function to call</p> required <code>*args</code> <p>Positional arguments</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Function result or fallback result</p> Source code in <code>flowyml/core/error_handling.py</code> <pre><code>def call(self, func: Callable, *args, **kwargs) -&gt; Any:\n    \"\"\"Call function with fallback protection.\n\n    Args:\n        func: Primary function to call\n        *args: Positional arguments\n        **kwargs: Keyword arguments\n\n    Returns:\n        Function result or fallback result\n    \"\"\"\n    try:\n        return func(*args, **kwargs)\n\n    except Exception as e:\n        if self._should_fallback(e) and self.fallback_attempts &lt; self.config.max_fallback_attempts:\n            self.fallback_attempts += 1\n            return self.config.fallback_func()\n        raise\n</code></pre>"},{"location":"api/exceptions/#flowyml.core.error_handling.FallbackHandler.reset","title":"<code>reset() -&gt; None</code>","text":"<p>Reset fallback attempts counter.</p> Source code in <code>flowyml/core/error_handling.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset fallback attempts counter.\"\"\"\n    self.fallback_attempts = 0\n</code></pre>"},{"location":"api/exceptions/#flowyml.core.error_handling.OnFailureConfig","title":"<code>OnFailureConfig(action: str = 'log', recipients: list[str] = list(), include_logs: bool = True, include_traceback: bool = True)</code>  <code>dataclass</code>","text":"<p>Configuration for failure handling.</p>"},{"location":"api/exceptions/#flowyml.core.error_handling.OnFailureConfig-attributes","title":"Attributes","text":""},{"location":"api/exceptions/#flowyml.core.error_handling.OnFailureConfig.action","title":"<code>action: str = 'log'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Action to take (log, email, slack, webhook)</p>"},{"location":"api/exceptions/#flowyml.core.error_handling.OnFailureConfig.include_logs","title":"<code>include_logs: bool = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Include logs in notification</p>"},{"location":"api/exceptions/#flowyml.core.error_handling.OnFailureConfig.include_traceback","title":"<code>include_traceback: bool = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Include full traceback</p>"},{"location":"api/exceptions/#flowyml.core.error_handling.OnFailureConfig.recipients","title":"<code>recipients: list[str] = field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Recipients for notifications</p>"},{"location":"api/exceptions/#flowyml.core.error_handling.RetryConfig","title":"<code>RetryConfig(max_attempts: int = 3, backoff: ExponentialBackoff | None = None, retry_on: list[type[Exception]] = (lambda: [Exception])(), not_retry_on: list[type[Exception]] = list())</code>  <code>dataclass</code>","text":"<p>Configuration for retry logic.</p>"},{"location":"api/exceptions/#flowyml.core.error_handling.RetryConfig-attributes","title":"Attributes","text":""},{"location":"api/exceptions/#flowyml.core.error_handling.RetryConfig.backoff","title":"<code>backoff: ExponentialBackoff | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Backoff strategy</p>"},{"location":"api/exceptions/#flowyml.core.error_handling.RetryConfig.max_attempts","title":"<code>max_attempts: int = 3</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum number of retry attempts</p>"},{"location":"api/exceptions/#flowyml.core.error_handling.RetryConfig.not_retry_on","title":"<code>not_retry_on: list[type[Exception]] = field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Exceptions NOT to retry on</p>"},{"location":"api/exceptions/#flowyml.core.error_handling.RetryConfig.retry_on","title":"<code>retry_on: list[type[Exception]] = field(default_factory=(lambda: [Exception]))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Exceptions to retry on</p>"},{"location":"api/exceptions/#flowyml.core.error_handling-functions","title":"Functions","text":""},{"location":"api/exceptions/#flowyml.core.error_handling.execute_with_retry","title":"<code>execute_with_retry(func: Callable, retry_config: RetryConfig, *args, **kwargs) -&gt; Any</code>","text":"<p>Execute function with retry logic.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>Function to execute</p> required <code>retry_config</code> <code>RetryConfig</code> <p>Retry configuration</p> required <code>*args</code> <p>Positional arguments</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Function result</p> Source code in <code>flowyml/core/error_handling.py</code> <pre><code>def execute_with_retry(\n    func: Callable,\n    retry_config: RetryConfig,\n    *args,\n    **kwargs,\n) -&gt; Any:\n    \"\"\"Execute function with retry logic.\n\n    Args:\n        func: Function to execute\n        retry_config: Retry configuration\n        *args: Positional arguments\n        **kwargs: Keyword arguments\n\n    Returns:\n        Function result\n\n    Raises:\n        Last exception if all retries fail\n    \"\"\"\n    last_exception = None\n\n    for attempt in range(retry_config.max_attempts):\n        try:\n            return func(*args, **kwargs)\n\n        except Exception as e:\n            # Don't retry if in not_retry_on list\n            if any(isinstance(e, exc_type) for exc_type in retry_config.not_retry_on):\n                raise\n\n            # Only retry if in retry_on list\n            if not any(isinstance(e, exc_type) for exc_type in retry_config.retry_on):\n                raise\n\n            last_exception = e\n\n            # Don't sleep on last attempt\n            if attempt &lt; retry_config.max_attempts - 1 and retry_config.backoff:\n                delay = retry_config.backoff.get_delay()\n                time.sleep(delay)\n\n    # All retries failed\n    if last_exception:\n        raise last_exception\n    raise RuntimeError(\"Retry failed with no exception captured\")\n</code></pre>"},{"location":"api/exceptions/#flowyml.core.error_handling.on_failure","title":"<code>on_failure(action: str = 'log', recipients: list[str] | None = None, include_logs: bool = True, include_traceback: bool = True) -&gt; OnFailureConfig</code>","text":"<p>Create failure handling configuration.</p> <p>Parameters:</p> Name Type Description Default <code>action</code> <code>str</code> <p>Action to take on failure</p> <code>'log'</code> <code>recipients</code> <code>list[str] | None</code> <p>Recipients for notifications</p> <code>None</code> <code>include_logs</code> <code>bool</code> <p>Include logs in notification</p> <code>True</code> <code>include_traceback</code> <code>bool</code> <p>Include full traceback</p> <code>True</code> <p>Returns:</p> Type Description <code>OnFailureConfig</code> <p>OnFailureConfig instance</p> Source code in <code>flowyml/core/error_handling.py</code> <pre><code>def on_failure(\n    action: str = \"log\",\n    recipients: list[str] | None = None,\n    include_logs: bool = True,\n    include_traceback: bool = True,\n) -&gt; OnFailureConfig:\n    \"\"\"Create failure handling configuration.\n\n    Args:\n        action: Action to take on failure\n        recipients: Recipients for notifications\n        include_logs: Include logs in notification\n        include_traceback: Include full traceback\n\n    Returns:\n        OnFailureConfig instance\n    \"\"\"\n    return OnFailureConfig(\n        action=action,\n        recipients=recipients or [],\n        include_logs=include_logs,\n        include_traceback=include_traceback,\n    )\n</code></pre>"},{"location":"api/exceptions/#flowyml.core.error_handling.retry","title":"<code>retry(max_attempts: int = 3, backoff: ExponentialBackoff | None = None, on: list[type[Exception]] | None = None, not_on: list[type[Exception]] | None = None) -&gt; RetryConfig</code>","text":"<p>Create retry configuration.</p> <p>Parameters:</p> Name Type Description Default <code>max_attempts</code> <code>int</code> <p>Maximum retry attempts</p> <code>3</code> <code>backoff</code> <code>ExponentialBackoff | None</code> <p>Backoff strategy</p> <code>None</code> <code>on</code> <code>list[type[Exception]] | None</code> <p>Exceptions to retry on</p> <code>None</code> <code>not_on</code> <code>list[type[Exception]] | None</code> <p>Exceptions not to retry on</p> <code>None</code> <p>Returns:</p> Type Description <code>RetryConfig</code> <p>RetryConfig instance</p> Source code in <code>flowyml/core/error_handling.py</code> <pre><code>def retry(\n    max_attempts: int = 3,\n    backoff: ExponentialBackoff | None = None,\n    on: list[type[Exception]] | None = None,\n    not_on: list[type[Exception]] | None = None,\n) -&gt; RetryConfig:\n    \"\"\"Create retry configuration.\n\n    Args:\n        max_attempts: Maximum retry attempts\n        backoff: Backoff strategy\n        on: Exceptions to retry on\n        not_on: Exceptions not to retry on\n\n    Returns:\n        RetryConfig instance\n    \"\"\"\n    return RetryConfig(\n        max_attempts=max_attempts,\n        backoff=backoff or ExponentialBackoff(),\n        retry_on=on or [Exception],\n        not_retry_on=not_on or [],\n    )\n</code></pre>"},{"location":"api/metadata_stores/","title":"Metadata Stores API \ud83d\uddc4\ufe0f","text":"<p>Metadata Stores track pipeline runs, step status, and artifact lineage.</p>"},{"location":"api/metadata_stores/#base-metadata-store","title":"Base Metadata Store","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for metadata storage backends.</p>"},{"location":"api/metadata_stores/#flowyml.storage.metadata.MetadataStore-functions","title":"Functions","text":""},{"location":"api/metadata_stores/#flowyml.storage.metadata.MetadataStore.list_assets","title":"<code>list_assets(limit: int | None = None, **filters) -&gt; list[dict]</code>  <code>abstractmethod</code>","text":"<p>List assets with optional filters.</p> Source code in <code>flowyml/storage/metadata.py</code> <pre><code>@abstractmethod\ndef list_assets(self, limit: int | None = None, **filters) -&gt; list[dict]:\n    \"\"\"List assets with optional filters.\"\"\"\n    pass\n</code></pre>"},{"location":"api/metadata_stores/#flowyml.storage.metadata.MetadataStore.list_pipelines","title":"<code>list_pipelines() -&gt; list[str]</code>  <code>abstractmethod</code>","text":"<p>List all unique pipeline names.</p> Source code in <code>flowyml/storage/metadata.py</code> <pre><code>@abstractmethod\ndef list_pipelines(self) -&gt; list[str]:\n    \"\"\"List all unique pipeline names.\"\"\"\n    pass\n</code></pre>"},{"location":"api/metadata_stores/#flowyml.storage.metadata.MetadataStore.list_runs","title":"<code>list_runs(limit: int | None = None) -&gt; list[dict]</code>  <code>abstractmethod</code>","text":"<p>List all runs.</p> Source code in <code>flowyml/storage/metadata.py</code> <pre><code>@abstractmethod\ndef list_runs(self, limit: int | None = None) -&gt; list[dict]:\n    \"\"\"List all runs.\"\"\"\n    pass\n</code></pre>"},{"location":"api/metadata_stores/#flowyml.storage.metadata.MetadataStore.load_artifact","title":"<code>load_artifact(artifact_id: str) -&gt; dict | None</code>  <code>abstractmethod</code>","text":"<p>Load artifact metadata.</p> Source code in <code>flowyml/storage/metadata.py</code> <pre><code>@abstractmethod\ndef load_artifact(self, artifact_id: str) -&gt; dict | None:\n    \"\"\"Load artifact metadata.\"\"\"\n    pass\n</code></pre>"},{"location":"api/metadata_stores/#flowyml.storage.metadata.MetadataStore.load_run","title":"<code>load_run(run_id: str) -&gt; dict | None</code>  <code>abstractmethod</code>","text":"<p>Load run metadata.</p> Source code in <code>flowyml/storage/metadata.py</code> <pre><code>@abstractmethod\ndef load_run(self, run_id: str) -&gt; dict | None:\n    \"\"\"Load run metadata.\"\"\"\n    pass\n</code></pre>"},{"location":"api/metadata_stores/#flowyml.storage.metadata.MetadataStore.query","title":"<code>query(**filters) -&gt; list[dict]</code>  <code>abstractmethod</code>","text":"<p>Query runs with filters.</p> Source code in <code>flowyml/storage/metadata.py</code> <pre><code>@abstractmethod\ndef query(self, **filters) -&gt; list[dict]:\n    \"\"\"Query runs with filters.\"\"\"\n    pass\n</code></pre>"},{"location":"api/metadata_stores/#flowyml.storage.metadata.MetadataStore.save_artifact","title":"<code>save_artifact(artifact_id: str, metadata: dict) -&gt; None</code>  <code>abstractmethod</code>","text":"<p>Save artifact metadata.</p> Source code in <code>flowyml/storage/metadata.py</code> <pre><code>@abstractmethod\ndef save_artifact(self, artifact_id: str, metadata: dict) -&gt; None:\n    \"\"\"Save artifact metadata.\"\"\"\n    pass\n</code></pre>"},{"location":"api/metadata_stores/#flowyml.storage.metadata.MetadataStore.save_run","title":"<code>save_run(run_id: str, metadata: dict) -&gt; None</code>  <code>abstractmethod</code>","text":"<p>Save run metadata.</p> Source code in <code>flowyml/storage/metadata.py</code> <pre><code>@abstractmethod\ndef save_run(self, run_id: str, metadata: dict) -&gt; None:\n    \"\"\"Save run metadata.\"\"\"\n    pass\n</code></pre>"},{"location":"api/metadata_stores/#sqlite-metadata-store","title":"SQLite Metadata Store","text":"<p>               Bases: <code>MetadataStore</code></p> <p>SQLite-based metadata storage.</p> <p>Initialize SQLite metadata store.</p> <p>Parameters:</p> Name Type Description Default <code>db_path</code> <code>str</code> <p>Path to SQLite database file</p> <code>'.flowyml/metadata.db'</code> Source code in <code>flowyml/storage/metadata.py</code> <pre><code>def __init__(self, db_path: str = \".flowyml/metadata.db\"):\n    \"\"\"Initialize SQLite metadata store.\n\n    Args:\n        db_path: Path to SQLite database file\n    \"\"\"\n    self.db_path = Path(db_path)\n    self.db_path.parent.mkdir(parents=True, exist_ok=True)\n    self._init_db()\n</code></pre>"},{"location":"api/metadata_stores/#flowyml.storage.metadata.SQLiteMetadataStore-functions","title":"Functions","text":""},{"location":"api/metadata_stores/#flowyml.storage.metadata.SQLiteMetadataStore.get_experiment","title":"<code>get_experiment(experiment_id: str) -&gt; dict | None</code>","text":"<p>Get experiment details.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_id</code> <code>str</code> <p>Experiment identifier</p> required <p>Returns:</p> Type Description <code>dict | None</code> <p>Experiment dictionary or None</p> Source code in <code>flowyml/storage/metadata.py</code> <pre><code>def get_experiment(self, experiment_id: str) -&gt; dict | None:\n    \"\"\"Get experiment details.\n\n    Args:\n        experiment_id: Experiment identifier\n\n    Returns:\n        Experiment dictionary or None\n    \"\"\"\n    conn = sqlite3.connect(self.db_path)\n    cursor = conn.cursor()\n\n    cursor.execute(\n        \"SELECT experiment_id, name, description, tags, created_at FROM experiments WHERE experiment_id = ?\",\n        (experiment_id,),\n    )\n    row = cursor.fetchone()\n\n    if not row:\n        conn.close()\n        return None\n\n    experiment = {\n        \"experiment_id\": row[0],\n        \"name\": row[1],\n        \"description\": row[2],\n        \"tags\": json.loads(row[3]),\n        \"created_at\": row[4],\n    }\n\n    # Get runs\n    cursor.execute(\n        \"\"\"\n        SELECT er.run_id, er.metrics, er.parameters, er.timestamp, r.status, r.duration\n        FROM experiment_runs er\n        LEFT JOIN runs r ON er.run_id = r.run_id\n        WHERE er.experiment_id = ?\n        ORDER BY er.timestamp DESC\n    \"\"\",\n        (experiment_id,),\n    )\n\n    runs = []\n    for r in cursor.fetchall():\n        runs.append(\n            {\n                \"run_id\": r[0],\n                \"metrics\": json.loads(r[1]),\n                \"parameters\": json.loads(r[2]),\n                \"timestamp\": r[3],\n                \"status\": r[4],\n                \"duration\": r[5],\n            },\n        )\n\n    experiment[\"runs\"] = runs\n\n    conn.close()\n    return experiment\n</code></pre>"},{"location":"api/metadata_stores/#flowyml.storage.metadata.SQLiteMetadataStore.get_metrics","title":"<code>get_metrics(run_id: str, name: str | None = None) -&gt; list[dict]</code>","text":"<p>Get metrics for a run.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier</p> required <code>name</code> <code>str | None</code> <p>Optional metric name filter</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>List of metric dictionaries</p> Source code in <code>flowyml/storage/metadata.py</code> <pre><code>def get_metrics(self, run_id: str, name: str | None = None) -&gt; list[dict]:\n    \"\"\"Get metrics for a run.\n\n    Args:\n        run_id: Run identifier\n        name: Optional metric name filter\n\n    Returns:\n        List of metric dictionaries\n    \"\"\"\n    conn = sqlite3.connect(self.db_path)\n    cursor = conn.cursor()\n\n    if name:\n        cursor.execute(\n            \"SELECT name, value, step, timestamp FROM metrics WHERE run_id = ? AND name = ? ORDER BY step\",\n            (run_id, name),\n        )\n    else:\n        cursor.execute(\n            \"SELECT name, value, step, timestamp FROM metrics WHERE run_id = ? ORDER BY step\",\n            (run_id,),\n        )\n\n    rows = cursor.fetchall()\n    conn.close()\n\n    return [{\"name\": row[0], \"value\": row[1], \"step\": row[2], \"timestamp\": row[3]} for row in rows]\n</code></pre>"},{"location":"api/metadata_stores/#flowyml.storage.metadata.SQLiteMetadataStore.get_pipeline_definition","title":"<code>get_pipeline_definition(pipeline_name: str) -&gt; dict | None</code>","text":"<p>Retrieve pipeline definition.</p> Source code in <code>flowyml/storage/metadata.py</code> <pre><code>def get_pipeline_definition(self, pipeline_name: str) -&gt; dict | None:\n    \"\"\"Retrieve pipeline definition.\"\"\"\n    conn = sqlite3.connect(self.db_path)\n    cursor = conn.cursor()\n    cursor.execute(\n        \"SELECT definition FROM pipeline_definitions WHERE pipeline_name = ?\",\n        (pipeline_name,),\n    )\n    row = cursor.fetchone()\n    conn.close()\n\n    if row:\n        return json.loads(row[0])\n    return None\n</code></pre>"},{"location":"api/metadata_stores/#flowyml.storage.metadata.SQLiteMetadataStore.get_statistics","title":"<code>get_statistics() -&gt; dict</code>","text":"<p>Get database statistics.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with statistics</p> Source code in <code>flowyml/storage/metadata.py</code> <pre><code>def get_statistics(self) -&gt; dict:\n    \"\"\"Get database statistics.\n\n    Returns:\n        Dictionary with statistics\n    \"\"\"\n    conn = sqlite3.connect(self.db_path)\n    cursor = conn.cursor()\n\n    stats = {}\n\n    cursor.execute(\"SELECT COUNT(*) FROM runs\")\n    stats[\"total_runs\"] = cursor.fetchone()[0]\n\n    cursor.execute(\"SELECT COUNT(*) FROM artifacts\")\n    stats[\"total_artifacts\"] = cursor.fetchone()[0]\n\n    cursor.execute(\"SELECT COUNT(*) FROM metrics\")\n    stats[\"total_metrics\"] = cursor.fetchone()[0]\n\n    cursor.execute(\"SELECT COUNT(DISTINCT pipeline_name) FROM runs\")\n    stats[\"total_pipelines\"] = cursor.fetchone()[0]\n\n    cursor.execute(\"SELECT COUNT(*) FROM experiments\")\n    stats[\"total_experiments\"] = cursor.fetchone()[0]\n\n    conn.close()\n\n    return stats\n</code></pre>"},{"location":"api/metadata_stores/#flowyml.storage.metadata.SQLiteMetadataStore.get_trace","title":"<code>get_trace(trace_id: str) -&gt; list[dict]</code>","text":"<p>Get all events for a trace.</p> <p>Parameters:</p> Name Type Description Default <code>trace_id</code> <code>str</code> <p>Trace identifier</p> required <p>Returns:</p> Type Description <code>list[dict]</code> <p>List of event dictionaries</p> Source code in <code>flowyml/storage/metadata.py</code> <pre><code>def get_trace(self, trace_id: str) -&gt; list[dict]:\n    \"\"\"Get all events for a trace.\n\n    Args:\n        trace_id: Trace identifier\n\n    Returns:\n        List of event dictionaries\n    \"\"\"\n    conn = sqlite3.connect(self.db_path)\n    cursor = conn.cursor()\n\n    cursor.execute(\n        \"\"\"\n        SELECT * FROM traces WHERE trace_id = ? ORDER BY start_time\n    \"\"\",\n        (trace_id,),\n    )\n\n    columns = [description[0] for description in cursor.description]\n    rows = cursor.fetchall()\n\n    events = []\n    for row in rows:\n        event = dict(zip(columns, row, strict=False))\n        # Parse JSON fields\n        for field in [\"inputs\", \"outputs\", \"metadata\"]:\n            if event[field]:\n                with contextlib.suppress(builtins.BaseException):\n                    event[field] = json.loads(event[field])\n        events.append(event)\n\n    conn.close()\n    return events\n</code></pre>"},{"location":"api/metadata_stores/#flowyml.storage.metadata.SQLiteMetadataStore.list_assets","title":"<code>list_assets(limit: int | None = None, **filters) -&gt; list[dict]</code>","text":"<p>List assets from database with optional filters.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int | None</code> <p>Optional limit on number of results</p> <code>None</code> <code>**filters</code> <p>Filter criteria (type, run_id, etc.)</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>List of artifact metadata dictionaries</p> Source code in <code>flowyml/storage/metadata.py</code> <pre><code>def list_assets(self, limit: int | None = None, **filters) -&gt; list[dict]:\n    \"\"\"List assets from database with optional filters.\n\n    Args:\n        limit: Optional limit on number of results\n        **filters: Filter criteria (type, run_id, etc.)\n\n    Returns:\n        List of artifact metadata dictionaries\n    \"\"\"\n    conn = sqlite3.connect(self.db_path)\n    cursor = conn.cursor()\n\n    conditions = []\n    params = []\n\n    for key, value in filters.items():\n        if value is not None:\n            conditions.append(f\"{key} = ?\")\n            params.append(value)\n\n    query = \"SELECT metadata FROM artifacts\"\n    if conditions:\n        query += \" WHERE \" + \" AND \".join(conditions)\n\n    query += \" ORDER BY created_at DESC\"\n\n    if limit:\n        query += f\" LIMIT {limit}\"\n\n    cursor.execute(query, params)\n    rows = cursor.fetchall()\n\n    conn.close()\n\n    return [json.loads(row[0]) for row in rows]\n</code></pre>"},{"location":"api/metadata_stores/#flowyml.storage.metadata.SQLiteMetadataStore.list_experiments","title":"<code>list_experiments() -&gt; list[dict]</code>","text":"<p>List all experiments.</p> <p>Returns:</p> Type Description <code>list[dict]</code> <p>List of experiment dictionaries</p> Source code in <code>flowyml/storage/metadata.py</code> <pre><code>def list_experiments(self) -&gt; list[dict]:\n    \"\"\"List all experiments.\n\n    Returns:\n        List of experiment dictionaries\n    \"\"\"\n    conn = sqlite3.connect(self.db_path)\n    cursor = conn.cursor()\n\n    cursor.execute(\n        \"SELECT experiment_id, name, description, tags, created_at FROM experiments ORDER BY created_at DESC\",\n    )\n    rows = cursor.fetchall()\n\n    experiments = []\n    for row in rows:\n        # Count runs for each experiment\n        cursor.execute(\"SELECT COUNT(*) FROM experiment_runs WHERE experiment_id = ?\", (row[0],))\n        run_count = cursor.fetchone()[0]\n\n        experiments.append(\n            {\n                \"experiment_id\": row[0],\n                \"name\": row[1],\n                \"description\": row[2],\n                \"tags\": json.loads(row[3]),\n                \"created_at\": row[4],\n                \"run_count\": run_count,\n            },\n        )\n    conn.close()\n    return experiments\n</code></pre>"},{"location":"api/metadata_stores/#flowyml.storage.metadata.SQLiteMetadataStore.list_pipelines","title":"<code>list_pipelines(project: str = None) -&gt; list[str]</code>","text":"<p>List all unique pipeline names.</p> <p>Parameters:</p> Name Type Description Default <code>project</code> <code>str</code> <p>Optional project name to filter by</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of pipeline names</p> Source code in <code>flowyml/storage/metadata.py</code> <pre><code>def list_pipelines(self, project: str = None) -&gt; list[str]:\n    \"\"\"List all unique pipeline names.\n\n    Args:\n        project: Optional project name to filter by\n\n    Returns:\n        List of pipeline names\n    \"\"\"\n    conn = sqlite3.connect(self.db_path)\n    cursor = conn.cursor()\n\n    if project:\n        cursor.execute(\n            \"SELECT DISTINCT pipeline_name FROM runs WHERE project = ? ORDER BY pipeline_name\",\n            (project,),\n        )\n    else:\n        cursor.execute(\"SELECT DISTINCT pipeline_name FROM runs ORDER BY pipeline_name\")\n\n    rows = cursor.fetchall()\n\n    conn.close()\n\n    return [row[0] for row in rows if row[0]]\n</code></pre>"},{"location":"api/metadata_stores/#flowyml.storage.metadata.SQLiteMetadataStore.list_runs","title":"<code>list_runs(limit: int | None = None) -&gt; list[dict]</code>","text":"<p>List all runs from database.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int | None</code> <p>Optional limit on number of results</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>List of run metadata dictionaries</p> Source code in <code>flowyml/storage/metadata.py</code> <pre><code>def list_runs(self, limit: int | None = None) -&gt; list[dict]:\n    \"\"\"List all runs from database.\n\n    Args:\n        limit: Optional limit on number of results\n\n    Returns:\n        List of run metadata dictionaries\n    \"\"\"\n    conn = sqlite3.connect(self.db_path)\n    cursor = conn.cursor()\n\n    query = \"SELECT metadata FROM runs ORDER BY created_at DESC\"\n    if limit:\n        query += f\" LIMIT {limit}\"\n\n    cursor.execute(query)\n    rows = cursor.fetchall()\n\n    conn.close()\n\n    return [json.loads(row[0]) for row in rows]\n</code></pre>"},{"location":"api/metadata_stores/#flowyml.storage.metadata.SQLiteMetadataStore.load_artifact","title":"<code>load_artifact(artifact_id: str) -&gt; dict | None</code>","text":"<p>Load artifact metadata from database.</p> <p>Parameters:</p> Name Type Description Default <code>artifact_id</code> <code>str</code> <p>Unique artifact identifier</p> required <p>Returns:</p> Type Description <code>dict | None</code> <p>Artifact metadata dictionary or None if not found</p> Source code in <code>flowyml/storage/metadata.py</code> <pre><code>def load_artifact(self, artifact_id: str) -&gt; dict | None:\n    \"\"\"Load artifact metadata from database.\n\n    Args:\n        artifact_id: Unique artifact identifier\n\n    Returns:\n        Artifact metadata dictionary or None if not found\n    \"\"\"\n    conn = sqlite3.connect(self.db_path)\n    cursor = conn.cursor()\n\n    cursor.execute(\"SELECT metadata FROM artifacts WHERE artifact_id = ?\", (artifact_id,))\n    row = cursor.fetchone()\n\n    conn.close()\n\n    if row:\n        return json.loads(row[0])\n    return None\n</code></pre>"},{"location":"api/metadata_stores/#flowyml.storage.metadata.SQLiteMetadataStore.load_run","title":"<code>load_run(run_id: str) -&gt; dict | None</code>","text":"<p>Load run metadata from database.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Unique run identifier</p> required <p>Returns:</p> Type Description <code>dict | None</code> <p>Run metadata dictionary or None if not found</p> Source code in <code>flowyml/storage/metadata.py</code> <pre><code>def load_run(self, run_id: str) -&gt; dict | None:\n    \"\"\"Load run metadata from database.\n\n    Args:\n        run_id: Unique run identifier\n\n    Returns:\n        Run metadata dictionary or None if not found\n    \"\"\"\n    conn = sqlite3.connect(self.db_path)\n    cursor = conn.cursor()\n\n    cursor.execute(\"SELECT metadata FROM runs WHERE run_id = ?\", (run_id,))\n    row = cursor.fetchone()\n\n    conn.close()\n\n    if row:\n        data = json.loads(row[0])\n        # Ensure project is in metadata if it's in the column but not the JSON blob\n        # (This might happen if we update the column directly)\n        # Actually, let's just return what's in the blob for now,\n        # but we should probably sync them.\n        return data\n    return None\n</code></pre>"},{"location":"api/metadata_stores/#flowyml.storage.metadata.SQLiteMetadataStore.log_experiment_run","title":"<code>log_experiment_run(experiment_id: str, run_id: str, metrics: dict = None, parameters: dict = None) -&gt; None</code>","text":"<p>Log a run to an experiment.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_id</code> <code>str</code> <p>Experiment identifier</p> required <code>run_id</code> <code>str</code> <p>Run identifier</p> required <code>metrics</code> <code>dict</code> <p>Metrics from the run</p> <code>None</code> <code>parameters</code> <code>dict</code> <p>Parameters used in the run</p> <code>None</code> Source code in <code>flowyml/storage/metadata.py</code> <pre><code>def log_experiment_run(\n    self,\n    experiment_id: str,\n    run_id: str,\n    metrics: dict = None,\n    parameters: dict = None,\n) -&gt; None:\n    \"\"\"Log a run to an experiment.\n\n    Args:\n        experiment_id: Experiment identifier\n        run_id: Run identifier\n        metrics: Metrics from the run\n        parameters: Parameters used in the run\n    \"\"\"\n    conn = sqlite3.connect(self.db_path)\n    cursor = conn.cursor()\n\n    cursor.execute(\n        \"\"\"\n        INSERT OR REPLACE INTO experiment_runs\n        (experiment_id, run_id, metrics, parameters)\n        VALUES (?, ?, ?, ?)\n    \"\"\",\n        (\n            experiment_id,\n            run_id,\n            json.dumps(metrics or {}),\n            json.dumps(parameters or {}),\n        ),\n    )\n\n    conn.commit()\n    conn.close()\n</code></pre>"},{"location":"api/metadata_stores/#flowyml.storage.metadata.SQLiteMetadataStore.query","title":"<code>query(**filters) -&gt; list[dict]</code>","text":"<p>Query runs with filters.</p> <p>Parameters:</p> Name Type Description Default <code>**filters</code> <p>Filter criteria (pipeline_name, status, etc.)</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>List of matching run metadata dictionaries</p> Source code in <code>flowyml/storage/metadata.py</code> <pre><code>def query(self, **filters) -&gt; list[dict]:\n    \"\"\"Query runs with filters.\n\n    Args:\n        **filters: Filter criteria (pipeline_name, status, etc.)\n\n    Returns:\n        List of matching run metadata dictionaries\n    \"\"\"\n    conn = sqlite3.connect(self.db_path)\n    cursor = conn.cursor()\n\n    where_clauses = []\n    params = []\n\n    for key, value in filters.items():\n        where_clauses.append(f\"{key} = ?\")\n        params.append(value)\n\n    query = \"SELECT metadata FROM runs\"\n    if where_clauses:\n        query += \" WHERE \" + \" AND \".join(where_clauses)\n    query += \" ORDER BY created_at DESC\"\n\n    cursor.execute(query, params)\n    rows = cursor.fetchall()\n\n    conn.close()\n\n    return [json.loads(row[0]) for row in rows]\n</code></pre>"},{"location":"api/metadata_stores/#flowyml.storage.metadata.SQLiteMetadataStore.save_artifact","title":"<code>save_artifact(artifact_id: str, metadata: dict) -&gt; None</code>","text":"<p>Save artifact metadata to database.</p> <p>Parameters:</p> Name Type Description Default <code>artifact_id</code> <code>str</code> <p>Unique artifact identifier</p> required <code>metadata</code> <code>dict</code> <p>Artifact metadata dictionary</p> required Source code in <code>flowyml/storage/metadata.py</code> <pre><code>def save_artifact(self, artifact_id: str, metadata: dict) -&gt; None:\n    \"\"\"Save artifact metadata to database.\n\n    Args:\n        artifact_id: Unique artifact identifier\n        metadata: Artifact metadata dictionary\n    \"\"\"\n    conn = sqlite3.connect(self.db_path)\n    cursor = conn.cursor()\n\n    cursor.execute(\n        \"\"\"\n        INSERT OR REPLACE INTO artifacts\n        (artifact_id, name, type, run_id, path, metadata, project)\n        VALUES (?, ?, ?, ?, ?, ?, ?)\n    \"\"\",\n        (\n            artifact_id,\n            metadata.get(\"name\"),\n            metadata.get(\"type\"),\n            metadata.get(\"run_id\"),\n            metadata.get(\"path\"),\n            json.dumps(metadata),\n            metadata.get(\"project\"),\n        ),\n    )\n\n    conn.commit()\n    conn.close()\n</code></pre>"},{"location":"api/metadata_stores/#flowyml.storage.metadata.SQLiteMetadataStore.save_experiment","title":"<code>save_experiment(experiment_id: str, name: str, description: str = '', tags: dict = None) -&gt; None</code>","text":"<p>Save experiment metadata.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_id</code> <code>str</code> <p>Unique experiment identifier</p> required <code>name</code> <code>str</code> <p>Experiment name</p> required <code>description</code> <code>str</code> <p>Experiment description</p> <code>''</code> <code>tags</code> <code>dict</code> <p>Experiment tags</p> <code>None</code> Source code in <code>flowyml/storage/metadata.py</code> <pre><code>def save_experiment(self, experiment_id: str, name: str, description: str = \"\", tags: dict = None) -&gt; None:\n    \"\"\"Save experiment metadata.\n\n    Args:\n        experiment_id: Unique experiment identifier\n        name: Experiment name\n        description: Experiment description\n        tags: Experiment tags\n    \"\"\"\n    conn = sqlite3.connect(self.db_path)\n    cursor = conn.cursor()\n\n    cursor.execute(\n        \"\"\"\n        INSERT OR REPLACE INTO experiments\n        (experiment_id, name, description, tags)\n        VALUES (?, ?, ?, ?)\n    \"\"\",\n        (\n            experiment_id,\n            name,\n            description,\n            json.dumps(tags or {}),\n        ),\n    )\n\n    conn.commit()\n    conn.close()\n</code></pre>"},{"location":"api/metadata_stores/#flowyml.storage.metadata.SQLiteMetadataStore.save_metric","title":"<code>save_metric(run_id: str, name: str, value: float, step: int = 0) -&gt; None</code>","text":"<p>Save a single metric value.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier</p> required <code>name</code> <code>str</code> <p>Metric name</p> required <code>value</code> <code>float</code> <p>Metric value</p> required <code>step</code> <code>int</code> <p>Training step/iteration</p> <code>0</code> Source code in <code>flowyml/storage/metadata.py</code> <pre><code>def save_metric(self, run_id: str, name: str, value: float, step: int = 0) -&gt; None:\n    \"\"\"Save a single metric value.\n\n    Args:\n        run_id: Run identifier\n        name: Metric name\n        value: Metric value\n        step: Training step/iteration\n    \"\"\"\n    conn = sqlite3.connect(self.db_path)\n    cursor = conn.cursor()\n\n    cursor.execute(\n        \"INSERT INTO metrics (run_id, name, value, step) VALUES (?, ?, ?, ?)\",\n        (run_id, name, value, step),\n    )\n\n    conn.commit()\n    conn.close()\n</code></pre>"},{"location":"api/metadata_stores/#flowyml.storage.metadata.SQLiteMetadataStore.save_pipeline_definition","title":"<code>save_pipeline_definition(pipeline_name: str, definition: dict) -&gt; None</code>","text":"<p>Save pipeline definition for scheduling.</p> Source code in <code>flowyml/storage/metadata.py</code> <pre><code>def save_pipeline_definition(self, pipeline_name: str, definition: dict) -&gt; None:\n    \"\"\"Save pipeline definition for scheduling.\"\"\"\n    from datetime import datetime\n\n    conn = sqlite3.connect(self.db_path)\n    cursor = conn.cursor()\n    now = datetime.now().isoformat()\n\n    # Check if definition already exists\n    cursor.execute(\n        \"SELECT pipeline_name FROM pipeline_definitions WHERE pipeline_name = ?\",\n        (pipeline_name,),\n    )\n    exists = cursor.fetchone()\n\n    if exists:\n        # Update existing\n        cursor.execute(\n            \"\"\"\n            UPDATE pipeline_definitions\n            SET definition = ?, updated_at = ?\n            WHERE pipeline_name = ?\n            \"\"\",\n            (json.dumps(definition), now, pipeline_name),\n        )\n    else:\n        # Insert new\n        cursor.execute(\n            \"\"\"\n            INSERT INTO pipeline_definitions (pipeline_name, definition, created_at, updated_at)\n            VALUES (?, ?, ?, ?)\n            \"\"\",\n            (pipeline_name, json.dumps(definition), now, now),\n        )\n\n    conn.commit()\n    conn.close()\n</code></pre>"},{"location":"api/metadata_stores/#flowyml.storage.metadata.SQLiteMetadataStore.save_run","title":"<code>save_run(run_id: str, metadata: dict) -&gt; None</code>","text":"<p>Save run metadata to database.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Unique run identifier</p> required <code>metadata</code> <code>dict</code> <p>Run metadata dictionary</p> required Source code in <code>flowyml/storage/metadata.py</code> <pre><code>def save_run(self, run_id: str, metadata: dict) -&gt; None:\n    \"\"\"Save run metadata to database.\n\n    Args:\n        run_id: Unique run identifier\n        metadata: Run metadata dictionary\n    \"\"\"\n    conn = sqlite3.connect(self.db_path)\n    cursor = conn.cursor()\n\n    cursor.execute(\n        \"\"\"\n        INSERT OR REPLACE INTO runs\n        (run_id, pipeline_name, status, start_time, end_time, duration, metadata, project)\n        VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n    \"\"\",\n        (\n            run_id,\n            metadata.get(\"pipeline_name\"),\n            metadata.get(\"status\"),\n            metadata.get(\"start_time\"),\n            metadata.get(\"end_time\"),\n            metadata.get(\"duration\"),\n            json.dumps(metadata),\n            metadata.get(\"project\"),\n        ),\n    )\n\n    # Save parameters\n    if \"parameters\" in metadata:\n        cursor.execute(\"DELETE FROM parameters WHERE run_id = ?\", (run_id,))\n        for name, value in metadata[\"parameters\"].items():\n            cursor.execute(\n                \"INSERT INTO parameters (run_id, name, value) VALUES (?, ?, ?)\",\n                (run_id, name, json.dumps(value)),\n            )\n\n    # Save metrics\n    if \"metrics\" in metadata:\n        cursor.execute(\"DELETE FROM metrics WHERE run_id = ?\", (run_id,))\n        for name, value in metadata[\"metrics\"].items():\n            cursor.execute(\n                \"INSERT INTO metrics (run_id, name, value, step) VALUES (?, ?, ?, ?)\",\n                (run_id, name, value, 0),\n            )\n\n    conn.commit()\n    conn.close()\n</code></pre>"},{"location":"api/metadata_stores/#flowyml.storage.metadata.SQLiteMetadataStore.save_trace_event","title":"<code>save_trace_event(event: dict) -&gt; None</code>","text":"<p>Save a trace event.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>dict</code> <p>Trace event dictionary</p> required Source code in <code>flowyml/storage/metadata.py</code> <pre><code>def save_trace_event(self, event: dict) -&gt; None:\n    \"\"\"Save a trace event.\n\n    Args:\n        event: Trace event dictionary\n    \"\"\"\n    conn = sqlite3.connect(self.db_path)\n    cursor = conn.cursor()\n\n    cursor.execute(\n        \"\"\"\n        INSERT OR REPLACE INTO traces\n        (event_id, trace_id, parent_id, event_type, name, inputs, outputs,\n         start_time, end_time, duration, status, error, metadata,\n         prompt_tokens, completion_tokens, total_tokens, cost, model)\n        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n    \"\"\",\n        (\n            event[\"event_id\"],\n            event[\"trace_id\"],\n            event[\"parent_id\"],\n            event[\"event_type\"],\n            event[\"name\"],\n            json.dumps(event.get(\"inputs\", {})),\n            json.dumps(event.get(\"outputs\", {})),\n            event.get(\"start_time\"),\n            event.get(\"end_time\"),\n            event.get(\"duration\"),\n            event.get(\"status\"),\n            event.get(\"error\"),\n            json.dumps(event.get(\"metadata\", {})),\n            event.get(\"prompt_tokens\", 0),\n            event.get(\"completion_tokens\", 0),\n            event.get(\"total_tokens\", 0),\n            event.get(\"cost\", 0.0),\n            event.get(\"model\"),\n        ),\n    )\n\n    conn.commit()\n    conn.close()\n</code></pre>"},{"location":"api/metadata_stores/#flowyml.storage.metadata.SQLiteMetadataStore.update_experiment_project","title":"<code>update_experiment_project(experiment_name: str, project_name: str) -&gt; None</code>","text":"<p>Update the project for an experiment.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>str</code> <p>Name of the experiment</p> required <code>project_name</code> <code>str</code> <p>New project name</p> required Source code in <code>flowyml/storage/metadata.py</code> <pre><code>def update_experiment_project(self, experiment_name: str, project_name: str) -&gt; None:\n    \"\"\"Update the project for an experiment.\n\n    Args:\n        experiment_name: Name of the experiment\n        project_name: New project name\n    \"\"\"\n    conn = sqlite3.connect(self.db_path)\n    cursor = conn.cursor()\n\n    try:\n        cursor.execute(\n            \"UPDATE experiments SET project = ? WHERE name = ?\",\n            (project_name, experiment_name),\n        )\n        conn.commit()\n    finally:\n        conn.close()\n</code></pre>"},{"location":"api/metadata_stores/#flowyml.storage.metadata.SQLiteMetadataStore.update_pipeline_project","title":"<code>update_pipeline_project(pipeline_name: str, project_name: str) -&gt; None</code>","text":"<p>Update the project for all runs of a pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline</p> required <code>project_name</code> <code>str</code> <p>New project name</p> required Source code in <code>flowyml/storage/metadata.py</code> <pre><code>def update_pipeline_project(self, pipeline_name: str, project_name: str) -&gt; None:\n    \"\"\"Update the project for all runs of a pipeline.\n\n    Args:\n        pipeline_name: Name of the pipeline\n        project_name: New project name\n    \"\"\"\n    conn = sqlite3.connect(self.db_path)\n    cursor = conn.cursor()\n\n    try:\n        # Update runs table\n        cursor.execute(\n            \"UPDATE runs SET project = ? WHERE pipeline_name = ?\",\n            (project_name, pipeline_name),\n        )\n\n        # Update artifacts table (optional, but good for consistency if artifacts store project)\n        # Currently artifacts are linked to runs, so run update might be enough\n        # But let's check if artifacts table has project column\n        cursor.execute(\"PRAGMA table_info(artifacts)\")\n        columns = [info[1] for info in cursor.fetchall()]\n        if \"project\" in columns:\n            cursor.execute(\n                \"\"\"\n                UPDATE artifacts\n                SET project = ?\n                WHERE run_id IN (SELECT run_id FROM runs WHERE pipeline_name = ?)\n                \"\"\",\n                (project_name, pipeline_name),\n            )\n\n        conn.commit()\n    finally:\n        conn.close()\n</code></pre>"},{"location":"api/metadata_stores/#flowyml.storage.metadata.SQLiteMetadataStore.update_run_project","title":"<code>update_run_project(run_id: str, project_name: str) -&gt; None</code>","text":"<p>Update the project for a run.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier</p> required <code>project_name</code> <code>str</code> <p>Name of the project</p> required Source code in <code>flowyml/storage/metadata.py</code> <pre><code>def update_run_project(self, run_id: str, project_name: str) -&gt; None:\n    \"\"\"Update the project for a run.\n\n    Args:\n        run_id: Run identifier\n        project_name: Name of the project\n    \"\"\"\n    conn = sqlite3.connect(self.db_path)\n    cursor = conn.cursor()\n\n    # 1. Update the column\n    cursor.execute(\"UPDATE runs SET project = ? WHERE run_id = ?\", (project_name, run_id))\n\n    # 2. Update the JSON blob\n    cursor.execute(\"SELECT metadata FROM runs WHERE run_id = ?\", (run_id,))\n    row = cursor.fetchone()\n    if row:\n        metadata = json.loads(row[0])\n        metadata[\"project\"] = project_name\n        cursor.execute(\n            \"UPDATE runs SET metadata = ? WHERE run_id = ?\",\n            (json.dumps(metadata), run_id),\n        )\n\n    conn.commit()\n    conn.close()\n</code></pre>"},{"location":"api/orchestrators/","title":"Orchestrators API \ud83c\udfbc","text":"<p>Orchestrators manage the execution of pipeline steps.</p>"},{"location":"api/orchestrators/#base-executor","title":"Base Executor","text":"<p>Base executor for running pipeline steps.</p>"},{"location":"api/orchestrators/#flowyml.core.executor.Executor-functions","title":"Functions","text":""},{"location":"api/orchestrators/#flowyml.core.executor.Executor.execute_step","title":"<code>execute_step(step, inputs: dict[str, Any], context_params: dict[str, Any], cache_store: Any | None = None) -&gt; ExecutionResult</code>","text":"<p>Execute a single step.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <p>Step to execute</p> required <code>inputs</code> <code>dict[str, Any]</code> <p>Input data for the step</p> required <code>context_params</code> <code>dict[str, Any]</code> <p>Parameters from context</p> required <code>cache_store</code> <code>Any | None</code> <p>Cache store for caching</p> <code>None</code> <p>Returns:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult with output or error</p> Source code in <code>flowyml/core/executor.py</code> <pre><code>def execute_step(\n    self,\n    step,\n    inputs: dict[str, Any],\n    context_params: dict[str, Any],\n    cache_store: Any | None = None,\n) -&gt; ExecutionResult:\n    \"\"\"Execute a single step.\n\n    Args:\n        step: Step to execute\n        inputs: Input data for the step\n        context_params: Parameters from context\n        cache_store: Cache store for caching\n\n    Returns:\n        ExecutionResult with output or error\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/orchestrators/#flowyml.core.executor.Executor.execute_step_group","title":"<code>execute_step_group(step_group, inputs: dict[str, Any], context_params: dict[str, Any], cache_store: Any | None = None, artifact_store: Any | None = None, run_id: str | None = None, project_name: str = 'default') -&gt; list[ExecutionResult]</code>","text":"<p>Execute a group of steps together.</p> <p>Parameters:</p> Name Type Description Default <code>step_group</code> <p>StepGroup to execute</p> required <code>inputs</code> <code>dict[str, Any]</code> <p>Input data available to the group</p> required <code>context_params</code> <code>dict[str, Any]</code> <p>Parameters from context</p> required <code>cache_store</code> <code>Any | None</code> <p>Cache store for caching</p> <code>None</code> <code>artifact_store</code> <code>Any | None</code> <p>Artifact store for materialization</p> <code>None</code> <code>run_id</code> <code>str | None</code> <p>Run identifier</p> <code>None</code> <code>project_name</code> <code>str</code> <p>Project name</p> <code>'default'</code> <p>Returns:</p> Type Description <code>list[ExecutionResult]</code> <p>List of ExecutionResult (one per step)</p> Source code in <code>flowyml/core/executor.py</code> <pre><code>def execute_step_group(\n    self,\n    step_group,  # StepGroup\n    inputs: dict[str, Any],\n    context_params: dict[str, Any],\n    cache_store: Any | None = None,\n    artifact_store: Any | None = None,\n    run_id: str | None = None,\n    project_name: str = \"default\",\n) -&gt; list[ExecutionResult]:\n    \"\"\"Execute a group of steps together.\n\n    Args:\n        step_group: StepGroup to execute\n        inputs: Input data available to the group\n        context_params: Parameters from context\n        cache_store: Cache store for caching\n        artifact_store: Artifact store for materialization\n        run_id: Run identifier\n        project_name: Project name\n\n    Returns:\n        List of ExecutionResult (one per step)\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/orchestrators/#local-executor","title":"Local Executor","text":"<p>               Bases: <code>Executor</code></p> <p>Local executor - runs steps in the current process.</p>"},{"location":"api/orchestrators/#flowyml.core.executor.LocalExecutor-functions","title":"Functions","text":""},{"location":"api/orchestrators/#flowyml.core.executor.LocalExecutor.execute_step","title":"<code>execute_step(step, inputs: dict[str, Any], context_params: dict[str, Any], cache_store: Any | None = None, artifact_store: Any | None = None, run_id: str | None = None, project_name: str = 'default') -&gt; ExecutionResult</code>","text":"<p>Execute step locally with retry, caching, and materialization.</p> Source code in <code>flowyml/core/executor.py</code> <pre><code>def execute_step(\n    self,\n    step,\n    inputs: dict[str, Any],\n    context_params: dict[str, Any],\n    cache_store: Any | None = None,\n    artifact_store: Any | None = None,\n    run_id: str | None = None,\n    project_name: str = \"default\",\n) -&gt; ExecutionResult:\n    \"\"\"Execute step locally with retry, caching, and materialization.\"\"\"\n    start_time = time.time()\n    retries = 0\n\n    # Check condition\n    if step.condition:\n        try:\n            # We pass inputs and context params to condition if it accepts them\n            # For simplicity, let's try to inspect the condition function\n            # or just pass what we can.\n            # A simple approach: pass nothing if it takes no args, or kwargs if it does.\n            # But inspect is safer.\n            import inspect\n\n            sig = inspect.signature(step.condition)\n            kwargs = {**inputs, **context_params}\n\n            # Filter kwargs to only what condition accepts\n            cond_kwargs = {k: v for k, v in kwargs.items() if k in sig.parameters}\n\n            should_run = step.condition(**cond_kwargs)\n\n            if not should_run:\n                duration = time.time() - start_time\n                return ExecutionResult(\n                    step_name=step.name,\n                    success=True,\n                    output=None,  # Skipped steps produce None\n                    duration_seconds=duration,\n                    skipped=True,\n                )\n        except Exception as e:\n            # If condition check fails, treat as step failure\n            duration = time.time() - start_time\n            return ExecutionResult(\n                step_name=step.name,\n                success=False,\n                error=f\"Condition check failed: {str(e)}\",\n                duration_seconds=duration,\n            )\n\n    # Check cache\n    if cache_store and step.cache:\n        cache_key = step.get_cache_key(inputs)\n        cached_result = cache_store.get(cache_key)\n\n        if cached_result is not None:\n            duration = time.time() - start_time\n            return ExecutionResult(\n                step_name=step.name,\n                success=True,\n                output=cached_result,\n                duration_seconds=duration,\n                cached=True,\n            )\n\n    # Execute with retry\n    max_retries = step.retry\n    last_error = None\n\n    for attempt in range(max_retries + 1):\n        try:\n            # Prepare arguments\n            kwargs = {**inputs, **context_params}\n\n            # Execute step\n            result = step.func(**kwargs)\n\n            # Materialize output if artifact store is available\n            artifact_uri = None\n            if artifact_store and result is not None and run_id:\n                with contextlib.suppress(Exception):\n                    artifact_uri = artifact_store.materialize(\n                        obj=result,\n                        name=\"output\",  # Default name for single output\n                        run_id=run_id,\n                        step_name=step.name,\n                        project_name=project_name,\n                    )\n\n            # Cache result\n            if cache_store and step.cache:\n                cache_key = step.get_cache_key(inputs)\n                cache_store.set_value(\n                    cache_key,\n                    result,\n                    step.name,\n                    step.get_code_hash(),\n                )\n\n            duration = time.time() - start_time\n            return ExecutionResult(\n                step_name=step.name,\n                success=True,\n                output=result,\n                duration_seconds=duration,\n                retries=retries,\n                artifact_uri=artifact_uri,\n            )\n\n        except Exception as e:\n            last_error = str(e)\n            retries += 1\n\n            if attempt &lt; max_retries:\n                # Wait before retry (exponential backoff)\n                wait_time = 2**attempt\n                time.sleep(wait_time)\n                continue\n\n            # All retries exhausted\n            duration = time.time() - start_time\n            return ExecutionResult(\n                step_name=step.name,\n                success=False,\n                error=f\"{last_error}\\n{traceback.format_exc()}\",\n                duration_seconds=duration,\n                retries=retries,\n            )\n\n    # Should never reach here\n    duration = time.time() - start_time\n    return ExecutionResult(\n        step_name=step.name,\n        success=False,\n        error=last_error,\n        duration_seconds=duration,\n        retries=retries,\n    )\n</code></pre>"},{"location":"api/orchestrators/#flowyml.core.executor.LocalExecutor.execute_step_group","title":"<code>execute_step_group(step_group, inputs: dict[str, Any], context_params: dict[str, Any], cache_store: Any | None = None, artifact_store: Any | None = None, run_id: str | None = None, project_name: str = 'default') -&gt; list[ExecutionResult]</code>","text":"<p>Execute a group of steps together in the same environment.</p> <p>For local execution, steps execute sequentially but share the same process.</p> <p>Parameters:</p> Name Type Description Default <code>step_group</code> <p>StepGroup containing steps to execute</p> required <code>inputs</code> <code>dict[str, Any]</code> <p>Input data available to the group</p> required <code>context_params</code> <code>dict[str, Any]</code> <p>Parameters from context</p> required <code>cache_store</code> <code>Any | None</code> <p>Cache store for caching</p> <code>None</code> <code>artifact_store</code> <code>Any | None</code> <p>Artifact store for materialization</p> <code>None</code> <code>run_id</code> <code>str | None</code> <p>Run identifier</p> <code>None</code> <code>project_name</code> <code>str</code> <p>Project name</p> <code>'default'</code> <p>Returns:</p> Type Description <code>list[ExecutionResult]</code> <p>List of ExecutionResult (one per step in execution order)</p> Source code in <code>flowyml/core/executor.py</code> <pre><code>def execute_step_group(\n    self,\n    step_group,  # StepGroup from step_grouping module\n    inputs: dict[str, Any],\n    context_params: dict[str, Any],\n    cache_store: Any | None = None,\n    artifact_store: Any | None = None,\n    run_id: str | None = None,\n    project_name: str = \"default\",\n) -&gt; list[ExecutionResult]:\n    \"\"\"Execute a group of steps together in the same environment.\n\n    For local execution, steps execute sequentially but share the same process.\n\n    Args:\n        step_group: StepGroup containing steps to execute\n        inputs: Input data available to the group\n        context_params: Parameters from context\n        cache_store: Cache store for caching\n        artifact_store: Artifact store for materialization\n        run_id: Run identifier\n        project_name: Project name\n\n    Returns:\n        List of ExecutionResult (one per step in execution order)\n    \"\"\"\n    results: list[ExecutionResult] = []\n    step_outputs = dict(inputs)  # Copy initial inputs\n\n    # Execute steps in their defined order\n    for step_name in step_group.execution_order:\n        # Find the step object\n        step = next(s for s in step_group.steps if s.name == step_name)\n\n        # Prepare inputs for this step\n        step_inputs = {}\n        for input_name in step.inputs:\n            if input_name in step_outputs:\n                step_inputs[input_name] = step_outputs[input_name]\n\n        # Execute this step\n        result = self.execute_step(\n            step=step,\n            inputs=step_inputs,\n            context_params=context_params,\n            cache_store=cache_store,\n            artifact_store=artifact_store,\n            run_id=run_id,\n            project_name=project_name,\n        )\n\n        results.append(result)\n\n        # If step failed, stop group execution\n        if not result.success:\n            # Mark remaining steps as skipped\n            current_index = step_group.execution_order.index(step_name)\n            remaining_steps = step_group.execution_order[current_index + 1 :]\n\n            for remaining_name in remaining_steps:\n                skip_result = ExecutionResult(\n                    step_name=remaining_name,\n                    success=True,  # Set to True since skipped steps technically don't fail\n                    error=\"Skipped due to earlier failure in group\",\n                    skipped=True,\n                )\n                results.append(skip_result)\n            break\n\n        # Store outputs for next steps in group\n        if result.output is not None:\n            if len(step.outputs) == 1:\n                step_outputs[step.outputs[0]] = result.output\n            elif isinstance(result.output, (list, tuple)) and len(result.output) == len(step.outputs):\n                for name, val in zip(step.outputs, result.output, strict=False):\n                    step_outputs[name] = val\n            elif isinstance(result.output, dict):\n                for name in step.outputs:\n                    if name in result.output:\n                        step_outputs[name] = result.output[name]\n            else:\n                if step.outputs:\n                    step_outputs[step.outputs[0]] = result.output\n\n    return results\n</code></pre>"},{"location":"api/orchestrators/#vertex-ai-orchestrator","title":"Vertex AI Orchestrator","text":"<p>               Bases: <code>Orchestrator</code></p> <p>Vertex AI orchestrator for running pipelines on Google Cloud.</p> <p>This orchestrator submits pipeline jobs to Vertex AI Pipelines, allowing for scalable, managed execution in the cloud.</p> Example <pre><code>from flowyml.stacks.gcp import VertexAIOrchestrator\n\norchestrator = VertexAIOrchestrator(\n    project_id=\"my-gcp-project\", region=\"us-central1\", service_account=\"my-sa@my-project.iam.gserviceaccount.com\"\n)\n</code></pre> <p>Initialize Vertex AI orchestrator.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the orchestrator</p> <code>'vertex_ai'</code> <code>project_id</code> <code>str | None</code> <p>GCP project ID</p> <code>None</code> <code>region</code> <code>str</code> <p>GCP region for Vertex AI</p> <code>'us-central1'</code> <code>service_account</code> <code>str | None</code> <p>Service account email for job execution</p> <code>None</code> <code>network</code> <code>str | None</code> <p>VPC network for jobs</p> <code>None</code> <code>encryption_key</code> <code>str | None</code> <p>Customer-managed encryption key</p> <code>None</code> Source code in <code>flowyml/stacks/gcp.py</code> <pre><code>def __init__(\n    self,\n    name: str = \"vertex_ai\",\n    project_id: str | None = None,\n    region: str = \"us-central1\",\n    service_account: str | None = None,\n    network: str | None = None,\n    encryption_key: str | None = None,\n):\n    \"\"\"Initialize Vertex AI orchestrator.\n\n    Args:\n        name: Name of the orchestrator\n        project_id: GCP project ID\n        region: GCP region for Vertex AI\n        service_account: Service account email for job execution\n        network: VPC network for jobs\n        encryption_key: Customer-managed encryption key\n    \"\"\"\n    super().__init__(name)\n    self.project_id = project_id\n    self.region = region\n    self.service_account = service_account\n    self.network = network\n    self.encryption_key = encryption_key\n</code></pre>"},{"location":"api/orchestrators/#flowyml.stacks.gcp.VertexAIOrchestrator-functions","title":"Functions","text":""},{"location":"api/orchestrators/#flowyml.stacks.gcp.VertexAIOrchestrator.get_run_status","title":"<code>get_run_status(run_id: str) -&gt; str</code>","text":"<p>Get status of a Vertex AI job.</p> Source code in <code>flowyml/stacks/gcp.py</code> <pre><code>def get_run_status(self, run_id: str) -&gt; str:\n    \"\"\"Get status of a Vertex AI job.\"\"\"\n    from google.cloud import aiplatform\n\n    job = aiplatform.CustomJob(run_id)\n    return job.state.name\n</code></pre>"},{"location":"api/orchestrators/#flowyml.stacks.gcp.VertexAIOrchestrator.run_pipeline","title":"<code>run_pipeline(pipeline: Any, resources: ResourceConfig | None = None, docker_config: DockerConfig | None = None, **kwargs) -&gt; str</code>","text":"<p>Run pipeline on Vertex AI.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>Any</code> <p>Pipeline to run</p> required <code>resources</code> <code>ResourceConfig | None</code> <p>Resource configuration</p> <code>None</code> <code>docker_config</code> <code>DockerConfig | None</code> <p>Docker configuration</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Job ID</p> Source code in <code>flowyml/stacks/gcp.py</code> <pre><code>def run_pipeline(\n    self,\n    pipeline: Any,\n    resources: ResourceConfig | None = None,\n    docker_config: DockerConfig | None = None,\n    **kwargs,\n) -&gt; str:\n    \"\"\"Run pipeline on Vertex AI.\n\n    Args:\n        pipeline: Pipeline to run\n        resources: Resource configuration\n        docker_config: Docker configuration\n        **kwargs: Additional arguments\n\n    Returns:\n        Job ID\n    \"\"\"\n    from google.cloud import aiplatform\n\n    # Initialize Vertex AI\n    aiplatform.init(project=self.project_id, location=self.region)\n\n    # Create custom job\n    job_display_name = f\"{pipeline.name}-{pipeline.run_id}\"\n\n    # Build worker pool specs\n    worker_pool_specs = self._build_worker_pool_specs(\n        docker_config=docker_config,\n        resources=resources,\n    )\n\n    # Create and run custom job\n    job = aiplatform.CustomJob(\n        display_name=job_display_name,\n        worker_pool_specs=worker_pool_specs,\n        service_account=self.service_account,\n        network=self.network,\n        encryption_spec_key_name=self.encryption_key,\n    )\n\n    job.run(sync=False)\n\n    return job.resource_name\n</code></pre>"},{"location":"api/orchestrators/#flowyml.stacks.gcp.VertexAIOrchestrator.to_dict","title":"<code>to_dict() -&gt; dict[str, Any]</code>","text":"<p>Convert to dictionary.</p> Source code in <code>flowyml/stacks/gcp.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert to dictionary.\"\"\"\n    return {\n        \"name\": self.name,\n        \"type\": \"vertex_ai\",\n        \"project_id\": self.project_id,\n        \"region\": self.region,\n        \"service_account\": self.service_account,\n        \"network\": self.network,\n    }\n</code></pre>"},{"location":"api/orchestrators/#flowyml.stacks.gcp.VertexAIOrchestrator.validate","title":"<code>validate() -&gt; bool</code>","text":"<p>Validate Vertex AI configuration.</p> Source code in <code>flowyml/stacks/gcp.py</code> <pre><code>def validate(self) -&gt; bool:\n    \"\"\"Validate Vertex AI configuration.\"\"\"\n    if not self.project_id:\n        raise ValueError(\"project_id is required for VertexAIOrchestrator\")\n\n    # Check if google-cloud-aiplatform is installed\n    import importlib.util\n\n    if importlib.util.find_spec(\"google.cloud.aiplatform\") is not None:\n        return True\n    raise ImportError(\n        \"google-cloud-aiplatform is required for VertexAIOrchestrator. \"\n        \"Install with: pip install google-cloud-aiplatform\",\n    )\n</code></pre>"},{"location":"api/pipeline/","title":"Pipeline API \ud83c\udfd7\ufe0f","text":"<p>The <code>Pipeline</code> class is the main entry point for defining and running workflows in flowyml.</p>"},{"location":"api/pipeline/#usage","title":"Usage","text":"<pre><code>from flowyml import Pipeline\n\npipeline = Pipeline(\"my_pipeline\")\npipeline.add_step(step_func)\npipeline.run()\n</code></pre>"},{"location":"api/pipeline/#class-pipeline","title":"Class <code>Pipeline</code>","text":"<p>Main pipeline class for orchestrating ML workflows.</p> Example <p>from flowyml import Pipeline, step, context ctx = context(learning_rate=0.001, epochs=10) @step(outputs=[\"model/trained\"]) ... def train(learning_rate: float, epochs: int): ...     return train_model(learning_rate, epochs) pipeline = Pipeline(\"my_pipeline\", context=ctx) pipeline.add_step(train) result = pipeline.run()</p> <p>Initialize pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the pipeline</p> required <code>context</code> <code>Context | None</code> <p>Optional context for parameter injection</p> <code>None</code> <code>executor</code> <code>Executor | None</code> <p>Optional executor (defaults to LocalExecutor)</p> <code>None</code> <code>enable_cache</code> <code>bool</code> <p>Whether to enable caching</p> <code>True</code> <code>cache_dir</code> <code>str | None</code> <p>Optional directory for cache</p> <code>None</code> <code>stack</code> <code>Any | None</code> <p>Optional stack instance to run on</p> <code>None</code> <code>project</code> <code>str | None</code> <p>Optional project name to attach this pipeline to.</p> <code>None</code> Source code in <code>flowyml/core/pipeline.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    context: Context | None = None,\n    executor: Executor | None = None,\n    enable_cache: bool = True,\n    cache_dir: str | None = None,\n    stack: Any | None = None,  # Stack instance\n    project: str | None = None,  # Project name to attach to\n):\n    \"\"\"Initialize pipeline.\n\n    Args:\n        name: Name of the pipeline\n        context: Optional context for parameter injection\n        executor: Optional executor (defaults to LocalExecutor)\n        enable_cache: Whether to enable caching\n        cache_dir: Optional directory for cache\n        stack: Optional stack instance to run on\n        project: Optional project name to attach this pipeline to.\n    \"\"\"\n    self.name = name\n    self.context = context or Context()\n    self.enable_cache = enable_cache\n    self.stack = stack  # Store stack instance\n\n    self.steps: list[Step] = []\n    self.dag = DAG()\n\n    # Storage\n    if cache_dir is None:\n        from flowyml.utils.config import get_config\n\n        cache_dir = str(get_config().cache_dir)\n\n    self.cache_store = CacheStore(cache_dir) if enable_cache else None\n\n    from flowyml.utils.config import get_config\n\n    self.runs_dir = get_config().runs_dir\n    self.runs_dir.mkdir(parents=True, exist_ok=True)\n\n    # Initialize components from stack or defaults\n    if self.stack:\n        self.executor = executor or self.stack.executor\n        self.metadata_store = self.stack.metadata_store\n    else:\n        self.executor = executor or LocalExecutor()\n        # Metadata store for UI integration\n        from flowyml.storage.metadata import SQLiteMetadataStore\n\n        self.metadata_store = SQLiteMetadataStore()\n\n    # Handle Project Attachment\n    if project:\n        from flowyml.core.project import ProjectManager\n\n        manager = ProjectManager()\n        # Get or create project\n        proj = manager.get_project(project)\n        if not proj:\n            proj = manager.create_project(project)\n\n        # Configure pipeline with project settings\n        self.runs_dir = proj.runs_dir\n        self.metadata_store = proj.metadata_store\n\n        # Register pipeline with project\n        if name not in proj.metadata[\"pipelines\"]:\n            proj.metadata[\"pipelines\"].append(name)\n            proj._save_metadata()\n\n    # State\n    self._built = False\n    self.step_groups: list[Any] = []  # Will hold StepGroup objects\n</code></pre>"},{"location":"api/pipeline/#flowyml.core.pipeline.Pipeline-functions","title":"Functions","text":""},{"location":"api/pipeline/#flowyml.core.pipeline.Pipeline.add_step","title":"<code>add_step(step: Step) -&gt; Pipeline</code>","text":"<p>Add a step to the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>Step</code> <p>Step to add</p> required <p>Returns:</p> Type Description <code>Pipeline</code> <p>Self for chaining</p> Source code in <code>flowyml/core/pipeline.py</code> <pre><code>def add_step(self, step: Step) -&gt; \"Pipeline\":\n    \"\"\"Add a step to the pipeline.\n\n    Args:\n        step: Step to add\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self.steps.append(step)\n    self._built = False\n    return self\n</code></pre>"},{"location":"api/pipeline/#flowyml.core.pipeline.Pipeline.build","title":"<code>build() -&gt; None</code>","text":"<p>Build the execution DAG.</p> Source code in <code>flowyml/core/pipeline.py</code> <pre><code>def build(self) -&gt; None:\n    \"\"\"Build the execution DAG.\"\"\"\n    if self._built:\n        return\n\n    # Clear previous DAG\n    self.dag = DAG()\n\n    # Add nodes\n    for step in self.steps:\n        node = Node(\n            name=step.name,\n            step=step,\n            inputs=step.inputs,\n            outputs=step.outputs,\n        )\n        self.dag.add_node(node)\n\n    # Build edges\n    self.dag.build_edges()\n\n    # Validate\n    errors = self.dag.validate()\n    if errors:\n        raise ValueError(\"Pipeline validation failed:\\n\" + \"\\n\".join(errors))\n\n    # Analyze step groups\n    from flowyml.core.step_grouping import StepGroupAnalyzer\n\n    analyzer = StepGroupAnalyzer()\n    self.step_groups = analyzer.analyze_groups(self.dag, self.steps)\n\n    self._built = True\n</code></pre>"},{"location":"api/pipeline/#flowyml.core.pipeline.Pipeline.cache_stats","title":"<code>cache_stats() -&gt; dict[str, Any]</code>","text":"<p>Get cache statistics.</p> Source code in <code>flowyml/core/pipeline.py</code> <pre><code>def cache_stats(self) -&gt; dict[str, Any]:\n    \"\"\"Get cache statistics.\"\"\"\n    if self.cache_store:\n        return self.cache_store.stats()\n    return {}\n</code></pre>"},{"location":"api/pipeline/#flowyml.core.pipeline.Pipeline.from_definition","title":"<code>from_definition(definition: dict, context: Context | None = None) -&gt; Pipeline</code>  <code>classmethod</code>","text":"<p>Reconstruct pipeline from stored definition.</p> <p>This creates a \"ghost\" pipeline that can be executed but uses the stored step structure. Actual step logic must still be available in the codebase.</p> <p>Parameters:</p> Name Type Description Default <code>definition</code> <code>dict</code> <p>Pipeline definition from to_definition()</p> required <code>context</code> <code>Context | None</code> <p>Optional context for execution</p> <code>None</code> <p>Returns:</p> Type Description <code>Pipeline</code> <p>Reconstructed Pipeline instance</p> Source code in <code>flowyml/core/pipeline.py</code> <pre><code>@classmethod\ndef from_definition(cls, definition: dict, context: Context | None = None) -&gt; \"Pipeline\":\n    \"\"\"Reconstruct pipeline from stored definition.\n\n    This creates a \"ghost\" pipeline that can be executed but uses\n    the stored step structure. Actual step logic must still be\n    available in the codebase.\n\n    Args:\n        definition: Pipeline definition from to_definition()\n        context: Optional context for execution\n\n    Returns:\n        Reconstructed Pipeline instance\n    \"\"\"\n    from flowyml.core.step import step as step_decorator\n\n    # Create pipeline instance\n    pipeline = cls(\n        name=definition[\"name\"],\n        context=context or Context(),\n    )\n\n    # Reconstruct steps\n    for step_def in definition[\"steps\"]:\n        # Create a generic step function that can be called\n        # In a real implementation, we'd need to either:\n        # 1. Store serialized functions (using cloudpickle)\n        # 2. Import functions by name from codebase\n        # 3. Use placeholder functions\n\n        # For now, we'll create a placeholder that logs execution\n        def generic_step_func(*args, **kwargs):\n            \"\"\"Generic step function for reconstructed pipeline.\"\"\"\n            print(f\"Executing reconstructed step with args={args}, kwargs={kwargs}\")\n            return\n\n        # Apply step decorator with stored metadata\n        decorated = step_decorator(\n            name=step_def[\"name\"],\n            inputs=step_def[\"inputs\"],\n            outputs=step_def[\"outputs\"],\n            tags=step_def.get(\"tags\", []),\n        )(generic_step_func)\n\n        # Add to pipeline\n        pipeline.add_step(decorated)\n\n    return pipeline\n</code></pre>"},{"location":"api/pipeline/#flowyml.core.pipeline.Pipeline.invalidate_cache","title":"<code>invalidate_cache(step: str | None = None, before: str | None = None) -&gt; None</code>","text":"<p>Invalidate cache entries.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>str | None</code> <p>Invalidate cache for specific step</p> <code>None</code> <code>before</code> <code>str | None</code> <p>Invalidate cache entries before date</p> <code>None</code> Source code in <code>flowyml/core/pipeline.py</code> <pre><code>def invalidate_cache(\n    self,\n    step: str | None = None,\n    before: str | None = None,\n) -&gt; None:\n    \"\"\"Invalidate cache entries.\n\n    Args:\n        step: Invalidate cache for specific step\n        before: Invalidate cache entries before date\n    \"\"\"\n    if self.cache_store:\n        if step:\n            self.cache_store.invalidate(step_name=step)\n        else:\n            self.cache_store.clear()\n</code></pre>"},{"location":"api/pipeline/#flowyml.core.pipeline.Pipeline.run","title":"<code>run(inputs: dict[str, Any] | None = None, debug: bool = False, stack: Any | None = None, resources: Any | None = None, docker_config: Any | None = None, context: dict[str, Any] | None = None) -&gt; PipelineResult</code>","text":"<p>Execute the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any] | None</code> <p>Optional input data for the pipeline</p> <code>None</code> <code>debug</code> <code>bool</code> <p>Enable debug mode with detailed logging</p> <code>False</code> <code>stack</code> <code>Any | None</code> <p>Stack override (uses self.stack if not provided)</p> <code>None</code> <code>resources</code> <code>Any | None</code> <p>Resource configuration for execution</p> <code>None</code> <code>docker_config</code> <code>Any | None</code> <p>Docker configuration for containerized execution</p> <code>None</code> <code>context</code> <code>dict[str, Any] | None</code> <p>Context variables override</p> <code>None</code> <p>Returns:</p> Type Description <code>PipelineResult</code> <p>PipelineResult with outputs and execution info</p> Source code in <code>flowyml/core/pipeline.py</code> <pre><code>def run(\n    self,\n    inputs: dict[str, Any] | None = None,\n    debug: bool = False,\n    stack: Any | None = None,  # Stack override\n    resources: Any | None = None,  # ResourceConfig\n    docker_config: Any | None = None,  # DockerConfig\n    context: dict[str, Any] | None = None,  # Context vars override\n) -&gt; PipelineResult:\n    \"\"\"Execute the pipeline.\n\n    Args:\n        inputs: Optional input data for the pipeline\n        debug: Enable debug mode with detailed logging\n        stack: Stack override (uses self.stack if not provided)\n        resources: Resource configuration for execution\n        docker_config: Docker configuration for containerized execution\n        context: Context variables override\n\n    Returns:\n        PipelineResult with outputs and execution info\n    \"\"\"\n    import uuid\n\n    run_id = str(uuid.uuid4())\n\n    # Use provided stack or instance stack\n    if stack is not None:\n        self.stack = stack\n        # Update components from new stack\n        self.executor = self.stack.executor\n        self.metadata_store = self.stack.metadata_store\n\n    # Determine artifact store\n    artifact_store = None\n    if self.stack:\n        artifact_store = self.stack.artifact_store\n\n    # Update context with provided values\n    if context:\n        self.context.update(context)\n\n    # Build DAG if needed\n    if not self._built:\n        self.build()\n\n    # Initialize result\n    result = PipelineResult(run_id, self.name)\n    step_outputs = inputs or {}\n\n    # Map step names to step objects for easier lookup\n    self.steps_dict = {step.name: step for step in self.steps}\n    if debug:\n        pass\n    else:\n        # Always print the run URL for better UX\n        pass\n\n    # Get execution units (individual steps or groups)\n    from flowyml.core.step_grouping import get_execution_units\n\n    execution_units = get_execution_units(self.dag, self.steps)\n\n    # Execute steps/groups in order\n    for unit in execution_units:\n        # Check if unit is a group or individual step\n        from flowyml.core.step_grouping import StepGroup\n\n        if isinstance(unit, StepGroup):\n            # Execute entire group\n            if debug:\n                pass\n\n            # Get context parameters (use first step's function as representative)\n            first_step = unit.steps[0]\n            context_params = self.context.inject_params(first_step.func)\n\n            # Execute the group\n            group_results = self.executor.execute_step_group(\n                step_group=unit,\n                inputs=step_outputs,\n                context_params=context_params,\n                cache_store=self.cache_store,\n                artifact_store=artifact_store,\n                run_id=run_id,\n                project_name=self.name,\n            )\n\n            # Process each step result\n            for step_result in group_results:\n                result.add_step_result(step_result)\n\n                if debug:\n                    pass\n\n                # Handle failure\n                if not step_result.success and not step_result.skipped:\n                    result.finalize(success=False)\n                    self._save_run(result)\n                    return result\n\n                # Store outputs for next steps/groups\n                if step_result.output is not None:\n                    # Find step definition to get output names\n                    step_def = next((s for s in self.steps if s.name == step_result.step_name), None)\n                    if step_def:\n                        if len(step_def.outputs) == 1:\n                            step_outputs[step_def.outputs[0]] = step_result.output\n                            result.outputs[step_def.outputs[0]] = step_result.output\n                        elif isinstance(step_result.output, (list, tuple)) and len(step_result.output) == len(\n                            step_def.outputs,\n                        ):\n                            for name, val in zip(step_def.outputs, step_result.output, strict=False):\n                                step_outputs[name] = val\n                                result.outputs[name] = val\n                        elif isinstance(step_result.output, dict):\n                            for name in step_def.outputs:\n                                if name in step_result.output:\n                                    step_outputs[name] = step_result.output[name]\n                                    result.outputs[name] = step_result.output[name]\n                        else:\n                            if step_def.outputs:\n                                step_outputs[step_def.outputs[0]] = step_result.output\n                                result.outputs[step_def.outputs[0]] = step_result.output\n\n        else:\n            # Execute single ungrouped step\n            step = unit\n\n            if debug:\n                pass\n\n            # Prepare step inputs\n            step_inputs = {}\n\n            # Get function signature to map inputs to parameters\n            import inspect\n\n            sig = inspect.signature(step.func)\n            params = list(sig.parameters.values())\n\n            # Filter out self/cls\n            params = [p for p in params if p.name not in (\"self\", \"cls\")]\n\n            # Strategy:\n            # 1. Map inputs to parameters\n            #    - If input name matches param name, use it\n            #    - If not, use positional mapping (input[i] -&gt; param[i])\n\n            # Track which parameters have been assigned\n            assigned_params = set()\n\n            if step.inputs:\n                for i, input_name in enumerate(step.inputs):\n                    if input_name not in step_outputs:\n                        continue\n\n                    val = step_outputs[input_name]\n\n                    # Check if input name matches a parameter\n                    param_match = next((p for p in params if p.name == input_name), None)\n\n                    if param_match:\n                        step_inputs[param_match.name] = val\n                        assigned_params.add(param_match.name)\n                    elif i &lt; len(params):\n                        # Positional fallback\n                        # Only if this parameter hasn't been assigned yet\n                        target_param = params[i]\n                        if target_param.name not in assigned_params:\n                            step_inputs[target_param.name] = val\n                            assigned_params.add(target_param.name)\n\n            # Auto-map parameters from available outputs if they match function signature\n            # This allows passing inputs to run() without declaring them as asset dependencies\n            for param in params:\n                if param.name in step_outputs and param.name not in step_inputs:\n                    step_inputs[param.name] = step_outputs[param.name]\n                    assigned_params.add(param.name)\n\n            # Validate context parameters\n            # Exclude parameters that are already provided in step_inputs\n            exclude_params = list(step.inputs) + list(step_inputs.keys())\n            missing_params = self.context.validate_for_step(step.func, exclude=exclude_params)\n            if missing_params:\n                if debug:\n                    pass\n\n                error_msg = f\"Missing required parameters: {missing_params}\"\n                step_result = ExecutionResult(\n                    step_name=step.name,\n                    success=False,\n                    error=error_msg,\n                )\n                result.add_step_result(step_result)\n                result.finalize(success=False)\n                self._save_run(result)  # Save run before returning\n                self._save_pipeline_definition()  # Save definition even on failure\n                print(\"DEBUG: Pipeline failed at step execution\")\n                return result\n\n            # Get context parameters for this step\n            context_params = self.context.inject_params(step.func)\n\n            # Execute step\n            step_result = self.executor.execute_step(\n                step,\n                step_inputs,\n                context_params,\n                self.cache_store,\n                artifact_store=artifact_store,\n                run_id=run_id,\n                project_name=self.name,\n            )\n\n            result.add_step_result(step_result)\n\n            if debug:\n                pass\n\n            # Handle failure\n            if not step_result.success:\n                if debug and not step_result.error:\n                    pass\n                result.finalize(success=False)\n                self._save_run(result)\n                self._save_pipeline_definition()  # Save definition even on failure\n                print(\"DEBUG: Pipeline failed at step execution\")\n                return result\n\n            # Store outputs for next steps\n            if step_result.output is not None:\n                if len(step.outputs) == 1:\n                    step_outputs[step.outputs[0]] = step_result.output\n                    result.outputs[step.outputs[0]] = step_result.output\n                elif isinstance(step_result.output, (list, tuple)) and len(step_result.output) == len(step.outputs):\n                    for name, val in zip(step.outputs, step_result.output, strict=False):\n                        step_outputs[name] = val\n                        result.outputs[name] = val\n                elif isinstance(step_result.output, dict):\n                    for name in step.outputs:\n                        if name in step_result.output:\n                            step_outputs[name] = step_result.output[name]\n                            result.outputs[name] = step_result.output[name]\n                else:\n                    # Fallback: assign to first output if available\n                    if step.outputs:\n                        step_outputs[step.outputs[0]] = step_result.output\n                        result.outputs[step.outputs[0]] = step_result.output\n\n    # Success!\n    result.finalize(success=True)\n\n    if debug:\n        pass\n\n    self._save_run(result)\n    self._save_pipeline_definition()  # Save pipeline structure for scheduling\n    return result\n</code></pre>"},{"location":"api/pipeline/#flowyml.core.pipeline.Pipeline.to_definition","title":"<code>to_definition() -&gt; dict</code>","text":"<p>Serialize pipeline to definition for storage and reconstruction.</p> Source code in <code>flowyml/core/pipeline.py</code> <pre><code>def to_definition(self) -&gt; dict:\n    \"\"\"Serialize pipeline to definition for storage and reconstruction.\"\"\"\n    if not self._built:\n        self.build()\n\n    return {\n        \"name\": self.name,\n        \"steps\": [\n            {\n                \"name\": step.name,\n                \"inputs\": step.inputs,\n                \"outputs\": step.outputs,\n                \"source_code\": step.source_code,\n                \"tags\": step.tags,\n            }\n            for step in self.steps\n        ],\n        \"dag\": {\n            \"nodes\": [\n                {\n                    \"name\": node.name,\n                    \"inputs\": node.inputs,\n                    \"outputs\": node.outputs,\n                }\n                for node in self.dag.nodes.values()\n            ],\n            \"edges\": [\n                {\"source\": dep, \"target\": node_name} for node_name, deps in self.dag.edges.items() for dep in deps\n            ],\n        },\n    }\n</code></pre>"},{"location":"api/pipeline/#flowyml.core.pipeline.Pipeline.visualize","title":"<code>visualize() -&gt; str</code>","text":"<p>Generate pipeline visualization.</p> Source code in <code>flowyml/core/pipeline.py</code> <pre><code>def visualize(self) -&gt; str:\n    \"\"\"Generate pipeline visualization.\"\"\"\n    if not self._built:\n        self.build()\n    return self.dag.visualize()\n</code></pre>"},{"location":"api/step/","title":"Step API \ud83d\udc63","text":"<p>Steps are the building blocks of flowyml pipelines.</p>"},{"location":"api/step/#usage","title":"Usage","text":"<pre><code>from flowyml import step\n\n@step(cache=True)\ndef my_step(data):\n    return process(data)\n</code></pre>"},{"location":"api/step/#decorator-step","title":"Decorator <code>@step</code>","text":"<p>Decorator to define a pipeline step with automatic context injection.</p> <p>Can be used as @step or @step(inputs=...)</p> <p>Parameters:</p> Name Type Description Default <code>_func</code> <code>Callable | None</code> <p>Function being decorated (when used as @step)</p> <code>None</code> <code>inputs</code> <code>list[str] | None</code> <p>List of input asset names</p> <code>None</code> <code>outputs</code> <code>list[str] | None</code> <p>List of output asset names</p> <code>None</code> <code>cache</code> <code>bool | str | Callable</code> <p>Caching strategy (\"code_hash\", \"input_hash\", callable, or False)</p> <code>'code_hash'</code> <code>retry</code> <code>int</code> <p>Number of retry attempts on failure</p> <code>0</code> <code>timeout</code> <code>int | None</code> <p>Maximum execution time in seconds</p> <code>None</code> <code>resources</code> <code>Union[dict[str, Any], ResourceRequirements, None]</code> <p>Resource requirements (ResourceRequirements object or dict for backward compat)</p> <code>None</code> <code>tags</code> <code>dict[str, str] | None</code> <p>Metadata tags for the step</p> <code>None</code> <code>name</code> <code>str | None</code> <p>Optional custom name for the step</p> <code>None</code> <code>condition</code> <code>Callable | None</code> <p>Optional callable that returns True if step should run</p> <code>None</code> <code>execution_group</code> <code>str | None</code> <p>Optional group name for executing multiple steps together</p> <code>None</code> Example <p>@step ... def simple_step(): ...     ... @step(inputs=[\"data/train\"], outputs=[\"model/trained\"]) ... def train_model(train_data): ...     ...</p> Source code in <code>flowyml/core/step.py</code> <pre><code>def step(\n    _func: Callable | None = None,\n    *,\n    inputs: list[str] | None = None,\n    outputs: list[str] | None = None,\n    cache: bool | str | Callable = \"code_hash\",\n    retry: int = 0,\n    timeout: int | None = None,\n    resources: Union[dict[str, Any], \"ResourceRequirements\", None] = None,\n    tags: dict[str, str] | None = None,\n    name: str | None = None,\n    condition: Callable | None = None,\n    execution_group: str | None = None,\n):\n    \"\"\"Decorator to define a pipeline step with automatic context injection.\n\n    Can be used as @step or @step(inputs=...)\n\n    Args:\n        _func: Function being decorated (when used as @step)\n        inputs: List of input asset names\n        outputs: List of output asset names\n        cache: Caching strategy (\"code_hash\", \"input_hash\", callable, or False)\n        retry: Number of retry attempts on failure\n        timeout: Maximum execution time in seconds\n        resources: Resource requirements (ResourceRequirements object or dict for backward compat)\n        tags: Metadata tags for the step\n        name: Optional custom name for the step\n        condition: Optional callable that returns True if step should run\n        execution_group: Optional group name for executing multiple steps together\n\n    Example:\n        &gt;&gt;&gt; @step\n        ... def simple_step():\n        ...     ...\n        &gt;&gt;&gt; @step(inputs=[\"data/train\"], outputs=[\"model/trained\"])\n        ... def train_model(train_data):\n        ...     ...\n        &gt;&gt;&gt; # With resource requirements\n        &gt;&gt;&gt; from flowyml.core.resources import ResourceRequirements, GPUConfig\n        &gt;&gt;&gt; @step(resources=ResourceRequirements(cpu=\"4\", memory=\"16Gi\", gpu=GPUConfig(gpu_type=\"nvidia-v100\", count=2)))\n        ... def gpu_train(data):\n        ...     ...\n    \"\"\"\n\n    def decorator(func: Callable) -&gt; Step:\n        return Step(\n            func=func,\n            name=name,\n            inputs=inputs,\n            outputs=outputs,\n            cache=cache,\n            retry=retry,\n            timeout=timeout,\n            resources=resources,\n            tags=tags,\n            condition=condition,\n            execution_group=execution_group,\n        )\n\n    if _func is None:\n        return decorator\n    else:\n        return decorator(_func)\n</code></pre>"},{"location":"api/step/#flowyml.core.step.step--with-resource-requirements","title":"With resource requirements","text":"<p>from flowyml.core.resources import ResourceRequirements, GPUConfig @step(resources=ResourceRequirements(cpu=\"4\", memory=\"16Gi\", gpu=GPUConfig(gpu_type=\"nvidia-v100\", count=2))) ... def gpu_train(data): ...     ...</p>"},{"location":"api/step/#class-step","title":"Class <code>Step</code>","text":"<p>A pipeline step that can be executed with automatic context injection.</p> Source code in <code>flowyml/core/step.py</code> <pre><code>def __init__(\n    self,\n    func: Callable,\n    name: str | None = None,\n    inputs: list[str] | None = None,\n    outputs: list[str] | None = None,\n    cache: bool | str | Callable = \"code_hash\",\n    retry: int = 0,\n    timeout: int | None = None,\n    resources: Union[dict[str, Any], \"ResourceRequirements\", None] = None,\n    tags: dict[str, str] | None = None,\n    condition: Callable | None = None,\n    execution_group: str | None = None,\n):\n    self.func = func\n    self.name = name or func.__name__\n    self.inputs = inputs or []\n    self.outputs = outputs or []\n    self.cache = cache\n    self.retry = retry\n    self.timeout = timeout\n\n    # Store resources (accept both dict for backward compatibility and ResourceRequirements)\n    self.resources = resources\n\n    self.tags = tags or {}\n    self.condition = condition\n    self.execution_group = execution_group\n\n    # Capture source code for UI display\n    try:\n        self.source_code = inspect.getsource(func)\n    except (OSError, TypeError):\n        self.source_code = \"# Source code not available\"\n\n    self.config = StepConfig(\n        name=self.name,\n        func=func,\n        inputs=self.inputs,\n        outputs=self.outputs,\n        cache=self.cache,\n        retry=self.retry,\n        timeout=self.timeout,\n        resources=self.resources,\n        tags=self.tags,\n        condition=self.condition,\n        execution_group=self.execution_group,\n    )\n</code></pre>"},{"location":"api/step/#flowyml.core.step.Step-functions","title":"Functions","text":""},{"location":"api/step/#flowyml.core.step.Step.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Execute the step function.</p> Source code in <code>flowyml/core/step.py</code> <pre><code>def __call__(self, *args, **kwargs):\n    \"\"\"Execute the step function.\"\"\"\n    # Check condition if present\n    if self.condition:\n        # We might need to inject context into condition too,\n        # but for now assume it takes no args or same args as step?\n        # This is tricky without context injection logic here.\n        # The executor handles execution, so maybe we just store it here.\n        pass\n\n    return self.func(*args, **kwargs)\n</code></pre>"},{"location":"api/step/#flowyml.core.step.Step.get_cache_key","title":"<code>get_cache_key(inputs: dict[str, Any] | None = None) -&gt; str</code>","text":"<p>Generate cache key based on caching strategy.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any] | None</code> <p>Input data for the step</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Cache key string</p> Source code in <code>flowyml/core/step.py</code> <pre><code>def get_cache_key(self, inputs: dict[str, Any] | None = None) -&gt; str:\n    \"\"\"Generate cache key based on caching strategy.\n\n    Args:\n        inputs: Input data for the step\n\n    Returns:\n        Cache key string\n    \"\"\"\n    if self.cache == \"code_hash\":\n        return f\"{self.name}:{self.get_code_hash()}\"\n    elif self.cache == \"input_hash\" and inputs:\n        return f\"{self.name}:{self.get_input_hash(inputs)}\"\n    elif callable(self.cache) and inputs:\n        return self.cache(inputs, {})\n    else:\n        return f\"{self.name}:no-cache\"\n</code></pre>"},{"location":"api/step/#flowyml.core.step.Step.get_code_hash","title":"<code>get_code_hash() -&gt; str</code>","text":"<p>Compute hash of the step's source code.</p> Source code in <code>flowyml/core/step.py</code> <pre><code>def get_code_hash(self) -&gt; str:\n    \"\"\"Compute hash of the step's source code.\"\"\"\n    try:\n        source = inspect.getsource(self.func)\n        return hashlib.md5(source.encode()).hexdigest()\n    except (OSError, TypeError):\n        # Fallback for dynamically defined functions or when source is unavailable\n        return hashlib.md5(self.name.encode()).hexdigest()[:16]\n</code></pre>"},{"location":"api/step/#flowyml.core.step.Step.get_input_hash","title":"<code>get_input_hash(inputs: dict[str, Any]) -&gt; str</code>","text":"<p>Generate hash of inputs for caching.</p> Source code in <code>flowyml/core/step.py</code> <pre><code>def get_input_hash(self, inputs: dict[str, Any]) -&gt; str:\n    \"\"\"Generate hash of inputs for caching.\"\"\"\n    input_str = json.dumps(inputs, sort_keys=True, default=str)\n    return hashlib.sha256(input_str.encode()).hexdigest()[:16]\n</code></pre>"},{"location":"api/storage/","title":"Storage API Reference","text":""},{"location":"api/storage/#artifactstore","title":"ArtifactStore","text":""},{"location":"api/storage/#flowyml.storage.artifacts.ArtifactStore","title":"<code>flowyml.storage.artifacts.ArtifactStore</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for artifact storage backends.</p>"},{"location":"api/storage/#flowyml.storage.artifacts.ArtifactStore-functions","title":"Functions","text":""},{"location":"api/storage/#flowyml.storage.artifacts.ArtifactStore.delete","title":"<code>delete(path: str) -&gt; None</code>  <code>abstractmethod</code>","text":"<p>Delete artifact at path.</p> Source code in <code>flowyml/storage/artifacts.py</code> <pre><code>@abstractmethod\ndef delete(self, path: str) -&gt; None:\n    \"\"\"Delete artifact at path.\"\"\"\n    pass\n</code></pre>"},{"location":"api/storage/#flowyml.storage.artifacts.ArtifactStore.exists","title":"<code>exists(path: str) -&gt; bool</code>  <code>abstractmethod</code>","text":"<p>Check if artifact exists at path.</p> Source code in <code>flowyml/storage/artifacts.py</code> <pre><code>@abstractmethod\ndef exists(self, path: str) -&gt; bool:\n    \"\"\"Check if artifact exists at path.\"\"\"\n    pass\n</code></pre>"},{"location":"api/storage/#flowyml.storage.artifacts.ArtifactStore.list_artifacts","title":"<code>list_artifacts(prefix: str = '') -&gt; list[str]</code>  <code>abstractmethod</code>","text":"<p>List all artifacts with optional prefix filter.</p> Source code in <code>flowyml/storage/artifacts.py</code> <pre><code>@abstractmethod\ndef list_artifacts(self, prefix: str = \"\") -&gt; list[str]:\n    \"\"\"List all artifacts with optional prefix filter.\"\"\"\n    pass\n</code></pre>"},{"location":"api/storage/#flowyml.storage.artifacts.ArtifactStore.load","title":"<code>load(path: str) -&gt; Any</code>  <code>abstractmethod</code>","text":"<p>Load an artifact from storage.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Storage path of the artifact</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The loaded artifact</p> Source code in <code>flowyml/storage/artifacts.py</code> <pre><code>@abstractmethod\ndef load(self, path: str) -&gt; Any:\n    \"\"\"Load an artifact from storage.\n\n    Args:\n        path: Storage path of the artifact\n\n    Returns:\n        The loaded artifact\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/storage/#flowyml.storage.artifacts.ArtifactStore.materialize","title":"<code>materialize(obj: Any, name: str, run_id: str, step_name: str, project_name: str = 'default') -&gt; str</code>","text":"<p>Materialize artifact to structured storage.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>Object to materialize</p> required <code>name</code> <code>str</code> <p>Name of the artifact</p> required <code>run_id</code> <code>str</code> <p>ID of the current run</p> required <code>step_name</code> <code>str</code> <p>Name of the step producing the artifact</p> required <code>project_name</code> <code>str</code> <p>Name of the project</p> <code>'default'</code> <p>Returns:</p> Type Description <code>str</code> <p>Path where artifact was saved</p> Source code in <code>flowyml/storage/artifacts.py</code> <pre><code>def materialize(self, obj: Any, name: str, run_id: str, step_name: str, project_name: str = \"default\") -&gt; str:\n    \"\"\"Materialize artifact to structured storage.\n\n    Args:\n        obj: Object to materialize\n        name: Name of the artifact\n        run_id: ID of the current run\n        step_name: Name of the step producing the artifact\n        project_name: Name of the project\n\n    Returns:\n        Path where artifact was saved\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/storage/#flowyml.storage.artifacts.ArtifactStore.save","title":"<code>save(artifact: Any, path: str, metadata: dict | None = None) -&gt; str</code>  <code>abstractmethod</code>","text":"<p>Save an artifact to storage.</p> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>Any</code> <p>The artifact to save</p> required <code>path</code> <code>str</code> <p>Storage path for the artifact</p> required <code>metadata</code> <code>dict | None</code> <p>Optional metadata dictionary</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Full path where artifact was saved</p> Source code in <code>flowyml/storage/artifacts.py</code> <pre><code>@abstractmethod\ndef save(self, artifact: Any, path: str, metadata: dict | None = None) -&gt; str:\n    \"\"\"Save an artifact to storage.\n\n    Args:\n        artifact: The artifact to save\n        path: Storage path for the artifact\n        metadata: Optional metadata dictionary\n\n    Returns:\n        Full path where artifact was saved\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/storage/#metadatastore","title":"MetadataStore","text":""},{"location":"api/storage/#flowyml.storage.metadata.MetadataStore","title":"<code>flowyml.storage.metadata.MetadataStore</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for metadata storage backends.</p>"},{"location":"api/storage/#flowyml.storage.metadata.MetadataStore-functions","title":"Functions","text":""},{"location":"api/storage/#flowyml.storage.metadata.MetadataStore.list_assets","title":"<code>list_assets(limit: int | None = None, **filters) -&gt; list[dict]</code>  <code>abstractmethod</code>","text":"<p>List assets with optional filters.</p> Source code in <code>flowyml/storage/metadata.py</code> <pre><code>@abstractmethod\ndef list_assets(self, limit: int | None = None, **filters) -&gt; list[dict]:\n    \"\"\"List assets with optional filters.\"\"\"\n    pass\n</code></pre>"},{"location":"api/storage/#flowyml.storage.metadata.MetadataStore.list_pipelines","title":"<code>list_pipelines() -&gt; list[str]</code>  <code>abstractmethod</code>","text":"<p>List all unique pipeline names.</p> Source code in <code>flowyml/storage/metadata.py</code> <pre><code>@abstractmethod\ndef list_pipelines(self) -&gt; list[str]:\n    \"\"\"List all unique pipeline names.\"\"\"\n    pass\n</code></pre>"},{"location":"api/storage/#flowyml.storage.metadata.MetadataStore.list_runs","title":"<code>list_runs(limit: int | None = None) -&gt; list[dict]</code>  <code>abstractmethod</code>","text":"<p>List all runs.</p> Source code in <code>flowyml/storage/metadata.py</code> <pre><code>@abstractmethod\ndef list_runs(self, limit: int | None = None) -&gt; list[dict]:\n    \"\"\"List all runs.\"\"\"\n    pass\n</code></pre>"},{"location":"api/storage/#flowyml.storage.metadata.MetadataStore.load_artifact","title":"<code>load_artifact(artifact_id: str) -&gt; dict | None</code>  <code>abstractmethod</code>","text":"<p>Load artifact metadata.</p> Source code in <code>flowyml/storage/metadata.py</code> <pre><code>@abstractmethod\ndef load_artifact(self, artifact_id: str) -&gt; dict | None:\n    \"\"\"Load artifact metadata.\"\"\"\n    pass\n</code></pre>"},{"location":"api/storage/#flowyml.storage.metadata.MetadataStore.load_run","title":"<code>load_run(run_id: str) -&gt; dict | None</code>  <code>abstractmethod</code>","text":"<p>Load run metadata.</p> Source code in <code>flowyml/storage/metadata.py</code> <pre><code>@abstractmethod\ndef load_run(self, run_id: str) -&gt; dict | None:\n    \"\"\"Load run metadata.\"\"\"\n    pass\n</code></pre>"},{"location":"api/storage/#flowyml.storage.metadata.MetadataStore.query","title":"<code>query(**filters) -&gt; list[dict]</code>  <code>abstractmethod</code>","text":"<p>Query runs with filters.</p> Source code in <code>flowyml/storage/metadata.py</code> <pre><code>@abstractmethod\ndef query(self, **filters) -&gt; list[dict]:\n    \"\"\"Query runs with filters.\"\"\"\n    pass\n</code></pre>"},{"location":"api/storage/#flowyml.storage.metadata.MetadataStore.save_artifact","title":"<code>save_artifact(artifact_id: str, metadata: dict) -&gt; None</code>  <code>abstractmethod</code>","text":"<p>Save artifact metadata.</p> Source code in <code>flowyml/storage/metadata.py</code> <pre><code>@abstractmethod\ndef save_artifact(self, artifact_id: str, metadata: dict) -&gt; None:\n    \"\"\"Save artifact metadata.\"\"\"\n    pass\n</code></pre>"},{"location":"api/storage/#flowyml.storage.metadata.MetadataStore.save_run","title":"<code>save_run(run_id: str, metadata: dict) -&gt; None</code>  <code>abstractmethod</code>","text":"<p>Save run metadata.</p> Source code in <code>flowyml/storage/metadata.py</code> <pre><code>@abstractmethod\ndef save_run(self, run_id: str, metadata: dict) -&gt; None:\n    \"\"\"Save run metadata.\"\"\"\n    pass\n</code></pre>"},{"location":"api/types/","title":"Types API \ud83d\udcdd","text":"<p>Type definitions used in flowyml.</p>"},{"location":"api/types/#resource-requirements","title":"Resource Requirements","text":"<p>Resource requirements for a pipeline step.</p> <p>Orchestrator-agnostic resource specification that can be translated to platform-specific formats (Kubernetes, Vertex AI, SageMaker, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>cpu</code> <code>Optional[str]</code> <p>CPU cores (e.g., \"2\", \"500m\", \"2.5\")</p> <code>None</code> <code>memory</code> <code>Optional[str]</code> <p>RAM amount (e.g., \"4Gi\", \"8192Mi\", \"16G\")</p> <code>None</code> <code>storage</code> <code>Optional[str]</code> <p>Ephemeral storage (e.g., \"100Gi\", \"50G\")</p> <code>None</code> <code>gpu</code> <code>Optional[GPUConfig]</code> <p>GPU configuration</p> <code>None</code> <code>node_affinity</code> <code>Optional[NodeAffinity]</code> <p>Node selection rules</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Simple CPU/memory\n&gt;&gt;&gt; resources = ResourceRequirements(cpu=\"2\", memory=\"4Gi\")\n</code></pre> <pre><code>&gt;&gt;&gt; # With GPU\n&gt;&gt;&gt; resources = ResourceRequirements(cpu=\"4\", memory=\"16Gi\", gpu=GPUConfig(gpu_type=\"nvidia-tesla-v100\", count=2))\n</code></pre> <pre><code>&gt;&gt;&gt; # With node affinity\n&gt;&gt;&gt; resources = ResourceRequirements(\n...     cpu=\"8\",\n...     memory=\"32Gi\",\n...     node_affinity=NodeAffinity(\n...         required={\"gpu\": \"true\"}, tolerations=[{\"key\": \"nvidia.com/gpu\", \"operator\": \"Exists\"}]\n...     ),\n... )\n</code></pre>"},{"location":"api/types/#flowyml.core.resources.ResourceRequirements-functions","title":"Functions","text":""},{"location":"api/types/#flowyml.core.resources.ResourceRequirements.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate resource specifications.</p> Source code in <code>flowyml/core/resources.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate resource specifications.\"\"\"\n    if self.cpu and not self._is_valid_cpu(self.cpu):\n        msg = f\"Invalid CPU format: {self.cpu}\"\n        raise ValueError(msg)\n    if self.memory and not self._is_valid_memory(self.memory):\n        msg = f\"Invalid memory format: {self.memory}\"\n        raise ValueError(msg)\n    if self.storage and not self._is_valid_memory(self.storage):\n        msg = f\"Invalid storage format: {self.storage}\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"api/types/#flowyml.core.resources.ResourceRequirements.get_gpu_count","title":"<code>get_gpu_count() -&gt; int</code>","text":"<p>Get total number of GPUs requested.</p> Source code in <code>flowyml/core/resources.py</code> <pre><code>def get_gpu_count(self) -&gt; int:\n    \"\"\"Get total number of GPUs requested.\"\"\"\n    return self.gpu.count if self.gpu else 0\n</code></pre>"},{"location":"api/types/#flowyml.core.resources.ResourceRequirements.has_gpu","title":"<code>has_gpu() -&gt; bool</code>","text":"<p>Check if GPU resources are requested.</p> Source code in <code>flowyml/core/resources.py</code> <pre><code>def has_gpu(self) -&gt; bool:\n    \"\"\"Check if GPU resources are requested.\"\"\"\n    return self.gpu is not None\n</code></pre>"},{"location":"api/types/#flowyml.core.resources.ResourceRequirements.merge_with","title":"<code>merge_with(other: ResourceRequirements) -&gt; ResourceRequirements</code>","text":"<p>Merge with another ResourceRequirements, taking maximum of each.</p> <p>This is used when grouping steps to aggregate their resource needs. Strategy: - CPU: Take maximum - Memory: Take maximum - Storage: Take maximum - GPU: Merge configs (max count, best type) - Node affinity: Merge constraints</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>ResourceRequirements</code> <p>Another ResourceRequirements to merge with</p> required <p>Returns:</p> Type Description <code>ResourceRequirements</code> <p>New ResourceRequirements with merged specifications</p> Source code in <code>flowyml/core/resources.py</code> <pre><code>def merge_with(self, other: \"ResourceRequirements\") -&gt; \"ResourceRequirements\":\n    \"\"\"Merge with another ResourceRequirements, taking maximum of each.\n\n    This is used when grouping steps to aggregate their resource needs.\n    Strategy:\n    - CPU: Take maximum\n    - Memory: Take maximum\n    - Storage: Take maximum\n    - GPU: Merge configs (max count, best type)\n    - Node affinity: Merge constraints\n\n    Args:\n        other: Another ResourceRequirements to merge with\n\n    Returns:\n        New ResourceRequirements with merged specifications\n    \"\"\"\n    # Merge CPU\n    merged_cpu = None\n    if self.cpu and other.cpu:\n        merged_cpu = self._compare_cpu(self.cpu, other.cpu)\n    elif self.cpu:\n        merged_cpu = self.cpu\n    elif other.cpu:\n        merged_cpu = other.cpu\n\n    # Merge memory\n    merged_memory = None\n    if self.memory and other.memory:\n        merged_memory = self._compare_memory(self.memory, other.memory)\n    elif self.memory:\n        merged_memory = self.memory\n    elif other.memory:\n        merged_memory = other.memory\n\n    # Merge storage\n    merged_storage = None\n    if self.storage and other.storage:\n        merged_storage = self._compare_memory(self.storage, other.storage)\n    elif self.storage:\n        merged_storage = self.storage\n    elif other.storage:\n        merged_storage = other.storage\n\n    # Merge GPU\n    merged_gpu = None\n    if self.gpu and other.gpu:\n        merged_gpu = self.gpu.merge_with(other.gpu)\n    elif self.gpu:\n        merged_gpu = self.gpu\n    elif other.gpu:\n        merged_gpu = other.gpu\n\n    # Merge node affinity\n    merged_affinity = None\n    if self.node_affinity and other.node_affinity:\n        merged_affinity = self.node_affinity.merge_with(other.node_affinity)\n    elif self.node_affinity:\n        merged_affinity = self.node_affinity\n    elif other.node_affinity:\n        merged_affinity = other.node_affinity\n\n    return ResourceRequirements(\n        cpu=merged_cpu,\n        memory=merged_memory,\n        storage=merged_storage,\n        gpu=merged_gpu,\n        node_affinity=merged_affinity,\n    )\n</code></pre>"},{"location":"api/types/#flowyml.core.resources.ResourceRequirements.to_dict","title":"<code>to_dict() -&gt; dict[str, Any]</code>","text":"<p>Convert to dictionary representation.</p> Source code in <code>flowyml/core/resources.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    result = {}\n    if self.cpu:\n        result[\"cpu\"] = self.cpu\n    if self.memory:\n        result[\"memory\"] = self.memory\n    if self.storage:\n        result[\"storage\"] = self.storage\n    if self.gpu:\n        result[\"gpu\"] = self.gpu.to_dict()\n    if self.node_affinity:\n        result[\"node_affinity\"] = self.node_affinity.to_dict()\n    return result\n</code></pre>"},{"location":"api/types/#scheduler-config","title":"Scheduler Config","text":"<p>               Bases: <code>BaseModel</code></p> <p>Scheduler configuration.</p>"},{"location":"api/types/#flowyml.core.scheduler_config.SchedulerConfig-functions","title":"Functions","text":""},{"location":"api/types/#flowyml.core.scheduler_config.SchedulerConfig.from_env","title":"<code>from_env() -&gt; SchedulerConfig</code>  <code>classmethod</code>","text":"<p>Load from environment variables.</p> Source code in <code>flowyml/core/scheduler_config.py</code> <pre><code>@classmethod\ndef from_env(cls) -&gt; \"SchedulerConfig\":\n    \"\"\"Load from environment variables.\"\"\"\n    return cls(\n        persist_schedules=os.getenv(\"FLOWYML_SCHEDULER_PERSIST\", \"true\").lower() == \"true\",\n        db_path=os.getenv(\"FLOWYML_SCHEDULER_DB_PATH\"),\n        distributed=os.getenv(\"FLOWYML_SCHEDULER_DISTRIBUTED\", \"false\").lower() == \"true\",\n        lock_backend=os.getenv(\"FLOWYML_SCHEDULER_LOCK_BACKEND\", \"file\"),\n        redis_url=os.getenv(\"FLOWYML_SCHEDULER_REDIS_URL\"),\n        check_interval_seconds=int(os.getenv(\"FLOWYML_SCHEDULER_CHECK_INTERVAL\", \"10\")),\n        max_concurrent_runs=int(os.getenv(\"FLOWYML_SCHEDULER_MAX_CONCURRENT\", \"5\")),\n        timezone=os.getenv(\"FLOWYML_SCHEDULER_TIMEZONE\", \"UTC\"),\n    )\n</code></pre>"},{"location":"api/utils/","title":"Utils API \ud83d\udee0\ufe0f","text":"<p>Helper functions.</p>"},{"location":"api/utils/#debug-utilities","title":"Debug Utilities","text":"<p>Pipeline and step debugging tools.</p>"},{"location":"api/utils/#flowyml.utils.debug-classes","title":"Classes","text":""},{"location":"api/utils/#flowyml.utils.debug.PipelineDebugger","title":"<code>PipelineDebugger(pipeline)</code>","text":"<p>Debug entire pipelines.</p> <p>Features: - Step-by-step execution - DAG visualization - Execution replay - Error analysis</p> Source code in <code>flowyml/utils/debug.py</code> <pre><code>def __init__(self, pipeline):\n    self.pipeline = pipeline\n    self.execution_log = []\n</code></pre>"},{"location":"api/utils/#flowyml.utils.debug.PipelineDebugger-functions","title":"Functions","text":""},{"location":"api/utils/#flowyml.utils.debug.PipelineDebugger.analyze_errors","title":"<code>analyze_errors(run_id: str) -&gt; None</code>","text":"<p>Analyze errors from a failed run.</p> Source code in <code>flowyml/utils/debug.py</code> <pre><code>def analyze_errors(self, run_id: str) -&gt; None:\n    \"\"\"Analyze errors from a failed run.\"\"\"\n    # Load run metadata\n    metadata = self.pipeline.metadata_store.load_run(run_id)\n\n    if not metadata:\n        return\n\n    steps_metadata = metadata.get(\"steps\", {})\n\n    failed_steps = []\n    for step_name, step_data in steps_metadata.items():\n        if not step_data.get(\"success\", True):\n            failed_steps.append((step_name, step_data))\n\n    if not failed_steps:\n        return\n\n    for _, step_data in failed_steps:\n        if step_data.get(\"source_code\"):\n            for _ in step_data[\"source_code\"].split(\"\\n\")[:10]:\n                pass\n</code></pre>"},{"location":"api/utils/#flowyml.utils.debug.PipelineDebugger.replay_run","title":"<code>replay_run(run_id: str, start_from: str | None = None) -&gt; None</code>","text":"<p>Replay a previous run, optionally starting from a specific step.</p> Source code in <code>flowyml/utils/debug.py</code> <pre><code>def replay_run(self, run_id: str, start_from: str | None = None) -&gt; None:\n    \"\"\"Replay a previous run, optionally starting from a specific step.\"\"\"\n    if start_from:\n        pass\n    _ = run_id  # Unused in placeholder\n</code></pre>"},{"location":"api/utils/#flowyml.utils.debug.PipelineDebugger.step_through","title":"<code>step_through() -&gt; None</code>","text":"<p>Execute pipeline step-by-step with breaks.</p> Source code in <code>flowyml/utils/debug.py</code> <pre><code>def step_through(self) -&gt; None:\n    \"\"\"Execute pipeline step-by-step with breaks.\"\"\"\n    self.pipeline.build()\n    order = self.pipeline.dag.topological_sort()\n\n    for _ in order:\n        response = input(\"\\nExecute this step? [Y/n/q]: \").lower()\n\n        if response == \"q\":\n            break\n        if response == \"n\":\n            continue\n</code></pre>"},{"location":"api/utils/#flowyml.utils.debug.PipelineDebugger.visualize_dag","title":"<code>visualize_dag() -&gt; None</code>","text":"<p>Visualize the pipeline DAG.</p> Source code in <code>flowyml/utils/debug.py</code> <pre><code>def visualize_dag(self) -&gt; None:\n    \"\"\"Visualize the pipeline DAG.\"\"\"\n    self.pipeline.build()\n</code></pre>"},{"location":"api/utils/#flowyml.utils.debug.StepDebugger","title":"<code>StepDebugger()</code>","text":"<p>Debug individual pipeline steps.</p> <p>Features: - Breakpoints - Input/output inspection - Exception debugging - Step profiling</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flowyml import step, StepDebugger\n&gt;&gt;&gt; debugger = StepDebugger()\n&gt;&gt;&gt; @step(outputs=[\"processed\"])\n... @debugger.breakpoint()\n... def process_data(data):\n...     # Debugger will stop here\n...     return data * 2\n</code></pre> Source code in <code>flowyml/utils/debug.py</code> <pre><code>def __init__(self):\n    self.breakpoints = set()\n    self.step_history = []\n    self.enabled = True\n</code></pre>"},{"location":"api/utils/#flowyml.utils.debug.StepDebugger-functions","title":"Functions","text":""},{"location":"api/utils/#flowyml.utils.debug.StepDebugger.break_at","title":"<code>break_at(condition: Callable | None = None)</code>","text":"<p>Add a breakpoint to a step.</p> <p>Parameters:</p> Name Type Description Default <code>condition</code> <code>Callable | None</code> <p>Optional condition function. Break only if returns True.</p> <code>None</code> Source code in <code>flowyml/utils/debug.py</code> <pre><code>def break_at(self, condition: Callable | None = None):\n    \"\"\"Add a breakpoint to a step.\n\n    Args:\n        condition: Optional condition function. Break only if returns True.\n    \"\"\"\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            if not self.enabled:\n                return func(*args, **kwargs)\n\n            # Check condition\n            should_break = True\n            if condition:\n                should_break = condition(*args, **kwargs)\n\n            if should_break:\n                while True:\n                    cmd = input(\"\\n(debug) \").strip()\n\n                    if cmd == \"c\":\n                        break\n                    if cmd == \"i\":\n                        pass\n                    elif cmd.startswith(\"p \"):\n                        expr = cmd[2:]\n                        with contextlib.suppress(Exception):\n                            # Evaluate in context\n                            result = eval(expr, {\"args\": args, \"kwargs\": kwargs})\n                    elif cmd == \"pdb\":\n                        import pdb  # noqa: T100\n\n                        pdb.set_trace()\n                        break\n\n            # Execute function\n            try:\n                result = func(*args, **kwargs)\n\n                # Log execution\n                self.step_history.append(\n                    {\n                        \"step\": func.__name__,\n                        \"inputs\": {\"args\": args, \"kwargs\": kwargs},\n                        \"output\": result,\n                        \"success\": True,\n                    },\n                )\n\n                return result\n            except Exception as e:\n                # Log error\n                self.step_history.append(\n                    {\n                        \"step\": func.__name__,\n                        \"inputs\": {\"args\": args, \"kwargs\": kwargs},\n                        \"error\": str(e),\n                        \"success\": False,\n                    },\n                )\n                raise\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"api/utils/#flowyml.utils.debug.StepDebugger.clear_history","title":"<code>clear_history() -&gt; None</code>","text":"<p>Clear execution history.</p> Source code in <code>flowyml/utils/debug.py</code> <pre><code>def clear_history(self) -&gt; None:\n    \"\"\"Clear execution history.\"\"\"\n    self.step_history = []\n</code></pre>"},{"location":"api/utils/#flowyml.utils.debug.StepDebugger.get_history","title":"<code>get_history()</code>","text":"<p>Get step execution history.</p> Source code in <code>flowyml/utils/debug.py</code> <pre><code>def get_history(self):\n    \"\"\"Get step execution history.\"\"\"\n    return self.step_history\n</code></pre>"},{"location":"api/utils/#flowyml.utils.debug.StepDebugger.profile","title":"<code>profile()</code>","text":"<p>Profile step execution time.</p> Source code in <code>flowyml/utils/debug.py</code> <pre><code>def profile(self):\n    \"\"\"Profile step execution time.\"\"\"\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            import time\n\n            start = time.time()\n            result = func(*args, **kwargs)\n            time.time() - start\n\n            return result\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"api/utils/#flowyml.utils.debug.StepDebugger.trace","title":"<code>trace()</code>","text":"<p>Enable step tracing (print inputs/outputs).</p> Source code in <code>flowyml/utils/debug.py</code> <pre><code>def trace(self):\n    \"\"\"Enable step tracing (print inputs/outputs).\"\"\"\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            if not self.enabled:\n                return func(*args, **kwargs)\n\n            result = func(*args, **kwargs)\n\n            return result\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"api/utils/#flowyml.utils.debug-functions","title":"Functions","text":""},{"location":"api/utils/#flowyml.utils.debug.debug_step","title":"<code>debug_step(*args, **kwargs)</code>","text":"<p>Convenience function to debug a step.</p> Source code in <code>flowyml/utils/debug.py</code> <pre><code>def debug_step(*args, **kwargs):\n    \"\"\"Convenience function to debug a step.\"\"\"\n    return _global_debugger.break_at(*args, **kwargs)\n</code></pre>"},{"location":"api/utils/#flowyml.utils.debug.inspect_step","title":"<code>inspect_step(step) -&gt; None</code>","text":"<p>Inspect a step's metadata.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <p>Step to inspect</p> required Source code in <code>flowyml/utils/debug.py</code> <pre><code>def inspect_step(step) -&gt; None:\n    \"\"\"Inspect a step's metadata.\n\n    Args:\n        step: Step to inspect\n    \"\"\"\n    if step.source_code:\n        pass\n</code></pre>"},{"location":"api/utils/#flowyml.utils.debug.print_dag","title":"<code>print_dag(pipeline) -&gt; None</code>","text":"<p>Pretty print pipeline DAG.</p> Source code in <code>flowyml/utils/debug.py</code> <pre><code>def print_dag(pipeline) -&gt; None:\n    \"\"\"Pretty print pipeline DAG.\"\"\"\n    pipeline.build()\n</code></pre>"},{"location":"api/utils/#flowyml.utils.debug.profile_step","title":"<code>profile_step()</code>","text":"<p>Convenience function to profile a step.</p> Source code in <code>flowyml/utils/debug.py</code> <pre><code>def profile_step():\n    \"\"\"Convenience function to profile a step.\"\"\"\n    return _global_debugger.profile()\n</code></pre>"},{"location":"api/utils/#flowyml.utils.debug.trace_step","title":"<code>trace_step()</code>","text":"<p>Convenience function to trace a step.</p> Source code in <code>flowyml/utils/debug.py</code> <pre><code>def trace_step():\n    \"\"\"Convenience function to trace a step.\"\"\"\n    return _global_debugger.trace()\n</code></pre>"},{"location":"api/utils/#validation-utilities","title":"Validation Utilities","text":"<p>Pydantic schemas and validation utilities for flowyml.</p>"},{"location":"api/utils/#flowyml.utils.validation-classes","title":"Classes","text":""},{"location":"api/utils/#flowyml.utils.validation.CacheStrategy","title":"<code>CacheStrategy</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Cache strategy options.</p>"},{"location":"api/utils/#flowyml.utils.validation.ContextConfig","title":"<code>ContextConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for pipeline context.</p>"},{"location":"api/utils/#flowyml.utils.validation.ContextConfig-functions","title":"Functions","text":""},{"location":"api/utils/#flowyml.utils.validation.ContextConfig.validate_device","title":"<code>validate_device(v: str | None) -&gt; str | None</code>  <code>classmethod</code>","text":"<p>Validate device string.</p> Source code in <code>flowyml/utils/validation.py</code> <pre><code>@field_validator(\"device\")\n@classmethod\ndef validate_device(cls, v: str | None) -&gt; str | None:\n    \"\"\"Validate device string.\"\"\"\n    if v is None:\n        return v\n\n    valid_devices = [\"cpu\", \"cuda\", \"mps\", \"tpu\"]\n    if v.lower() not in valid_devices and not v.startswith(\"cuda:\"):\n        raise ValueError(f\"Invalid device. Must be one of: {', '.join(valid_devices)} or 'cuda:N'\")\n\n    return v.lower()\n</code></pre>"},{"location":"api/utils/#flowyml.utils.validation.DatasetSchema","title":"<code>DatasetSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for dataset validation.</p>"},{"location":"api/utils/#flowyml.utils.validation.ExperimentConfig","title":"<code>ExperimentConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for experiments.</p>"},{"location":"api/utils/#flowyml.utils.validation.MetricsSchema","title":"<code>MetricsSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for metrics validation.</p>"},{"location":"api/utils/#flowyml.utils.validation.ModelSchema","title":"<code>ModelSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for model validation.</p>"},{"location":"api/utils/#flowyml.utils.validation.PipelineConfig","title":"<code>PipelineConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for pipelines.</p>"},{"location":"api/utils/#flowyml.utils.validation.ResourceRequirements","title":"<code>ResourceRequirements</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Resource requirements for step execution.</p>"},{"location":"api/utils/#flowyml.utils.validation.ResourceRequirements-functions","title":"Functions","text":""},{"location":"api/utils/#flowyml.utils.validation.ResourceRequirements.validate_size_format","title":"<code>validate_size_format(v: str | None) -&gt; str | None</code>  <code>classmethod</code>","text":"<p>Validate memory/disk size format.</p> Source code in <code>flowyml/utils/validation.py</code> <pre><code>@field_validator(\"memory\", \"disk\")\n@classmethod\ndef validate_size_format(cls, v: str | None) -&gt; str | None:\n    \"\"\"Validate memory/disk size format.\"\"\"\n    if v is None:\n        return v\n\n    valid_units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]\n    v_upper = v.upper()\n\n    for unit in valid_units:\n        if v_upper.endswith(unit):\n            try:\n                size_val = v_upper[: -len(unit)]\n                float(size_val)\n                return v\n            except ValueError:\n                raise ValueError(f\"Invalid size format: {v}. Expected format: &lt;number&gt;&lt;unit&gt; (e.g., '4GB')\")\n\n    raise ValueError(f\"Invalid size unit. Must be one of: {', '.join(valid_units)}\")\n</code></pre>"},{"location":"api/utils/#flowyml.utils.validation.RetryConfig","title":"<code>RetryConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Retry configuration for step execution.</p>"},{"location":"api/utils/#flowyml.utils.validation.RetryConfig-functions","title":"Functions","text":""},{"location":"api/utils/#flowyml.utils.validation.RetryConfig.max_delay_greater_than_initial","title":"<code>max_delay_greater_than_initial(v: float, info) -&gt; float</code>  <code>classmethod</code>","text":"<p>Validate max_delay is greater than initial_delay.</p> Source code in <code>flowyml/utils/validation.py</code> <pre><code>@field_validator(\"max_delay\")\n@classmethod\ndef max_delay_greater_than_initial(cls, v: float, info) -&gt; float:\n    \"\"\"Validate max_delay is greater than initial_delay.\"\"\"\n    if \"initial_delay\" in info.data and v &lt; info.data[\"initial_delay\"]:\n        raise ValueError(\"max_delay must be greater than or equal to initial_delay\")\n    return v\n</code></pre>"},{"location":"api/utils/#flowyml.utils.validation.StackConfig","title":"<code>StackConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for execution stacks.</p>"},{"location":"api/utils/#flowyml.utils.validation.StepConfig","title":"<code>StepConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for pipeline steps.</p>"},{"location":"api/utils/#flowyml.utils.validation-functions","title":"Functions","text":""},{"location":"api/utils/#flowyml.utils.validation.validate_context_config","title":"<code>validate_context_config(config: dict[str, Any]) -&gt; ContextConfig</code>","text":"<p>Validate context configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>Context configuration dictionary</p> required <p>Returns:</p> Type Description <code>ContextConfig</code> <p>Validated ContextConfig instance</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If validation fails</p> Source code in <code>flowyml/utils/validation.py</code> <pre><code>def validate_context_config(config: dict[str, Any]) -&gt; ContextConfig:\n    \"\"\"Validate context configuration.\n\n    Args:\n        config: Context configuration dictionary\n\n    Returns:\n        Validated ContextConfig instance\n\n    Raises:\n        ValidationError: If validation fails\n    \"\"\"\n    return ContextConfig(**config)\n</code></pre>"},{"location":"api/utils/#flowyml.utils.validation.validate_metrics","title":"<code>validate_metrics(metrics: dict[str, Any]) -&gt; MetricsSchema</code>","text":"<p>Validate metrics.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>dict[str, Any]</code> <p>Metrics dictionary</p> required <p>Returns:</p> Type Description <code>MetricsSchema</code> <p>Validated MetricsSchema instance</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If validation fails</p> Source code in <code>flowyml/utils/validation.py</code> <pre><code>def validate_metrics(metrics: dict[str, Any]) -&gt; MetricsSchema:\n    \"\"\"Validate metrics.\n\n    Args:\n        metrics: Metrics dictionary\n\n    Returns:\n        Validated MetricsSchema instance\n\n    Raises:\n        ValidationError: If validation fails\n    \"\"\"\n    return MetricsSchema(**metrics)\n</code></pre>"},{"location":"api/utils/#flowyml.utils.validation.validate_pipeline_config","title":"<code>validate_pipeline_config(config: dict[str, Any]) -&gt; PipelineConfig</code>","text":"<p>Validate pipeline configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>Pipeline configuration dictionary</p> required <p>Returns:</p> Type Description <code>PipelineConfig</code> <p>Validated PipelineConfig instance</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If validation fails</p> Source code in <code>flowyml/utils/validation.py</code> <pre><code>def validate_pipeline_config(config: dict[str, Any]) -&gt; PipelineConfig:\n    \"\"\"Validate pipeline configuration.\n\n    Args:\n        config: Pipeline configuration dictionary\n\n    Returns:\n        Validated PipelineConfig instance\n\n    Raises:\n        ValidationError: If validation fails\n    \"\"\"\n    return PipelineConfig(**config)\n</code></pre>"},{"location":"api/utils/#flowyml.utils.validation.validate_step_config","title":"<code>validate_step_config(config: dict[str, Any]) -&gt; StepConfig</code>","text":"<p>Validate step configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>Step configuration dictionary</p> required <p>Returns:</p> Type Description <code>StepConfig</code> <p>Validated StepConfig instance</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If validation fails</p> Source code in <code>flowyml/utils/validation.py</code> <pre><code>def validate_step_config(config: dict[str, Any]) -&gt; StepConfig:\n    \"\"\"Validate step configuration.\n\n    Args:\n        config: Step configuration dictionary\n\n    Returns:\n        Validated StepConfig instance\n\n    Raises:\n        ValidationError: If validation fails\n    \"\"\"\n    return StepConfig(**config)\n</code></pre>"},{"location":"api/visualization/","title":"Visualization API \ud83d\udcca","text":"<p>Tools for visualizing pipelines and artifacts.</p>"},{"location":"api/visualization/#flowymlvisualize","title":"<code>flowyml.visualize</code>","text":""},{"location":"api/visualization/#showpipeline","title":"<code>show(pipeline)</code>","text":"<p>Display the DAG of a pipeline in a Jupyter notebook.</p> <pre><code>from flowyml.visualize import show\nshow(pipeline)\n</code></pre>"},{"location":"api/visualization/#compare_runsruns","title":"<code>compare_runs(runs)</code>","text":"<p>Display a comparison table of multiple runs.</p> <pre><code>from flowyml.visualize import compare_runs\ncompare_runs([run1, run2])\n</code></pre>"},{"location":"architecture/stacks/","title":"\ud83c\udfd7\ufe0f Stack Architecture Guide","text":""},{"location":"architecture/stacks/#overview","title":"Overview","text":"<p>flowyml's stack system provides a flexible, modular architecture for running pipelines across different infrastructure environments. Similar to ZenML, stacks are composable collections of components that define where and how your pipelines execute.</p>"},{"location":"architecture/stacks/#core-concepts","title":"Core Concepts","text":""},{"location":"architecture/stacks/#stack-components","title":"Stack Components","text":"<p>A Stack is composed of several components:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           flowyml Stack             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 \u25b8 Orchestrator (optional)           \u2502\n\u2502   - Vertex AI, Kubeflow, Airflow   \u2502\n\u2502                                     \u2502\n\u2502 \u25b8 Executor                          \u2502\n\u2502   - Local, Remote, Kubernetes      \u2502\n\u2502                                     \u2502\n\u2502 \u25b8 Artifact Store                    \u2502\n\u2502   - Local FS, GCS, S3, Azure       \u2502\n\u2502                                     \u2502\n\u2502 \u25b8 Metadata Store                    \u2502\n\u2502   - SQLite, PostgreSQL, MySQL      \u2502\n\u2502                                     \u2502\n\u2502 \u25b8 Container Registry (optional)     \u2502\n\u2502   - GCR, ECR, Docker Hub           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/stacks/#component-types","title":"Component Types","text":""},{"location":"architecture/stacks/#1-orchestrator","title":"1. Orchestrator","text":"<p>Manages pipeline workflow execution and scheduling.</p> <ul> <li>Vertex AI: Google Cloud's managed ML platform</li> <li>Kubeflow: Kubernetes-native ML workflows</li> <li>Airflow: Workflow scheduling and monitoring</li> <li>None: Direct execution (development)</li> </ul>"},{"location":"architecture/stacks/#2-executor","title":"2. Executor","text":"<p>Runs individual pipeline steps.</p> <ul> <li>LocalExecutor: Runs steps in current process</li> <li>RemoteExecutor: Submits to remote compute</li> <li>KubernetesExecutor: Runs in K8s pods</li> <li>VertexAIExecutor: Runs on Vertex AI</li> </ul>"},{"location":"architecture/stacks/#3-artifact-store","title":"3. Artifact Store","text":"<p>Stores pipeline artifacts and outputs.</p> <ul> <li>LocalArtifactStore: Local filesystem</li> <li>GCSArtifactStore: Google Cloud Storage</li> <li>S3ArtifactStore: Amazon S3</li> <li>AzureBlobStore: Azure Blob Storage</li> </ul>"},{"location":"architecture/stacks/#4-metadata-store","title":"4. Metadata Store","text":"<p>Tracks pipeline runs, metrics, and lineage.</p> <ul> <li>SQLiteMetadataStore: Local SQLite database</li> <li>PostgreSQLMetadataStore: PostgreSQL database</li> <li>CloudSQLMetadataStore: Google Cloud SQL</li> </ul>"},{"location":"architecture/stacks/#5-container-registry","title":"5. Container Registry","text":"<p>Manages Docker images for containerized execution.</p> <ul> <li>GCRContainerRegistry: Google Container Registry</li> <li>ECRContainerRegistry: AWS Elastic Container Registry</li> <li>DockerHubRegistry: Docker Hub</li> </ul>"},{"location":"architecture/stacks/#stack-types","title":"Stack Types","text":""},{"location":"architecture/stacks/#local-stack","title":"Local Stack","text":"<p>For development and testing:</p> <pre><code>from flowyml.stacks import LocalStack\n\nstack = LocalStack(\n    name=\"local\",\n    artifact_path=\".flowyml/artifacts\",\n    metadata_path=\".flowyml/metadata.db\"\n)\n</code></pre> <p>Use Cases: - Local development - Unit testing - Prototyping - Small datasets</p>"},{"location":"architecture/stacks/#gcp-stack","title":"GCP Stack","text":"<p>For production on Google Cloud Platform:</p> <pre><code>from flowyml.stacks.gcp import GCPStack\n\nstack = GCPStack(\n    name=\"production\",\n    project_id=\"my-gcp-project\",\n    region=\"us-central1\",\n    bucket_name=\"my-artifacts\",\n    registry_uri=\"gcr.io/my-project\"\n)\n</code></pre> <p>Use Cases: - Production ML training - Large-scale data processing - Team collaboration - CI/CD pipelines</p>"},{"location":"architecture/stacks/#aws-stack-coming-soon","title":"AWS Stack (Coming Soon)","text":"<pre><code>from flowyml.stacks.aws import AWSStack\n\nstack = AWSStack(\n    name=\"aws-prod\",\n    region=\"us-east-1\",\n    s3_bucket=\"my-artifacts\",\n    ecr_registry=\"123456789.dkr.ecr.us-east-1.amazonaws.com\"\n)\n</code></pre>"},{"location":"architecture/stacks/#kubernetes-stack-coming-soon","title":"Kubernetes Stack (Coming Soon)","text":"<pre><code>from flowyml.stacks.k8s import KubernetesStack\n\nstack = KubernetesStack(\n    name=\"k8s-cluster\",\n    namespace=\"flowyml\",\n    storage_class=\"standard\"\n)\n</code></pre>"},{"location":"architecture/stacks/#resource-configuration","title":"Resource Configuration","text":"<p>Define compute resources for your pipelines:</p> <pre><code>from flowyml.stacks.components import ResourceConfig\n\n# CPU-intensive workload\ncpu_config = ResourceConfig(\n    cpu=\"8\",\n    memory=\"32Gi\",\n    disk_size=\"100Gi\"\n)\n\n# GPU workload\ngpu_config = ResourceConfig(\n    cpu=\"16\",\n    memory=\"64Gi\",\n    gpu=\"nvidia-tesla-v100\",\n    gpu_count=4,\n    machine_type=\"n1-highmem-16\"\n)\n\n# Memory-intensive workload\nmemory_config = ResourceConfig(\n    cpu=\"32\",\n    memory=\"256Gi\",\n    machine_type=\"n1-megamem-96\"\n)\n</code></pre>"},{"location":"architecture/stacks/#docker-configuration","title":"Docker Configuration","text":"<p>Containerize your pipelines:</p> <pre><code>from flowyml.stacks.components import DockerConfig\n\n# Pre-built image\ndocker_config = DockerConfig(\n    image=\"gcr.io/my-project/ml-pipeline:v1.0\"\n)\n\n# Build from Dockerfile\ndocker_config = DockerConfig(\n    dockerfile=\"./Dockerfile\",\n    build_context=\".\",\n    build_args={\"PYTHON_VERSION\": \"3.11\"}\n)\n\n# Dynamic requirements\ndocker_config = DockerConfig(\n    base_image=\"python:3.11-slim\",\n    requirements=[\n        \"tensorflow&gt;=2.12.0\",\n        \"pandas&gt;=2.0.0\"\n    ],\n    env_vars={\n        \"PYTHONUNBUFFERED\": \"1\",\n        \"TF_CPP_MIN_LOG_LEVEL\": \"2\"\n    }\n)\n</code></pre>"},{"location":"architecture/stacks/#stack-registry","title":"Stack Registry","text":"<p>Manage multiple stacks and switch seamlessly:</p> <pre><code>from flowyml.stacks.registry import StackRegistry\n\n# Create registry\nregistry = StackRegistry()\n\n# Register stacks\nregistry.register_stack(local_stack)\nregistry.register_stack(gcp_stack)\nregistry.register_stack(aws_stack)\n\n# List available stacks\nprint(registry.list_stacks())\n# ['local', 'gcp-prod', 'aws-prod']\n\n# Switch stacks\nregistry.set_active_stack(\"local\")      # Development\nregistry.set_active_stack(\"gcp-prod\")   # Production\n\n# Get active stack\nactive = registry.get_active_stack()\n</code></pre>"},{"location":"architecture/stacks/#using-stacks-with-pipelines","title":"Using Stacks with Pipelines","text":""},{"location":"architecture/stacks/#method-1-direct-assignment","title":"Method 1: Direct Assignment","text":"<pre><code>from flowyml import Pipeline\nfrom flowyml.stacks.gcp import GCPStack\n\nstack = GCPStack(...)\npipeline = Pipeline(\"my_pipeline\", stack=stack)\n</code></pre>"},{"location":"architecture/stacks/#method-2-global-registry","title":"Method 2: Global Registry","text":"<pre><code>from flowyml import Pipeline\nfrom flowyml.stacks.registry import set_active_stack\n\n# Set active stack globally\nset_active_stack(\"production\")\n\n# All new pipelines use active stack\npipeline = Pipeline(\"my_pipeline\")\n</code></pre>"},{"location":"architecture/stacks/#method-3-per-run-override","title":"Method 3: Per-Run Override","text":"<pre><code>pipeline = Pipeline(\"my_pipeline\")\n\n# Run locally\npipeline.run(stack=local_stack)\n\n# Run on GCP\npipeline.run(stack=gcp_stack)\n</code></pre>"},{"location":"architecture/stacks/#best-practices","title":"Best Practices","text":""},{"location":"architecture/stacks/#1-environment-based-configuration","title":"1. Environment-Based Configuration","text":"<pre><code>import os\nfrom flowyml.stacks import LocalStack\nfrom flowyml.stacks.gcp import GCPStack\n\nenv = os.getenv(\"ENVIRONMENT\", \"local\")\n\nif env == \"production\":\n    stack = GCPStack(...)\nelif env == \"staging\":\n    stack = GCPStack(..., bucket_name=\"staging-artifacts\")\nelse:\n    stack = LocalStack()\n</code></pre>"},{"location":"architecture/stacks/#2-configuration-files","title":"2. Configuration Files","text":"<pre><code># flowyml.yaml\nstacks:\n  local:\n    type: local\n    artifact_path: .flowyml/artifacts\n\n  production:\n    type: gcp\n    project_id: ${GCP_PROJECT_ID}\n    region: us-central1\n    bucket_name: ${GCP_BUCKET}\n</code></pre>"},{"location":"architecture/stacks/#3-validation","title":"3. Validation","text":"<pre><code># Always validate before production\nstack.validate()\n\n# Check configuration\nprint(stack.to_dict())\n</code></pre>"},{"location":"architecture/stacks/#4-cost-optimization","title":"4. Cost Optimization","text":"<pre><code># Use preemptible instances for fault-tolerant workloads\nResourceConfig(\n    machine_type=\"n1-standard-4\",\n    preemptible=True  # 80% cost savings\n)\n\n# Right-size resources\nResourceConfig(\n    cpu=\"2\",  # Start small\n    memory=\"8Gi\",\n    autoscaling=True  # Scale as needed\n)\n</code></pre>"},{"location":"architecture/stacks/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"architecture/stacks/#multi-cloud-setup","title":"Multi-Cloud Setup","text":"<pre><code># GCP for training\ntrain_stack = GCPStack(name=\"gcp-train\", ...)\n\n# AWS for inference\ninference_stack = AWSStack(name=\"aws-inference\", ...)\n\n# Different pipelines, different clouds\ntraining_pipeline.run(stack=train_stack)\ninference_pipeline.run(stack=inference_stack)\n</code></pre>"},{"location":"architecture/stacks/#hybrid-execution","title":"Hybrid Execution","text":"<pre><code># Some steps local, some on cloud\n@step(stack=local_stack)\ndef preprocess():\n    ...\n\n@step(stack=gcp_stack, resources=gpu_config)\ndef train():\n    ...\n\n@step(stack=local_stack)\ndef evaluate():\n    ...\n</code></pre>"},{"location":"architecture/stacks/#troubleshooting","title":"Troubleshooting","text":""},{"location":"architecture/stacks/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Authentication Errors <pre><code>gcloud auth login\ngcloud auth application-default login\n</code></pre></p> </li> <li> <p>Permission Denied</p> </li> <li>Check service account roles</li> <li> <p>Verify IAM permissions</p> </li> <li> <p>Resource Quota</p> </li> <li>Request quota increase</li> <li> <p>Use smaller machine types</p> </li> <li> <p>Image Not Found</p> </li> <li>Push image to registry</li> <li>Check image URI format</li> </ol>"},{"location":"architecture/stacks/#next-steps","title":"Next Steps","text":"<ul> <li>GCP Stack Guide</li> <li>Resource Optimization</li> <li>CI/CD Integration</li> </ul>"},{"location":"core/assets/","title":"Assets &amp; Artifacts \ud83d\udc8e","text":"<p>In flowyml, data lineage and artifact tracking are first-class features. Every piece of data flowing through your pipeline is tracked, versioned, and queryable.</p> <p>[!NOTE] What you'll learn: How to work with typed assets (Datasets, Models, Metrics) and track complete data lineage</p> <p>Key insight: Reproducibility requires lineage. flowyml tracks not just what models you trained, but what data created them, which code version, and all hyperparameters.</p>"},{"location":"core/assets/#why-assets-matter","title":"Why Assets Matter","text":"<p>Without structured assets, teams face: - \"Which data trained this model?\" \u2014 Unknown, guesswork - \"Can we reproduce this result?\" \u2014 Maybe, if you kept notes - \"Where did this artifact come from?\" \u2014 Lost in the pipeline - \"What changed between runs?\" \u2014 Manual diffing, error-prone</p> <p>With flowyml assets, you get: - Automatic lineage tracking: Every asset knows its parents - Version control for data: Not just code, but datasets and models - Audit trails: Full provenance from raw data to predictions - Reproducibility: Re-create any result on demand</p> <p>[!IMPORTANT] For regulated industries (finance, healthcare, legal): Asset lineage isn't optional. flowyml provides audit-ready traceability out of the box.</p>"},{"location":"core/assets/#the-asset-hierarchy","title":"The Asset Hierarchy \ud83c\udfdb\ufe0f","text":"<p>flowyml provides specialized classes for different ML artifact types:</p> <ul> <li>Asset: The base class for all versioned objects.</li> <li>Dataset: Represents data (DataFrames, file paths, tensors).</li> <li>Model: Represents trained ML models.</li> <li>Metrics: Represents evaluation results (accuracy, loss).</li> <li>FeatureSet: Represents engineered features.</li> </ul>"},{"location":"core/assets/#creating-assets","title":"Creating Assets \ud83d\udd28","text":"<p>You can create assets explicitly using the <code>.create()</code> factory method. This automatically handles versioning, metadata generation, and lineage tracking.</p>"},{"location":"core/assets/#datasets","title":"Datasets","text":"<pre><code>from flowyml import Dataset\nimport pandas as pd\n\ndf = pd.DataFrame(...)\n\n# Create a versioned dataset\ndataset = Dataset.create(\n    data=df,\n    name=\"training_data\",\n    properties={\n        \"source\": \"s3://bucket/data.csv\",\n        \"rows\": len(df)\n    }\n)\n</code></pre>"},{"location":"core/assets/#models","title":"Models","text":"<pre><code>from flowyml import Model\n\n# Create a versioned model\nmodel_asset = Model.create(\n    data=trained_model_object,\n    name=\"resnet50_finetuned\",\n    framework=\"pytorch\",\n    parameters={\"epochs\": 10, \"lr\": 0.001}\n)\n</code></pre>"},{"location":"core/assets/#metrics","title":"Metrics","text":"<pre><code>from flowyml import Metrics\n\n# Create a metrics object\nmetrics = Metrics.create(\n    accuracy=0.95,\n    f1_score=0.92,\n    loss=0.15\n)\n</code></pre>"},{"location":"core/assets/#lineage-tracking","title":"Lineage Tracking \ud83d\udd17","text":"<p>flowyml automatically tracks the lineage of every asset.</p> <ul> <li>Parents: The assets that were used to create this asset.</li> <li>Children: The assets that were created using this asset.</li> <li>Producer: The pipeline step that generated this asset.</li> </ul> <p>When you pass an asset from one step to another, flowyml records this relationship.</p> <pre><code>@step\ndef preprocess(raw_data):\n    # ...\n    return clean_data  # clean_data's parent is raw_data\n\n@step\ndef train(clean_data):\n    # ...\n    return model      # model's parent is clean_data\n</code></pre> <p>Visualize It</p> <p>You can visualize this lineage graph in the flowyml UI.</p>"},{"location":"core/assets/#storage","title":"Storage \ud83d\udcbe","text":"<p>Assets are stored in the Artifact Store. By default, this is the <code>.flowyml/artifacts</code> directory in your project.</p> <p>flowyml supports pluggable storage backends (S3, GCS, Azure) via <code>fsspec</code>. Configuration is handled in <code>flowyml.yaml</code>.</p>"},{"location":"core/assets/#automatic-materialization","title":"Automatic Materialization \ud83d\udce6","text":"<p>When running a pipeline with a Stack that has an Artifact Store configured, flowyml automatically materializes step outputs.</p> <p>The artifacts are stored in a structured path: <code>{project_name}/{date}/{run_id}/data/{step_name}/{artifact_name}</code></p> <p>This ensures that every run is reproducible and all intermediate data is persisted. flowyml uses Materializers to handle serialization for different data types (Pandas, NumPy, Keras, PyTorch, etc.).</p>"},{"location":"core/context/","title":"Context &amp; Parameters \ud83e\udde0","text":"<p>flowyml's context system eliminates configuration hell by providing automatic parameter injection across pipeline steps.</p> <p>[!NOTE] What you'll learn: How to manage configuration without hardcoding, enabling the same pipeline to run in dev/staging/prod</p> <p>Key insight: Context separates what your pipeline does from how it's configured. Change parameters, not code.</p>"},{"location":"core/context/#why-context-matters","title":"Why Context Matters","text":"<p>Without context, ML pipelines suffer from: - Hardcoded parameters: <code>learning_rate = 0.001</code> buried in code - Environment coupling: Different code for dev vs. prod - Configuration sprawl: Parameters scattered across files - Manual wiring: Pass every parameter through every function</p> <p>With flowyml context, you get: - Automatic injection: Parameters flow to steps that need them - Environment flexibility: Same code, different configs - Centralized configuration: All parameters in one place - Type safety: Type hints validate parameters automatically</p> <p>[!TIP] The killer feature: Run the same pipeline with different configs just by swapping context. No code changes to go from dev (small dataset, CPU) to prod (full dataset, GPU).</p>"},{"location":"core/context/#the-context-object","title":"The Context Object","text":"<p>The <code>Context</code> object serves as a container for: 1. Global Parameters: Hyperparameters, configuration settings 2. Environment Variables: Paths, endpoints, credentials 3. Runtime Settings: Batch sizes, resource requirements 4. Domain Logic: Business rules, thresholds</p>"},{"location":"core/context/#creating-a-context","title":"Creating a Context","text":"<p>You can create a context with any number of keyword arguments:</p> <pre><code>from flowyml import context\n\n# Define parameters\nctx = context(\n    learning_rate=0.001,\n    batch_size=64,\n    model_type=\"resnet50\",\n    random_seed=42,\n    data_path=\"./data/train.csv\"\n)\n</code></pre>"},{"location":"core/context/#using-context-with-pipelines","title":"Using Context with Pipelines","text":"<pre><code>from flowyml import Pipeline, context\n\nctx = context(learning_rate=0.01, epochs=100)\n\n# Pass context to pipeline\npipeline = Pipeline(\"training_pipeline\", context=ctx)\n</code></pre>"},{"location":"core/context/#automatic-injection","title":"Automatic Injection \ud83d\udc89","text":"<p>The most powerful feature of flowyml's context is automatic parameter injection. If a step function argument matches a key in the context, flowyml will automatically inject the value when the step is executed.</p>"},{"location":"core/context/#example","title":"Example","text":"<pre><code>from flowyml import Pipeline, step, context\n\n# 1. Define Context\nctx = context(\n    learning_rate=0.01,\n    epochs=10,\n    batch_size=32\n)\n\n# 2. Define Steps with matching parameter names\n@step(outputs=[\"model\"])\ndef train_model(data, learning_rate: float, epochs: int):\n    # 'learning_rate' and 'epochs' are automatically injected from context!\n    print(f\"Training with lr={learning_rate}, epochs={epochs}\")\n    model = train(data, lr=learning_rate, epochs=epochs)\n    return model\n\n@step(inputs=[\"model\"], outputs=[\"metrics\"])\ndef evaluate(model, batch_size: int):\n    # 'batch_size' automatically injected!\n    return evaluate_model(model, batch_size=batch_size)\n\n# 3. Create Pipeline with Context\npipeline = Pipeline(\"ml_pipeline\", context=ctx)\npipeline.add_step(train_model)\npipeline.add_step(evaluate)\n\n# 4. Run - parameters automatically injected!\nresult = pipeline.run()\n</code></pre>"},{"location":"core/context/#how-it-works","title":"How It Works","text":"<ol> <li>Parameter Matching: flowyml inspects each step function's signature</li> <li>Context Lookup: For each parameter, it checks if a matching key exists in the context</li> <li>Automatic Injection: If found, the value is injected when calling the step</li> <li>Type Validation: Type hints are used to validate injected values</li> </ol> <pre><code># This step signature:\ndef train(data, learning_rate: float, epochs: int):\n    ...\n\n# With this context:\nctx = context(learning_rate=0.01, epochs=100)\n\n# Results in this call:\ntrain(data=previous_output, learning_rate=0.01, epochs=100)\n</code></pre>"},{"location":"core/context/#parameter-overrides","title":"Parameter Overrides \ud83d\udd04","text":"<p>You can override context parameters at runtime:</p> <pre><code># Original context\nctx = context(learning_rate=0.01, epochs=10)\npipeline = Pipeline(\"training\", context=ctx)\n\n# Override for this specific run\nresult = pipeline.run(context={\"epochs\": 20, \"learning_rate\": 0.05})\n# Now uses epochs=20 and learning_rate=0.05\n</code></pre>"},{"location":"core/context/#use-cases-for-overrides","title":"Use Cases for Overrides","text":"<ol> <li>Experimentation: Try different hyperparameters quickly</li> <li>Production vs Development: Different settings for different environments</li> <li>A/B Testing: Run same pipeline with variations</li> </ol> <pre><code># Compare different learning rates\nfor lr in [0.001, 0.01, 0.1]:\n    result = pipeline.run(context={\"learning_rate\": lr})\n    print(f\"LR={lr}: accuracy={result.outputs['metrics'].accuracy}\")\n</code></pre>"},{"location":"core/context/#context-updates","title":"Context Updates","text":"<p>You can also update the context dynamically:</p> <pre><code>ctx = context(epochs=10)\n\n# Update context\nctx.update({\"learning_rate\": 0.01, \"batch_size\": 32})\n\n# Now context has all three parameters\n</code></pre>"},{"location":"core/context/#accessing-context-data","title":"Accessing Context Data \ud83d\udd0d","text":""},{"location":"core/context/#individual-parameters","title":"Individual Parameters","text":"<p>Most commonly, you just declare parameters in your step function:</p> <pre><code>@step\ndef my_step(param1: str, param2: int):\n    # param1 and param2 injected from context\n    print(f\"{param1}, {param2}\")\n</code></pre>"},{"location":"core/context/#full-context-access","title":"Full Context Access","text":"<p>If you need access to the entire context object:</p> <pre><code>@step\ndef inspect_context(context):\n    # Access all parameters\n    print(f\"All params: {context.to_dict()}\")\n\n    # Check if parameter exists\n    if \"optional_param\" in context:\n        use_param(context[\"optional_param\"])\n</code></pre>"},{"location":"core/context/#context-properties","title":"Context Properties","text":"<p>The Context object provides useful methods:</p> <pre><code>ctx = context(a=1, b=2, c=3)\n\n# Convert to dictionary\nparams_dict = ctx.to_dict()  # {\"a\": 1, \"b\": 2, \"c\": 3}\n\n# Iterate over parameters\nfor key, value in ctx.items():\n    print(f\"{key}={value}\")\n\n# Get keys\nkeys = list(ctx.keys())  # [\"a\", \"b\", \"c\"]\n\n# Check membership\nif \"a\" in ctx:\n    print(ctx[\"a\"])\n</code></pre>"},{"location":"core/context/#mixing-input-data-and-context-parameters","title":"Mixing Input Data and Context Parameters","text":"<p>Steps can receive both pipeline data (from previous steps) and context parameters:</p> <pre><code>@step(outputs=[\"data\"])\ndef load_data(file_path: str):\n    # file_path from context\n    return pd.read_csv(file_path)\n\n@step(inputs=[\"data\"], outputs=[\"processed\"])\ndef process(data, threshold: float, normalize: bool):\n    # 'data' from previous step (load_data output)\n    # 'threshold' and 'normalize' from context\n    if normalize:\n        data = normalize_data(data)\n    return filter_by_threshold(data, threshold)\n\nctx = context(\n    file_path=\"data/train.csv\",\n    threshold=0.5,\n    normalize=True\n)\n</code></pre>"},{"location":"core/context/#type-hints-and-validation","title":"Type Hints and Validation \ud83c\udfaf","text":"<p>Type hints serve two purposes:</p> <ol> <li>Documentation: Clarify expected types</li> <li>Validation: Help flowyml match parameters correctly</li> </ol> <pre><code>from typing import List, Dict, Optional\n\n@step\ndef train(\n    data: List[float],           # From previous step\n    learning_rate: float,        # From context - must be float\n    layers: List[int],           # From context - must be list of ints\n    config: Dict[str, any],      # From context - must be dict\n    optional_param: Optional[str] = None  # Optional context parameter\n):\n    ...\n\nctx = context(\n    learning_rate=0.01,          # \u2713 Matches type\n    layers=[128, 64, 32],        # \u2713 Matches type\n    config={\"dropout\": 0.5},     # \u2713 Matches type\n    # optional_param not provided - uses default None\n)\n</code></pre>"},{"location":"core/context/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"core/context/#environment-specific-contexts","title":"Environment-Specific Contexts","text":"<pre><code>import os\n\ndef get_context():\n    env = os.getenv(\"ENV\", \"development\")\n\n    if env == \"production\":\n        return context(\n            data_path=\"s3://prod-bucket/data\",\n            batch_size=256,\n            use_gpu=True\n        )\n    else:\n        return context(\n            data_path=\"./local_data\",\n            batch_size=32,\n            use_gpu=False\n        )\n\npipeline = Pipeline(\"adaptive\", context=get_context())\n</code></pre>"},{"location":"core/context/#nested-configuration","title":"Nested Configuration","text":"<pre><code>ctx = context(\n    model_config={\n        \"type\": \"transformer\",\n        \"hidden_size\": 768,\n        \"num_layers\": 12\n    },\n    training_config={\n        \"learning_rate\": 0.001,\n        \"warmup_steps\": 1000\n    }\n)\n\n@step\ndef train(model_config: dict, training_config: dict):\n    # Access nested configuration\n    model = create_model(**model_config)\n    optimizer = create_optimizer(**training_config)\n    ...\n</code></pre>"},{"location":"core/context/#context-inheritance","title":"Context Inheritance","text":"<pre><code># Base context\nbase_ctx = context(\n    random_seed=42,\n    verbose=True\n)\n\n# Extend for specific use case\ntraining_ctx = context(\n    **base_ctx.to_dict(),\n    learning_rate=0.01,\n    epochs=100\n)\n</code></pre>"},{"location":"core/context/#best-practices","title":"Best Practices \ud83c\udf1f","text":""},{"location":"core/context/#1-use-descriptive-parameter-names","title":"1. Use Descriptive Parameter Names","text":"<pre><code># \u2705 Good - clear and specific\nctx = context(\n    learning_rate=0.01,\n    max_epochs=100,\n    early_stopping_patience=10\n)\n\n# \u274c Bad - unclear abbreviations\nctx = context(\n    lr=0.01,\n    e=100,\n    p=10\n)\n</code></pre>"},{"location":"core/context/#2-always-use-type-hints","title":"2. Always Use Type Hints","text":"<pre><code># \u2705 Good - types make injection reliable\n@step\ndef process(data, threshold: float, iterations: int):\n    ...\n\n# \u26a0\ufe0f Less ideal - no type information\n@step\ndef process(data, threshold, iterations):\n    ...\n</code></pre>"},{"location":"core/context/#3-provide-sensible-defaults","title":"3. Provide Sensible Defaults","text":"<pre><code># \u2705 Good - works with or without context values\n@step\ndef train(\n    data,\n    learning_rate: float = 0.001,\n    epochs: int = 10,\n    verbose: bool = False\n):\n    ...\n\n# Can run without providing these in context\n</code></pre>"},{"location":"core/context/#4-group-related-parameters","title":"4. Group Related Parameters","text":"<pre><code># \u2705 Good - organized into logical groups\nctx = context(\n    # Data parameters\n    data_path=\"./data\",\n    validation_split=0.2,\n\n    # Model parameters\n    model_type=\"resnet50\",\n    pretrained=True,\n\n    # Training parameters\n    learning_rate=0.001,\n    batch_size=32,\n    epochs=100\n)\n</code></pre>"},{"location":"core/context/#5-document-your-context-requirements","title":"5. Document Your Context Requirements","text":"<pre><code>def create_training_pipeline(context):\n    \"\"\"Create a training pipeline.\n\n    Required context parameters:\n        - data_path (str): Path to training data\n        - learning_rate (float): Learning rate for optimizer\n        - epochs (int): Number of training epochs\n\n    Optional context parameters:\n        - batch_size (int): Batch size, default 32\n        - random_seed (int): Random seed, default 42\n    \"\"\"\n    pipeline = Pipeline(\"training\", context=context)\n    # ...\n    return pipeline\n</code></pre>"},{"location":"core/context/#debugging-context-issues","title":"Debugging Context Issues \ud83d\udd27","text":""},{"location":"core/context/#check-context-contents","title":"Check Context Contents","text":"<pre><code>ctx = context(a=1, b=2, c=3)\n\n# Print all parameters\nprint(ctx.to_dict())\n\n# Check during pipeline execution\npipeline = Pipeline(\"debug\", context=ctx)\nresult = pipeline.run(debug=True)  # Shows parameter injection\n</code></pre>"},{"location":"core/context/#missing-parameter-errors","title":"Missing Parameter Errors","text":"<p>If a step requires a parameter not in the context or previous outputs:</p> <pre><code>@step\ndef needs_param(required_param: str):\n    ...\n\n# If context doesn't have 'required_param', execution fails\npipeline.run()  # Error: Missing required parameters: ['required_param']\n</code></pre>"},{"location":"core/context/#next-steps","title":"Next Steps \ud83d\udcda","text":"<ul> <li>Pipelines: Learn how to build workflows</li> <li>Steps: Master step configuration</li> <li>Configuration: External configuration files</li> <li>Caching: Understand caching with context</li> </ul>"},{"location":"core/pipelines/","title":"Pipelines \ud83d\ude80","text":"<p>Pipelines are the core abstraction in flowyml \u2014 they represent workflows that orchestrate your ML operations from data to deployment.</p> <p>[!NOTE] What you'll learn: How to design, build, and run production-grade pipelines</p> <p>Key insight: A well-designed pipeline is infrastructure-agnostic. Write it once, run it anywhere (local, staging, production) without code changes.</p>"},{"location":"core/pipelines/#why-pipelines-matter","title":"Why Pipelines Matter","text":"<p>Without pipelines, ML workflows are often: - Scripts scattered across notebooks \u2014 Hard to reproduce, impossible to version - Tightly coupled to infrastructure \u2014 Rewrite for every environment - Manually orchestrated \u2014 Prone to human error, doesn't scale - Opaque \u2014 Can't see what's running, debug failures, or track lineage</p> <p>flowyml pipelines solve this by providing: - Declarative workflows \u2014 Define what to do, not how to execute it - Automatic dependency resolution \u2014 Steps run in the right order - Built-in observability \u2014 Track every run, inspect every artifact - Environment portability \u2014 Same code, different stacks</p>"},{"location":"core/pipelines/#pipeline-design-principles","title":"Pipeline Design Principles","text":"<p>Before diving into code, understand these design principles:</p>"},{"location":"core/pipelines/#1-steps-should-be-pure-functions","title":"1. Steps Should Be Pure Functions","text":"<pre><code># \u2705 Good: Pure function, testable in isolation\n@step(outputs=[\"processed\"])\ndef clean_data(raw_data):\n    return raw_data.dropna().reset_index(drop=True)\n\n# \u274c Bad: Side effects, hard to test\n@step\ndef clean_data():\n    global df  # Don't do this!\n    df = df.dropna()\n</code></pre> <p>Why: Pure functions are testable, cacheable, and parallelizable.</p>"},{"location":"core/pipelines/#2-one-step-one-responsibility","title":"2. One Step, One Responsibility","text":"<pre><code># \u2705 Good: Focused steps\n@step(outputs=[\"split_data\"])\ndef split_data(data): ...\n\n@step(inputs=[\"split_data\"], outputs=[\"model\"])\ndef train_model(split_data): ...\n\n# \u274c Bad: Doing too much\n@step(outputs=[\"model\"])\ndef split_and_train(data):\n    # Splitting and training in one step = can't cache independently\n    split = split_data(data)\n    model = train(split)\n    return model\n</code></pre> <p>Why: Granular steps enable better caching, debugging, and reuse.</p>"},{"location":"core/pipelines/#3-configuration-belongs-in-context","title":"3. Configuration Belongs in Context","text":"<pre><code># \u2705 Good: Context injection\nctx = context(learning_rate=0.001, epochs=10)\npipeline = Pipeline(\"training\", context=ctx)\n\n@step(outputs=[\"model\"])\ndef train(data, learning_rate: float, epochs: int):\n    # Parameters injected automatically\n    ...\n\n# \u274c Bad: Hardcoded configuration\n@step(outputs=[\"model\"])\ndef train(data):\n    learning_rate = 0.001  # Can't change without code edit\n    epochs = 10\n</code></pre> <p>Why: Separation of code and config enables dev/staging/prod with one codebase.</p>"},{"location":"core/pipelines/#creating-your-first-pipeline","title":"Creating Your First Pipeline","text":"<p>Here's a complete, runnable example:</p> <pre><code>from flowyml import Pipeline, step, context\n\n# Define steps\n@step(outputs=[\"raw_data\"])\ndef extract():\n    return [1, 2, 3, 4, 5]\n\n@step(inputs=[\"raw_data\"], outputs=[\"processed_data\"])\ndef transform(raw_data):\n    return [x * 2 for x in raw_data]\n\n# Create pipeline\npipeline = Pipeline(\"etl_pipeline\")\n\n# Add steps in execution order\npipeline.add_step(extract)\npipeline.add_step(transform)\n\n# Run the pipeline\nresult = pipeline.run()\n\nif result.success:\n    print(f\"\u2713 Processed data: {result.outputs['processed_data']}\")\n</code></pre> <p>What just happened: flowyml built a DAG, determined execution order, and ran your steps. No Airflow DAG files, no Kubeflow YAML, just Python.</p>"},{"location":"core/pipelines/#pipeline-configuration","title":"Pipeline Configuration \u2699\ufe0f","text":"<p>The <code>Pipeline</code> class accepts several configuration options:</p> <pre><code>from flowyml import Pipeline, context\n\nctx = context(\n    learning_rate=0.001,\n    batch_size=32\n)\n\npipeline = Pipeline(\n    name=\"training_pipeline\",\n    context=ctx,              # Context for parameter injection\n    enable_cache=True,        # Enable intelligent caching\n    cache_dir=\"./my_cache\",   # Custom cache directory\n    stack=my_stack            # Execution stack (local, cloud, etc.)\n)\n</code></pre>"},{"location":"core/pipelines/#configuration-options","title":"Configuration Options","text":"Parameter Type Description Default <code>name</code> <code>str</code> Pipeline name (required) - <code>context</code> <code>Context</code> Context object for parameter injection <code>Context()</code> <code>executor</code> <code>Executor</code> Custom executor <code>LocalExecutor()</code> <code>enable_cache</code> <code>bool</code> Enable step caching <code>True</code> <code>cache_dir</code> <code>str</code> Cache storage directory <code>.flowyml/cache</code> <code>stack</code> <code>Stack</code> Execution stack (local/cloud) <code>None</code>"},{"location":"core/pipelines/#execution-graph-dag","title":"Execution Graph (DAG) \ud83d\udd78\ufe0f","text":"<p>When you add steps to a pipeline, flowyml builds a Directed Acyclic Graph (DAG):</p> <ul> <li>Nodes: Steps in the pipeline</li> <li>Edges: Data dependencies between steps</li> </ul> <p>flowyml analyzes the <code>inputs</code> and <code>outputs</code> of each step to determine the execution order automatically.</p> <pre><code>@step(outputs=[\"data\"])\ndef step_a():\n    return [1, 2, 3]\n\n@step(inputs=[\"data\"], outputs=[\"result\"])\ndef step_b(data):\n    return sum(data)\n\n# flowyml automatically determines step_a must run before step_b\n</code></pre>"},{"location":"core/pipelines/#running-pipelines","title":"Running Pipelines \u25b6\ufe0f","text":""},{"location":"core/pipelines/#basic-execution","title":"Basic Execution","text":"<pre><code># Run the pipeline\nresult = pipeline.run()\n\n# Check success\nif result.success:\n    print(f\"\u2713 Pipeline completed successfully!\")\n    print(f\"Outputs: {result.outputs}\")\nelse:\n    print(f\"\u2717 Pipeline failed\")\n</code></pre>"},{"location":"core/pipelines/#with-runtime-overrides","title":"With Runtime Overrides","text":"<p>You can override configuration at runtime:</p> <pre><code># Override context\nresult = pipeline.run(context={\"learning_rate\": 0.05})\n\n# Use different stack\nresult = pipeline.run(stack=my_production_stack)\n\n# Enable debug mode\nresult = pipeline.run(debug=True)\n</code></pre>"},{"location":"core/pipelines/#the-pipelineresult-object","title":"The <code>PipelineResult</code> Object","text":"<p>The result of a pipeline execution is a <code>PipelineResult</code> object:</p> <pre><code>result = pipeline.run()\n\n# Properties\nresult.run_id              # Unique run identifier\nresult.pipeline_name       # Pipeline name\nresult.success             # Boolean: overall success\nresult.outputs             # Dict: all step outputs\nresult.step_results        # Dict: detailed results per step\nresult.duration_seconds    # Total execution time\n\n# Methods\nresult.summary()           # Human-readable summary\nresult.to_dict()          # Convert to dictionary\nresult[\"step_name\"]       # Access specific output\n</code></pre>"},{"location":"core/pipelines/#example-inspecting-results","title":"Example: Inspecting Results","text":"<pre><code>result = pipeline.run()\n\nif result.success:\n    print(result.summary())\n\n    # Access step results\n    for step_name, step_result in result.step_results.items():\n        print(f\"Step: {step_name}\")\n        print(f\"  Duration: {step_result.duration_seconds:.2f}s\")\n        print(f\"  Cached: {step_result.cached}\")\n        if step_result.artifact_uri:\n            print(f\"  Artifact: {step_result.artifact_uri}\")\n</code></pre>"},{"location":"core/pipelines/#pipeline-building","title":"Pipeline Building","text":""},{"location":"core/pipelines/#adding-steps","title":"Adding Steps","text":"<p>Steps are added in the order they should be registered with the pipeline:</p> <pre><code>pipeline = Pipeline(\"ml_workflow\")\n\n# Add steps\npipeline.add_step(load_data)\npipeline.add_step(preprocess)\npipeline.add_step(train_model)\npipeline.add_step(evaluate)\n\n# Add returns the pipeline for chaining\npipeline = (Pipeline(\"workflow\")\n    .add_step(step1)\n    .add_step(step2)\n    .add_step(step3)\n)\n</code></pre>"},{"location":"core/pipelines/#dag-visualization","title":"DAG Visualization","text":"<pre><code># Build the DAG\npipeline.build()\n\n# Visualize the execution graph\nprint(pipeline.dag.visualize())\n</code></pre> <p>Output: <pre><code>Step Execution Order:\n  1. load_data\n  2. preprocess (depends on: load_data)\n  3. train_model (depends on: preprocess)\n  4. evaluate (depends on: train_model)\n</code></pre></p>"},{"location":"core/pipelines/#working-with-stacks","title":"Working with Stacks \ud83e\udde9","text":"<p>Stacks define where and how your pipeline executes:</p> <pre><code>from flowyml import LocalStack\n\n# Create a local stack with custom paths\nstack = LocalStack(\n    artifact_path=\".flowyml/artifacts\",\n    metadata_path=\".flowyml/metadata.db\"\n)\n\n# Use stack in pipeline\npipeline = Pipeline(\"my_pipeline\", stack=stack)\n\n# Or set at runtime\nresult = pipeline.run(stack=stack)\n</code></pre> <p>See the Stack Architecture guide for more details on stacks.</p>"},{"location":"core/pipelines/#advanced-features","title":"Advanced Features","text":""},{"location":"core/pipelines/#conditional-execution","title":"Conditional Execution","text":"<pre><code>from flowyml import when\n\n@step(outputs=[\"data\"])\ndef load():\n    return {\"quality\": 0.95}\n\n@step(inputs=[\"data\"], outputs=[\"model\"])\n@when(lambda data: data[\"quality\"] &gt; 0.9)\ndef train(data):\n    return \"trained_model\"\n\n# train() only executes if quality &gt; 0.9\n</code></pre>"},{"location":"core/pipelines/#parallel-execution","title":"Parallel Execution","text":"<pre><code>from flowyml import parallel_map\n\n@step\ndef process_batch(items):\n    return parallel_map(expensive_function, items, num_workers=4)\n</code></pre>"},{"location":"core/pipelines/#error-handling","title":"Error Handling","text":"<pre><code>from flowyml import retry, on_failure\n\n@step\n@retry(max_attempts=3, backoff=2.0)\n@on_failure(fallback_function)\ndef flaky_step():\n    # Retries up to 3 times with exponential backoff\n    return fetch_external_data()\n</code></pre>"},{"location":"core/pipelines/#pipeline-patterns-anti-patterns","title":"Pipeline Patterns &amp; Anti-Patterns","text":""},{"location":"core/pipelines/#pattern-environment-agnostic-design","title":"\u2705 Pattern: Environment-Agnostic Design","text":"<pre><code># Same pipeline code works everywhere\nctx_dev = context(data_path=\"./local_data.csv\", batch_size=32)\nctx_prod = context(data_path=\"gs://bucket/data.csv\", batch_size=512)\n\n# Development\npipeline = Pipeline(\"ml_training\", context=ctx_dev)\npipeline.run()\n\n# Production (same code!)\npipeline = Pipeline(\"ml_training\", context=ctx_prod, stack=prod_stack)\npipeline.run()\n</code></pre> <p>Why this works: Zero code changes from dev to prod. Just swap context and stack.</p>"},{"location":"core/pipelines/#anti-pattern-environment-specific-branches","title":"\u274c Anti-Pattern: Environment-Specific Branches","text":"<pre><code># Don't do this!\n@step(outputs=[\"data\"])\ndef load_data():\n    if os.getenv(\"ENV\") == \"production\":\n        return load_from_gcs()\n    else:\n        return load_from_local()\n</code></pre> <p>Why it's bad: Logic pollution, hard to test, environments drift apart.</p> <p>Fix: Use context and stacks to handle environment differences.</p>"},{"location":"core/pipelines/#pattern-composition-over-inheritance","title":"\u2705 Pattern: Composition Over Inheritance","text":"<pre><code># Reusable, composable steps\n@step(outputs=[\"data\"])\ndef load_csv(path: str):\n    return pd.read_csv(path)\n\n@step(inputs=[\"data\"], outputs=[\"clean\"])\ndef clean(data):\n    return data.dropna()\n\n# Build multiple pipelines from same steps\netl_pipeline = Pipeline(\"etl\").add_step(load_csv).add_step(clean)\nvalidation_pipeline = Pipeline(\"validation\").add_step(load_csv).add_step(validate)\n</code></pre> <p>Why this works: Steps are building blocks. Mix and match for different workflows.</p>"},{"location":"core/pipelines/#anti-pattern-monolithic-pipelines","title":"\u274c Anti-Pattern: Monolithic Pipelines","text":"<pre><code># Don't do this!\n@step(outputs=[\"everything\"])\ndef do_everything():\n    data = load()\n    clean = process(data)\n    model = train(clean)\n    metrics = evaluate(model)\n    deploy(model)\n    return metrics\n</code></pre> <p>Why it's bad: Can't cache parts, can't parallelize, can't reuse, hard to debug.</p>"},{"location":"core/pipelines/#pattern-fail-fast-with-validation-steps","title":"\u2705 Pattern: Fail Fast with Validation Steps","text":"<pre><code>@step(outputs=[\"data\"])\ndef load_data():\n    return fetch_data()\n\n@step(inputs=[\"data\"])\ndef validate_data(data):\n    if len(data) &lt; 100:\n        raise ValueError(\"Insufficient data\")\n    if data['target'].isnull().any():\n        raise ValueError(\"Missing target values\")\n    # Validation passes, no output needed\n\n@step(inputs=[\"data\"], outputs=[\"model\"])\ndef train_model(data):\n    # Only runs if validation passed\n    return train(data)\n</code></pre> <p>Why this works: Catch problems early, save expensive compute on bad data.</p>"},{"location":"core/pipelines/#decision-guide-when-to-split-steps","title":"Decision Guide: When to Split Steps","text":"Scenario Split? Reason Step takes &gt;5 minutes to run \u2705 Yes Better caching granularity Might want to run parts separately \u2705 Yes Enables reuse Operation is expensive (GPU, API calls) \u2705 Yes Cache results independently Tightly coupled operations (save + load same artifact) \u274c No Keep together for atomicity Fast operations (&lt;1 second) \u274c Maybe Balance overhead vs. benefit"},{"location":"core/pipelines/#real-world-pipeline-examples","title":"Real-World Pipeline Examples","text":""},{"location":"core/pipelines/#ml-training-pipeline","title":"ML Training Pipeline","text":"<pre><code>from flowyml import Pipeline, step, context\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\nctx = context(\n    data_path=\"data/train.csv\",\n    test_size=0.2,\n    n_estimators=100,\n    random_state=42\n)\n\n@step(outputs=[\"raw_data\"])\ndef load_data(data_path: str):\n    return pd.read_csv(data_path)\n\n@step(inputs=[\"raw_data\"], outputs=[\"X_train\", \"X_test\", \"y_train\", \"y_test\"])\ndef split_data(raw_data, test_size: float, random_state: int):\n    X = raw_data.drop('target', axis=1)\n    y = raw_data['target']\n    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n@step(inputs=[\"X_train\", \"y_train\"], outputs=[\"model\"])\ndef train(X_train, y_train, n_estimators: int):\n    model = RandomForestClassifier(n_estimators=n_estimators)\n    model.fit(X_train, y_train)\n    return model\n\n@step(inputs=[\"model\", \"X_test\", \"y_test\"], outputs=[\"accuracy\"])\ndef evaluate(model, X_test, y_test):\n    return model.score(X_test, y_test)\n\npipeline = Pipeline(\"ml_training\", context=ctx)\npipeline.add_step(load_data)\npipeline.add_step(split_data)\npipeline.add_step(train)\npipeline.add_step(evaluate)\n\nresult = pipeline.run()\nprint(f\"Model accuracy: {result.outputs['accuracy']:.2%}\")\n</code></pre> <p>Why this design works: - Each step is independently cacheable - Parameters in context, not hardcoded - Can easily add more steps (preprocessing, feature engineering) - Testable components</p>"},{"location":"core/pipelines/#best-practices-summary","title":"Best Practices Summary","text":"<ol> <li>Name descriptively: <code>customer_churn_prediction</code> &gt; <code>pipeline1</code></li> <li>Explicit dependencies: Always declare <code>inputs</code> and <code>outputs</code></li> <li>One responsibility per step: Split complex logic into focused steps</li> <li>Context for configuration: Never hardcode parameters</li> <li>Enable caching in dev: Speed up iteration, disable in prod if needed</li> <li>Fail fast: Validate early, don't waste compute on bad data</li> <li>Test in isolation: Each step should be unit-testable</li> <li>Document assumptions: Use docstrings to explain step requirements</li> <li>Version your pipelines: Use Git for pipeline code versioning</li> <li>Monitor in production: Use the UI to track runs and catch failures</li> </ol>"},{"location":"core/pipelines/#next-steps","title":"Next Steps \ud83d\udcda","text":"<p>Master the building blocks: - Steps: Deep dive into step configuration, decorators, and best practices - Context: Learn parameter injection patterns and environment management - Assets: Work with typed artifacts (Datasets, Models, Metrics)</p> <p>Level up your pipelines: - Caching: Optimize iteration speed with intelligent caching - Conditional Execution: Build adaptive workflows - Parallel Execution: Speed up independent operations - Error Handling: Build resilient production pipelines</p> <p>Deploy to production: - Stack Architecture: Understand local vs. cloud execution - Projects: Organize multi-tenant deployments - Scheduling: Automate recurring pipelines</p>"},{"location":"core/steps/","title":"Steps \ud83d\udc63","text":"<p>Steps are the atomic units of work in flowyml pipelines. They transform regular Python functions into tracked, cacheable, retriable building blocks.</p> <p>[!NOTE] What you'll learn: How to design reusable, testable steps that compose into production pipelines</p> <p>Key insight: Well-designed steps are pure, focused, and composable. They work in isolation, cache intelligently, and combine into complex workflows.</p>"},{"location":"core/steps/#why-steps-matter","title":"Why Steps Matter","text":"<p>Without steps, you have: - Functions scattered across files, hard to reuse - No automatic caching of expensive operations - Manual error handling and retries - No visibility into what's running or failed</p> <p>With flowyml steps, you get: - Automatic dependency tracking: flowyml knows execution order - Intelligent caching: Skip redundant computation automatically - Built-in retry logic: Handle transient failures gracefully - Full observability: See inputs, outputs, duration for every execution - Testability: Each step is a pure function you can unit test</p>"},{"location":"core/steps/#step-design-principles","title":"Step Design Principles","text":""},{"location":"core/steps/#1-steps-should-be-pure-functions","title":"1. Steps Should Be Pure Functions","text":"<pre><code># \u2705 Good: Pure function, deterministic output\n@step(outputs=[\"processed\"])\ndef clean_data(raw_data):\n    return raw_data.dropna().reset_index(drop=True)\n\n# \u274c Bad: Side effects, global state\n@step\ndef clean_data_impure():\n    global df_global  # Don't do this!\n    df_global = df_global.dropna()\n    return df_global\n</code></pre> <p>Why: Pure functions are testable, cacheable, and parallelizable. Side effects break caching and make debugging nightmares.</p>"},{"location":"core/steps/#2-one-step-one-responsibility","title":"2. One Step, One Responsibility","text":"<pre><code># \u2705 Good: Focused steps, independently cacheable\n@step(outputs=[\"split\"])\ndef split_data(data, test_size=0.2): ...\n\n@step(inputs=[\"split\"], outputs=[\"model\"])\ndef train_model(split): ...\n\n# \u274c Bad: Monolithic step, can't cache parts\n@step(outputs=[\"model\"])\ndef split_and_train_and_evaluate(data):\n    split = split_data(data)      # Can't cache this separately\n    model = train_model(split)    # Or this\n    metrics = evaluate(model)     # Or this\n    return model, metrics\n</code></pre> <p>Why: Granular steps enable better caching. Tweaking training doesn't re-run data splitting.</p>"},{"location":"core/steps/#3-explicit-is-better-than-implicit","title":"3. Explicit Is Better Than Implicit","text":"<pre><code># \u2705 Good: Clear inputs/outputs\n@step(inputs=[\"raw_data\"], outputs=[\"clean_data\"])\ndef clean(raw_data):\n    return process(raw_data)\n\n# \u26a0\ufe0f Less clear: flowyml can't validate dependencies\n@step\ndef clean(data):\n    return process(data)\n</code></pre> <p>Why: Explicit <code>inputs</code>/<code>outputs</code> enable DAG visualization, validation, and better error messages.</p>"},{"location":"core/steps/#anatomy-of-a-step","title":"Anatomy of a Step","text":"<p>A step is a Python function with the <code>@step</code> decorator:</p> <pre><code>from flowyml import step\n\n@step(outputs=[\"result\"])\ndef my_step(input_data):\n    # Do some work\n    processed = input_data * 2\n    return processed\n</code></pre>"},{"location":"core/steps/#the-step-decorator","title":"The <code>@step</code> Decorator","text":"<p>The <code>@step</code> decorator accepts several arguments to configure behavior:</p>"},{"location":"core/steps/#configuration-options","title":"Configuration Options","text":"Argument Type Description Default <code>inputs</code> <code>List[str]</code> Names of input assets this step requires <code>[]</code> <code>outputs</code> <code>List[str]</code> Names of output assets this step produces <code>[]</code> <code>cache</code> <code>str \\| bool</code> Caching strategy: <code>\"code_hash\"</code>, <code>\"input_hash\"</code>, or <code>False</code> <code>\"code_hash\"</code> <code>retry</code> <code>int</code> Number of retry attempts on failure <code>0</code> <code>timeout</code> <code>int</code> Maximum execution time in seconds <code>None</code> <code>resources</code> <code>dict</code> Resource requirements (e.g., <code>{\"gpu\": 1}</code>) <code>None</code>"},{"location":"core/steps/#example-with-full-configuration","title":"Example with Full Configuration","text":"<pre><code>@step(\n    inputs=[\"raw_dataset\"],\n    outputs=[\"trained_model\"],\n    cache=\"input_hash\",\n    retry=3,\n    timeout=3600,\n    resources={\"gpu\": 1, \"memory\": \"16Gi\"}\n)\ndef train_model(raw_dataset, learning_rate: float):\n    \"\"\"Train a machine learning model.\"\"\"\n    model = train(raw_dataset, lr=learning_rate)\n    return model\n</code></pre>"},{"location":"core/steps/#inputs-and-outputs","title":"Inputs and Outputs \ud83d\udd0c","text":""},{"location":"core/steps/#defining-dependencies","title":"Defining Dependencies","text":"<p>Steps declare their dependencies through <code>inputs</code> and <code>outputs</code>:</p> <pre><code>@step(outputs=[\"data\"])\ndef load():\n    return [1, 2, 3, 4, 5]\n\n@step(inputs=[\"data\"], outputs=[\"processed\"])\ndef process(data):\n    return [x * 2 for x in data]\n\n# flowyml automatically determines execution order\npipeline = Pipeline(\"etl\")\npipeline.add_step(load)\npipeline.add_step(process)  # Runs after load()\n</code></pre>"},{"location":"core/steps/#how-wiring-works","title":"How Wiring Works","text":"<ol> <li>Step Outputs: When a step completes, its output is stored with the name specified in <code>outputs</code></li> <li>Step Inputs: When aFor the next step, flowyml matches <code>inputs</code> names to stored outputs</li> <li>Auto-Injection: The values are automatically passed as function arguments</li> </ol> <pre><code># After load() completes, output is stored as \"data\"\n# When process() runs, \"data\" is injected as the 'data' parameter\n</code></pre>"},{"location":"core/steps/#multiple-inputs-and-outputs","title":"Multiple Inputs and Outputs \ud83d\udce6","text":""},{"location":"core/steps/#multiple-outputs","title":"Multiple Outputs","text":"<p>A step can return multiple values using tuples:</p> <pre><code>@step(outputs=[\"train_data\", \"test_data\"])\ndef split_data(data, split_ratio=0.8):\n    split_point = int(len(data) * split_ratio)\n    return data[:split_point], data[split_point:]\n</code></pre>"},{"location":"core/steps/#multiple-inputs","title":"Multiple Inputs","text":"<p>A step can depend on multiple previous steps:</p> <pre><code>@step(outputs=[\"data\"])\ndef load_data():\n    return [1, 2, 3]\n\n@step(outputs=[\"labels\"])\ndef load_labels():\n    return [\"a\", \"b\", \"c\"]\n\n@step(inputs=[\"data\", \"labels\"], outputs=[\"dataset\"])\ndef combine(data, labels):\n    return list(zip(data, labels))\n</code></pre>"},{"location":"core/steps/#context-and-parameter-injection","title":"Context and Parameter Injection \ud83e\udde0","text":"<p>Steps can automatically receive parameters from the pipeline's context:</p> <pre><code>from flowyml import Pipeline, context\n\nctx = context(\n    learning_rate=0.001,\n    epochs=100,\n    batch_size=32\n)\n\n@step(outputs=[\"model\"])\ndef train(data, learning_rate: float, epochs: int):\n    # learning_rate and epochs automatically injected from context!\n    print(f\"Training with lr={learning_rate} for {epochs} epochs\")\n    return trained_model\n\npipeline = Pipeline(\"training\", context=ctx)\n</code></pre>"},{"location":"core/steps/#type-hints","title":"Type Hints","text":"<p>Type hints help flowyml match parameters correctly:</p> <pre><code>@step\ndef process(\n    data: list,           # From previous step output\n    threshold: float,     # From context\n    normalize: bool = True  # From context (with default)\n):\n    # flowyml injects context parameters based on names and types\n    ...\n</code></pre> <p>See Context &amp; Parameters for detailed information.</p>"},{"location":"core/steps/#caching-strategies","title":"Caching Strategies \ud83d\udcbe","text":"<p>flowyml supports intelligent caching to avoid re-running expensive steps:</p>"},{"location":"core/steps/#cachecode_hash-default","title":"<code>cache=\"code_hash\"</code> (Default)","text":"<p>Caches based on the step's code. Re-runs only if the function code changes:</p> <pre><code>@step(cache=\"code_hash\")\ndef expensive_computation(data):\n    # Cached unless this function's code changes\n    return complex_calculation(data)\n</code></pre>"},{"location":"core/steps/#cacheinput_hash","title":"<code>cache=\"input_hash\"</code>","text":"<p>Caches based on input values. Re-runs if inputs change:</p> <pre><code>@step(cache=\"input_hash\")\ndef preprocess(data, config):\n    # Re-runs only if data or config changes\n    return clean(data, config)\n</code></pre>"},{"location":"core/steps/#cachefalse","title":"<code>cache=False</code>","text":"<p>Disable caching for this step:</p> <pre><code>@step(cache=False)\ndef fetch_latest_data():\n    # Always runs (e.g., for fetching real-time data)\n    return api.fetch()\n</code></pre> <p>See Caching for more details.</p>"},{"location":"core/steps/#error-handling-and-retries","title":"Error Handling and Retries \ud83d\udd04","text":""},{"location":"core/steps/#automatic-retries","title":"Automatic Retries","text":"<p>Configure retry attempts for flaky operations:</p> <pre><code>@step(retry=3)\ndef fetch_external_data():\n    # Retries up to 3 times on failure\n    response = requests.get(API_URL)\n    return response.json()\n</code></pre>"},{"location":"core/steps/#timeout-protection","title":"Timeout Protection","text":"<p>Set maximum execution time:</p> <pre><code>@step(timeout=300)  # 5 minutes\ndef long_running_task():\n    # Fails if exceeds 5 minutes\n    return expensive_operation()\n</code></pre>"},{"location":"core/steps/#advanced-error-handling","title":"Advanced Error Handling","text":"<p>Use decorators for sophisticated error handling:</p> <pre><code>from flowyml import retry, on_failure, CircuitBreaker\n\ndef fallback_data():\n    return {\"status\": \"unavailable\"}\n\n@step\n@retry(max_attempts=3, backoff=2.0)\n@on_failure(fallback_data)\n@CircuitBreaker(failure_threshold=5)\ndef fetch_data():\n    return external_api.get_data()\n</code></pre> <p>See Error Handling for comprehensive guide.</p>"},{"location":"core/steps/#resource-requirements","title":"Resource Requirements \ud83d\udcaa","text":"<p>Specify compute resources needed for a step:</p> <pre><code># CPU-intensive\n@step(resources={\"cpu\": \"4\", \"memory\": \"16Gi\"})\ndef train_cpu():\n    ...\n\n# GPU-accelerated\n@step(resources={\"gpu\": \"nvidia-tesla-v100\", \"gpu_count\": 2})\ndef train_gpu():\n    ...\n</code></pre> <p>Resources are used by: - Local execution: For monitoring/limiting - Cloud stacks: To provision appropriate instances - Kubernetes: To set pod resources</p>"},{"location":"core/steps/#decision-guide-caching-strategies","title":"Decision Guide: Caching Strategies","text":"Scenario Use Why Expensive computation, stable code <code>code_hash</code> (default) Re-runs only when logic changes Data preprocessing <code>input_hash</code> Re-runs when input data changes Real-time data fetching <code>cache=False</code> Always get latest Model training (hours) <code>input_hash</code> Don't retrain unless data/params change API calls (rate-limited) <code>input_hash</code> Cache responses, respect limits <p>Pro tip: Use <code>input_hash</code> for expensive operations where inputs determine outputs. Use <code>code_hash</code> for pure transformations.</p>"},{"location":"core/steps/#step-testing-patterns","title":"Step Testing Patterns \ud83e\uddea","text":""},{"location":"core/steps/#unit-testing-steps","title":"Unit Testing Steps","text":"<p>Steps are just functions \u2014 test them like functions:</p> <pre><code>import pytest\nfrom my_pipeline import clean_data, train_model\n\ndef test_clean_data():\n    # Arrange\n    raw = pd.DataFrame({\"a\": [1, None, 3]})\n\n    # Act\n    result = clean_data(raw)\n\n    # Assert\n    assert result[\"a\"].isnull().sum() == 0\n    assert len(result) == 2\n\ndef test_train_model():\n    X = np.random.rand(100, 10)\n    y = np.random.rand(100)\n\n    model = train_model(X, y, learning_rate=0.01, epochs=10)\n\n    assert model is not None\n    assert hasattr(model, 'predict')\n</code></pre> <p>Why this works: Steps are pure functions. No pipeline infrastructure needed for unit tests.</p>"},{"location":"core/steps/#integration-testing-steps","title":"Integration Testing Steps","text":"<pre><code>def test_pipeline_integration():\n    from flowyml import Pipeline, context\n\n    ctx = context(learning_rate=0.01)\n    pipeline = Pipeline(\"test_pipeline\", context=ctx)\n    pipeline.add_step(load_data)\n    pipeline.add_step(train_model)\n\n    result = pipeline.run()\n\n    assert result.success\n    assert \"model\" in result.outputs\n</code></pre>"},{"location":"core/steps/#step-execution-lifecycle","title":"Step Execution Lifecycle","text":"<p>Understanding what happens when a step runs:</p> <ol> <li>Validation: Check all required inputs are available</li> <li>Cache Check: Look for cached result matching strategy</li> <li>Execution: Run the step function if not cached</li> <li>Materialization: Save outputs to artifact store (if configured)</li> <li>Result Storage: Store output for downstream steps</li> <li>Lineage: Record provenance metadata</li> </ol> <pre><code># This all happens automatically!\nresult = pipeline.run()\n\n# You get full observability:\nfor step_name, step_result in result.step_results.items():\n    print(f\"{step_name}: {step_result.duration_seconds:.2f}s, cached={step_result.cached}\")\n</code></pre> <p>[!TIP] Performance insight: Viewing cached steps in the UI shows how much time you're saving. A well-cached pipeline can go from hours to minutes during iteration.</p>"},{"location":"core/steps/#best-practices","title":"Best Practices \ud83d\udca1","text":""},{"location":"core/steps/#1-make-steps-pure-functions","title":"1. Make Steps Pure Functions","text":"<p>Steps should be deterministic and side-effect free when possible:</p> <pre><code># \u2705 Good - pure function\n@step\ndef transform(data):\n    return [x * 2 for x in data]\n\n# \u26a0\ufe0f Be careful - side effects\n@step\ndef transform_with_side_effect(data):\n    global counter\n    counter += 1  # Side effect\n    return [x * 2 for x in data]\n</code></pre>"},{"location":"core/steps/#2-name-steps-descriptively","title":"2. Name Steps Descriptively","text":"<pre><code># \u2705 Good\n@step(outputs=[\"cleaned_data\"])\ndef remove_duplicates_and_nulls(raw_data):\n    ...\n\n# \u274c Bad\n@step(outputs=[\"data\"])\ndef process(data):\n    ...\n</code></pre>"},{"location":"core/steps/#3-keep-steps-focused","title":"3. Keep Steps Focused","text":"<p>Each step should do one thing well:</p> <pre><code># \u2705 Good - separate concerns\n@step(outputs=[\"cleaned\"])\ndef clean_data(raw):\n    ...\n\n@step(inputs=[\"cleaned\"], outputs=[\"features\"])\ndef engineer_features(cleaned):\n    ...\n\n# \u274c Bad - doing too much\n@step\ndef clean_and_engineer_and_train(raw):\n    cleaned = clean(raw)\n    features = engineer(cleaned)\n    model = train(features)\n    return model\n</code></pre>"},{"location":"core/steps/#4-use-type-hints","title":"4. Use Type Hints","text":"<p>Type hints improve code clarity and enable better IDE support:</p> <pre><code>from typing import List, Dict\nimport pandas as pd\n\n@step(outputs=[\"dataframe\"])\ndef load_data(path: str) -&gt; pd.DataFrame:\n    return pd.read_csv(path)\n</code></pre>"},{"location":"core/steps/#5-document-your-steps","title":"5. Document Your Steps","text":"<pre><code>@step(inputs=[\"features\"], outputs=[\"predictions\"])\ndef predict(features: pd.DataFrame, threshold: float = 0.5) -&gt; pd.Series:\n    \"\"\"Generate predictions from features.\n\n    Args:\n        features: Input feature matrix\n        threshold: Classification threshold\n\n    Returns:\n        Binary predictions\n    \"\"\"\n    ...\n</code></pre>"},{"location":"core/steps/#advanced-step-patterns","title":"Advanced Step Patterns","text":""},{"location":"core/steps/#conditional-execution","title":"Conditional Execution","text":"<pre><code>from flowyml import when, unless\n\n@step\n@when(lambda x: x &gt; 100)\ndef expensive_path(data):\n    # Only runs if data &gt; 100\n    ...\n\n@step\n@unless(lambda x: x is None)\ndef process_if_exists(data):\n    # Skips if data is None\n    ...\n</code></pre>"},{"location":"core/steps/#parallel-processing","title":"Parallel Processing","text":"<pre><code>from flowyml import parallel_map\n\n@step\ndef process_items(items: List):\n    # Process items in parallel\n    return parallel_map(heavy_function, items, num_workers=4)\n</code></pre>"},{"location":"core/steps/#dynamic-step-generation","title":"Dynamic Step Generation","text":"<pre><code>def create_training_step(model_type: str):\n    @step(outputs=[f\"{model_type}_model\"])\n    def train():\n        return train_model(model_type)\n    return train\n\n# Create multiple training steps\nfor model_type in [\"xgboost\", \"random_forest\", \"neural_net\"]:\n    step_func = create_training_step(model_type)\n    pipeline.add_step(step_func)\n</code></pre>"},{"location":"core/steps/#next-steps","title":"Next Steps \ud83d\udcda","text":"<ul> <li>Pipelines: Connect steps into workflows</li> <li>Context: Master parameter injection</li> <li>Caching: Understand caching strategies</li> <li>Error Handling: Advanced error handling patterns</li> </ul>"},{"location":"guides/custom-components/","title":"Creating Custom Stack Components for flowyml","text":"<p>This guide shows you how to create custom stack components and integrate them seamlessly into flowyml.</p>"},{"location":"guides/custom-components/#quick-start","title":"Quick Start","text":""},{"location":"guides/custom-components/#1-create-a-custom-component","title":"1. Create a Custom Component","text":"<pre><code># my_components.py\nfrom flowyml.stacks.components import Orchestrator\nfrom flowyml.stacks.plugins import register_component\n\n@register_component\nclass MyOrchestrator(Orchestrator):\n    \"\"\"My custom orchestrator.\"\"\"\n\n    def __init__(self, name: str = \"my_orchestrator\", **kwargs):\n        super().__init__(name)\n        # Your initialization\n\n    def validate(self) -&gt; bool:\n        # Validation logic\n        return True\n\n    def run_pipeline(self, pipeline, **kwargs):\n        # Execution logic\n        pass\n\n    def get_run_status(self, run_id: str) -&gt; str:\n        # Status checking\n        return \"SUCCESS\"\n\n    def to_dict(self):\n        return {\"name\": self.name, \"type\": \"my_orchestrator\"}\n</code></pre>"},{"location":"guides/custom-components/#2-use-in-configuration","title":"2. Use in Configuration","text":"<pre><code># flowyml.yaml\nstacks:\n  my_stack:\n    type: local\n    orchestrator:\n      type: my_orchestrator\n      # Your custom config\n</code></pre>"},{"location":"guides/custom-components/#3-load-component","title":"3. Load Component","text":"<pre><code># Auto-loads from my_components.py if in PYTHONPATH\nflowyml run pipeline.py --stack my_stack\n\n# Or explicitly load\nflowyml component load my_components\n</code></pre>"},{"location":"guides/custom-components/#creating-components","title":"Creating Components","text":""},{"location":"guides/custom-components/#base-classes","title":"Base Classes","text":"<p>flowyml provides these base classes:</p> <pre><code>from flowyml.stacks.components import (\n    Orchestrator,          # For pipeline orchestration\n    ArtifactStore,        # For artifact storage\n    ContainerRegistry,    # For Docker images\n    StackComponent,       # Generic component\n)\n</code></pre>"},{"location":"guides/custom-components/#orchestrator-example","title":"Orchestrator Example","text":"<pre><code>from flowyml.stacks.components import Orchestrator\nfrom flowyml.stacks.plugins import register_component\n\n@register_component\nclass AirflowOrchestrator(Orchestrator):\n    def __init__(self, name=\"airflow\", dag_folder=\"~/airflow/dags\"):\n        super().__init__(name)\n        self.dag_folder = dag_folder\n\n    def validate(self) -&gt; bool:\n        try:\n            import airflow\n            return True\n        except ImportError:\n            raise ImportError(\"pip install apache-airflow\")\n\n    def run_pipeline(self, pipeline, **kwargs):\n        # Convert to Airflow DAG\n        from airflow import DAG\n        # ... implementation\n        return \"dag_run_id\"\n\n    def get_run_status(self, run_id: str) -&gt; str:\n        # Check Airflow DAG run status\n        return \"RUNNING\"\n\n    def to_dict(self):\n        return {\n            \"type\": \"airflow\",\n            \"dag_folder\": self.dag_folder\n        }\n</code></pre>"},{"location":"guides/custom-components/#artifact-store-example","title":"Artifact Store Example","text":"<pre><code>from flowyml.stacks.components import ArtifactStore\nfrom flowyml.stacks.plugins import register_component\n\n@register_component\nclass MinIOArtifactStore(ArtifactStore):\n    def __init__(\n        self,\n        name=\"minio\",\n        endpoint=\"localhost:9000\",\n        bucket=\"flowyml\"\n    ):\n        super().__init__(name)\n        self.endpoint = endpoint\n        self.bucket = bucket\n\n    def validate(self) -&gt; bool:\n        from minio import Minio\n        client = Minio(self.endpoint)\n        return client.bucket_exists(self.bucket)\n\n    def save(self, artifact, path: str) -&gt; str:\n        # Save to MinIO\n        return f\"s3://{self.bucket}/{path}\"\n\n    def load(self, path: str):\n        # Load from MinIO\n        pass\n\n    def exists(self, path: str) -&gt; bool:\n        # Check existence\n        return True\n\n    def to_dict(self):\n        return {\n            \"type\": \"minio\",\n            \"endpoint\": self.endpoint,\n            \"bucket\": self.bucket\n        }\n</code></pre>"},{"location":"guides/custom-components/#registration-methods","title":"Registration Methods","text":""},{"location":"guides/custom-components/#method-1-decorator-recommended","title":"Method 1: Decorator (Recommended)","text":"<pre><code>from flowyml.stacks.plugins import register_component\n\n@register_component\nclass MyComponent(Orchestrator):\n    pass\n\n# Or with custom name\n@register_component(name=\"custom_name\")\nclass MyComponent(Orchestrator):\n    pass\n</code></pre>"},{"location":"guides/custom-components/#method-2-manual-registration","title":"Method 2: Manual Registration","text":"<pre><code>from flowyml.stacks.plugins import get_component_registry\n\nclass MyComponent(Orchestrator):\n    pass\n\nregistry = get_component_registry()\nregistry.register(MyComponent, \"my_component\")\n</code></pre>"},{"location":"guides/custom-components/#method-3-entry-points-for-packages","title":"Method 3: Entry Points (for packages)","text":"<p>In your <code>pyproject.toml</code>:</p> <pre><code>[project.entry-points.\"flowyml.stack_components\"]\nmy_orchestrator = \"my_package.components:MyOrchestrator\"\nmy_artifact_store = \"my_package.components:MyArtifactStore\"\n</code></pre> <p>Then components auto-load when package is installed!</p>"},{"location":"guides/custom-components/#loading-components","title":"Loading Components","text":""},{"location":"guides/custom-components/#from-module","title":"From Module","text":"<pre><code>from flowyml.stacks.plugins import load_component\n\n# Load all components from module\nload_component(\"my_package.components\")\n</code></pre>"},{"location":"guides/custom-components/#from-file","title":"From File","text":"<pre><code># Load specific class from file\nload_component(\"/path/to/component.py:MyOrchestrator\")\n</code></pre>"},{"location":"guides/custom-components/#from-configuration","title":"From Configuration","text":"<pre><code># flowyml.yaml\ncomponents:\n  - module: my_package.components\n  - file: /path/to/custom.py:CustomComponent\n</code></pre>"},{"location":"guides/custom-components/#using-zenml-components","title":"Using ZenML Components","text":""},{"location":"guides/custom-components/#wrap-zenml-component","title":"Wrap ZenML Component","text":"<pre><code>from flowyml.stacks.plugins import get_component_registry\n\n# Load ZenML component\nregistry = get_component_registry()\nregistry.wrap_zenml_component(\n    zenml_component_class=KubernetesOrchestrator,\n    name=\"k8s\"\n)\n</code></pre>"},{"location":"guides/custom-components/#via-configuration","title":"Via Configuration","text":"<pre><code># flowyml.yaml\ncomponents:\n  - zenml: zenml.orchestrators.kubernetes.KubernetesOrchestrator\n    name: k8s\n\nstacks:\n  k8s_stack:\n    orchestrator:\n      type: k8s\n      # ZenML config\n</code></pre>"},{"location":"guides/custom-components/#using-the-cli","title":"Using the CLI","text":"<pre><code># Load ZenML component\nflowyml component load zenml:zenml.orchestrators.kubernetes.KubernetesOrchestrator\n\n# List available components\nflowyml component list\n</code></pre>"},{"location":"guides/custom-components/#component-discovery","title":"Component Discovery","text":"<p>flowyml automatically discovers components from:</p> <ol> <li>Installed packages with entry points</li> <li><code>PYTHONPATH</code> - any module in path</li> <li>Current directory - <code>./components/</code> folder</li> <li>Configuration - <code>flowyml.yaml</code> components section</li> </ol>"},{"location":"guides/custom-components/#publishing-components","title":"Publishing Components","text":""},{"location":"guides/custom-components/#as-python-package","title":"As Python Package","text":"<pre><code># setup.py or pyproject.toml\n[project.entry-points.\"flowyml.stack_components\"]\nmy_orchestrator = \"my_flowyml_plugin:MyOrchestrator\"\n</code></pre> <pre><code>pip install my-flowyml-plugin\n\n# Auto-available in flowyml!\nflowyml component list\n</code></pre>"},{"location":"guides/custom-components/#as-module","title":"As Module","text":"<pre><code># Add to PYTHONPATH\nexport PYTHONPATH=/path/to/components:$PYTHONPATH\n\n# Or install in development mode\npip install -e /path/to/my-components\n</code></pre>"},{"location":"guides/custom-components/#community-components","title":"Community Components","text":""},{"location":"guides/custom-components/#using-existing-components","title":"Using Existing Components","text":"<pre><code># Install from PyPI\npip install flowyml-airflow-orchestrator\npip install flowyml-minio-store\n\n# Automatically available!\n</code></pre>"},{"location":"guides/custom-components/#creating-shareable-components","title":"Creating Shareable Components","text":"<pre><code>my-flowyml-components/\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 my_flowyml_components/\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 airflow_orchestrator.py\n    \u2514\u2500\u2500 minio_store.py\n</code></pre> <pre><code># pyproject.toml\n[project]\nname = \"my-flowyml-components\"\nversion = \"0.1.0\"\n\n[project.entry-points.\"flowyml.stack_components\"]\nairflow = \"my_flowyml_components.airflow_orchestrator:AirflowOrchestrator\"\nminio = \"my_flowyml_components.minio_store:MinIOArtifactStore\"\n</code></pre>"},{"location":"guides/custom-components/#advanced-zenml-integration","title":"Advanced: ZenML Integration","text":""},{"location":"guides/custom-components/#complete-zenml-compatibility","title":"Complete ZenML Compatibility","text":"<pre><code>from zenml.stack import Stack as ZenMLStack\nfrom flowyml.stacks.plugins import get_component_registry\n\n# Import all ZenML components\ndef import_zenml_stack(zenml_stack: ZenMLStack):\n    registry = get_component_registry()\n\n    # Wrap each component\n    registry.wrap_zenml_component(\n        zenml_stack.orchestrator,\n        \"zenml_orchestrator\"\n    )\n\n    registry.wrap_zenml_component(\n        zenml_stack.artifact_store,\n        \"zenml_artifact_store\"\n    )\n\n    # Now use in flowyml!\n</code></pre>"},{"location":"guides/custom-components/#gradual-migration-from-zenml","title":"Gradual Migration from ZenML","text":"<pre><code># Keep using ZenML components\nfrom zenml.integrations.kubernetes import KubernetesOrchestrator\n\n# Use in flowyml\nfrom flowyml.stacks import Stack\nfrom flowyml.stacks.plugins import get_component_registry\n\nregistry = get_component_registry()\nregistry.wrap_zenml_component(KubernetesOrchestrator, \"k8s\")\n\n# Create flowyml stack with ZenML component\nstack = Stack(\n    name=\"hybrid\",\n    orchestrator=registry.get_orchestrator(\"k8s\"),\n    # ... other components\n)\n</code></pre>"},{"location":"guides/custom-components/#examples","title":"Examples","text":"<p>See: - <code>examples/custom_components/my_components.py</code> - Complete examples - <code>examples/custom_components/airflow_integration.py</code> - Airflow integration - <code>examples/custom_components/minio_integration.py</code> - MinIO integration</p>"},{"location":"guides/custom-components/#best-practices","title":"Best Practices","text":"<ol> <li>\u2705 Use <code>@register_component</code> decorator</li> <li>\u2705 Implement all required methods</li> <li>\u2705 Add comprehensive validation</li> <li>\u2705 Include good documentation</li> <li>\u2705 Provide configuration examples</li> <li>\u2705 Test thoroughly</li> <li>\u2705 Publish as package for reuse</li> </ol>"},{"location":"guides/custom-components/#testing-custom-components","title":"Testing Custom Components","text":"<pre><code>import unittest\nfrom my_components import MyOrchestrator\n\nclass TestMyOrchestrator(unittest.TestCase):\n    def test_validation(self):\n        orch = MyOrchestrator()\n        self.assertTrue(orch.validate())\n\n    def test_run_pipeline(self):\n        # Test pipeline execution\n        pass\n</code></pre>"},{"location":"guides/custom-components/#next-steps","title":"Next Steps","text":"<ul> <li>Create your custom component</li> <li>Test locally</li> <li>Publish to PyPI</li> <li>Share with community!</li> </ul>"},{"location":"guides/improved-ux/","title":"Improved UX Guide - Configuration-Driven Infrastructure","text":""},{"location":"guides/improved-ux/#overview","title":"Overview","text":"<p>flowyml now supports complete separation between pipeline logic and infrastructure configuration. Your pipeline code remains clean and infrastructure-agnostic.</p>"},{"location":"guides/improved-ux/#key-improvements","title":"Key Improvements","text":""},{"location":"guides/improved-ux/#configuration-driven","title":"\u2728 Configuration-Driven","text":"<p>All infrastructure is defined in <code>flowyml.yaml</code>:</p> <pre><code>stacks:\n  local:\n    type: local\n\n  production:\n    type: gcp\n    project_id: ${GCP_PROJECT_ID}\n    bucket_name: ${GCP_BUCKET}\n\nresources:\n  gpu_training:\n    cpu: \"8\"\n    memory: \"32Gi\"\n    gpu: \"nvidia-tesla-v100\"\n</code></pre>"},{"location":"guides/improved-ux/#cli-based-execution","title":"\ud83c\udfaf CLI-Based Execution","text":"<p>Run the same pipeline on different stacks without code changes:</p> <pre><code># Development\nflowyml run pipeline.py\n\n# Production\nflowyml run pipeline.py --stack production\n\n# With GPU resources\nflowyml run pipeline.py --stack production --resources gpu_training\n</code></pre>"},{"location":"guides/improved-ux/#auto-detection","title":"\ud83d\udce6 Auto-Detection","text":"<p>flowyml automatically detects: - \u2705 Existing <code>Dockerfile</code> - \u2705 <code>pyproject.toml</code> for Poetry - \u2705 <code>requirements.txt</code> - \u2705 Environment variables from <code>.env</code></p>"},{"location":"guides/improved-ux/#clean-separation","title":"\ud83d\udd12 Clean Separation","text":"<p>Before (Tightly Coupled): <pre><code>from flowyml.stacks.gcp import GCPStack\nfrom flowyml.stacks.components import ResourceConfig, DockerConfig\n\n# Infrastructure hardcoded in pipeline!\nstack = GCPStack(project_id=\"...\", bucket_name=\"...\")\nresources = ResourceConfig(cpu=\"8\", memory=\"32Gi\")\ndocker = DockerConfig(image=\"...\")\n\npipeline = Pipeline(\"my_pipeline\", stack=stack)\nresult = pipeline.run(resources=resources, docker=docker)\n</code></pre></p> <p>After (Decoupled): <pre><code># Pure pipeline logic - NO infrastructure!\npipeline = Pipeline(\"my_pipeline\")\nresult = pipeline.run()\n\n# Infrastructure configured externally via:\n# - flowyml.yaml\n# - CLI flags\n# - Environment variables\n</code></pre></p>"},{"location":"guides/improved-ux/#quick-start","title":"Quick Start","text":""},{"location":"guides/improved-ux/#1-initialize-configuration","title":"1. Initialize Configuration","text":"<pre><code>flowyml init\n</code></pre> <p>Creates <code>flowyml.yaml</code> with sensible defaults.</p>"},{"location":"guides/improved-ux/#2-configure-stacks","title":"2. Configure Stacks","text":"<p>Edit <code>flowyml.yaml</code>:</p> <pre><code>stacks:\n  local:\n    type: local\n\n  staging:\n    type: gcp\n    project_id: my-project-staging\n    region: us-central1\n\n  production:\n    type: gcp\n    project_id: my-project-prod\n    region: us-central1\n</code></pre>"},{"location":"guides/improved-ux/#3-write-clean-pipelines","title":"3. Write Clean Pipelines","text":"<pre><code># pipeline.py\nfrom flowyml import Pipeline, step\n\n@step\ndef process_data(input_path: str):\n    # Your logic here\n    return {\"result\": \"processed\"}\n\npipeline = Pipeline(\"data_processing\")\npipeline.add_step(process_data)\n</code></pre>"},{"location":"guides/improved-ux/#4-run-anywhere","title":"4. Run Anywhere","text":"<pre><code># Development\nflowyml run pipeline.py --context input_path=local/data.csv\n\n# Staging\nflowyml run pipeline.py --stack staging --context input_path=gs://staging/data.csv\n\n# Production\nflowyml run pipeline.py --stack production --context input_path=gs://prod/data.csv\n</code></pre>"},{"location":"guides/improved-ux/#environment-variables","title":"Environment Variables","text":"<p>Use <code>.env</code> file for secrets:</p> <pre><code># .env\nGCP_PROJECT_ID=my-project\nGCP_BUCKET=my-artifacts\nGCP_SERVICE_ACCOUNT=my-sa@project.iam.gserviceaccount.com\n</code></pre> <p>Reference in <code>flowyml.yaml</code>:</p> <pre><code>stacks:\n  production:\n    project_id: ${GCP_PROJECT_ID}\n    bucket_name: ${GCP_BUCKET}\n</code></pre>"},{"location":"guides/improved-ux/#docker-integration","title":"Docker Integration","text":""},{"location":"guides/improved-ux/#option-1-existing-dockerfile","title":"Option 1: Existing Dockerfile","text":"<p>flowyml automatically uses it:</p> <pre><code>docker:\n  dockerfile: ./Dockerfile\n  build_context: .\n</code></pre>"},{"location":"guides/improved-ux/#option-2-poetry","title":"Option 2: Poetry","text":"<p>Uses <code>pyproject.toml</code>:</p> <pre><code>docker:\n  use_poetry: true\n  base_image: python:3.11-slim\n</code></pre>"},{"location":"guides/improved-ux/#option-3-requirements-file","title":"Option 3: Requirements File","text":"<pre><code>docker:\n  requirements_file: requirements.txt\n  base_image: python:3.11-slim\n</code></pre>"},{"location":"guides/improved-ux/#cli-commands","title":"CLI Commands","text":""},{"location":"guides/improved-ux/#stack-management","title":"Stack Management","text":"<pre><code># List configured stacks\nflowyml stack list\n\n# Show stack details\nflowyml stack show production\n\n# Set default stack\nflowyml stack set-default production\n</code></pre>"},{"location":"guides/improved-ux/#running-pipelines","title":"Running Pipelines","text":"<pre><code># Basic run\nflowyml run pipeline.py\n\n# Specify stack\nflowyml run pipeline.py --stack production\n\n# Specify resources\nflowyml run pipeline.py --resources gpu_training\n\n# Pass context\nflowyml run pipeline.py --context key1=value1 --context key2=value2\n\n# Dry run (show configuration)\nflowyml run pipeline.py --stack production --dry-run\n\n# Custom config file\nflowyml run pipeline.py --config custom.yaml\n</code></pre>"},{"location":"guides/improved-ux/#benefits","title":"Benefits","text":""},{"location":"guides/improved-ux/#for-data-scientists","title":"\ud83c\udfaf For Data Scientists","text":"<ul> <li>Write pure pipeline logic</li> <li>No infrastructure code</li> <li>Same code, multiple environments</li> <li>Easy testing locally</li> </ul>"},{"location":"guides/improved-ux/#for-mlops-engineers","title":"\ud83c\udfd7\ufe0f For MLOps Engineers","text":"<ul> <li>Centralized infrastructure config</li> <li>Version control for infra</li> <li>Easy environment management</li> <li>Security via environment variables</li> </ul>"},{"location":"guides/improved-ux/#for-teams","title":"\ud83d\udc65 For Teams","text":"<ul> <li>Consistent deployment</li> <li>Easy collaboration</li> <li>Clear separation of concerns</li> <li>Reduced merge conflicts</li> </ul>"},{"location":"guides/improved-ux/#migration-guide","title":"Migration Guide","text":""},{"location":"guides/improved-ux/#old-style-coupled","title":"Old Style (Coupled)","text":"<pre><code>from flowyml import Pipeline\nfrom flowyml.stacks.gcp import GCPStack\n\nstack = GCPStack(\n    project_id=\"my-project\",\n    bucket_name=\"my-bucket\"\n)\n\npipeline = Pipeline(\"my_pipeline\", stack=stack)\n</code></pre>"},{"location":"guides/improved-ux/#new-style-decoupled","title":"New Style (Decoupled)","text":"<ol> <li> <p>Create <code>flowyml.yaml</code>: <pre><code>stacks:\n  production:\n    type: gcp\n    project_id: my-project\n    bucket_name: my-bucket\n</code></pre></p> </li> <li> <p>Simplify pipeline: <pre><code>from flowyml import Pipeline\n\npipeline = Pipeline(\"my_pipeline\")\n# Stack loaded from flowyml.yaml\n</code></pre></p> </li> <li> <p>Run with CLI: <pre><code>flowyml run pipeline.py --stack production\n</code></pre></p> </li> </ol>"},{"location":"guides/improved-ux/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"guides/improved-ux/#per-step-resources","title":"Per-Step Resources","text":"<pre><code>resources:\n  preprocessing:\n    cpu: \"2\"\n    memory: \"8Gi\"\n\n  training:\n    cpu: \"16\"\n    memory: \"64Gi\"\n    gpu: \"nvidia-tesla-v100\"\n\n  inference:\n    cpu: \"4\"\n    memory: \"16Gi\"\n</code></pre>"},{"location":"guides/improved-ux/#multi-region-deployment","title":"Multi-Region Deployment","text":"<pre><code>stacks:\n  us-prod:\n    type: gcp\n    region: us-central1\n    bucket_name: us-artifacts\n\n  eu-prod:\n    type: gcp\n    region: europe-west1\n    bucket_name: eu-artifacts\n</code></pre>"},{"location":"guides/improved-ux/#environment-specific-configs","title":"Environment-Specific Configs","text":"<pre><code># dev.yaml\nstacks:\n  dev:\n    type: local\n\n# prod.yaml\nstacks:\n  prod:\n    type: gcp\n</code></pre> <pre><code># Development\nflowyml run pipeline.py --config dev.yaml\n\n# Production\nflowyml run pipeline.py --config prod.yaml\n</code></pre>"},{"location":"guides/improved-ux/#best-practices","title":"Best Practices","text":"<ol> <li>\u2705 Never hardcode infrastructure in pipeline code</li> <li>\u2705 Use flowyml.yaml for all stack configuration</li> <li>\u2705 Use environment variables for secrets</li> <li>\u2705 Define resource presets for common workloads</li> <li>\u2705 Version control flowyml.yaml (without secrets)</li> <li>\u2705 Use .env for local development secrets</li> <li>\u2705 Document required environment variables</li> </ol>"},{"location":"guides/improved-ux/#next-steps","title":"Next Steps","text":"<ul> <li>See examples/clean_pipeline.py for a complete example</li> <li>Read Stack Architecture for deep dive</li> <li>Check GCP Stack Guide for cloud deployment</li> </ul>"},{"location":"integrations/aws/","title":"Amazon Web Services (AWS) \u2601\ufe0f","text":"<p>Deploy your flowyml pipelines to the world's leading cloud provider.</p> <p>[!NOTE] What you'll learn: How to use S3 for storage and SageMaker for execution</p> <p>Key insight: flowyml abstracts away the complexity of Boto3 and AWS SDKs.</p>"},{"location":"integrations/aws/#why-use-aws-with-flowyml","title":"Why Use AWS with flowyml?","text":"<ul> <li>S3 Durability: 99.999999999% durability for your model artifacts.</li> <li>SageMaker Power: Access specialized ML instances (Trainium, Inferentia).</li> <li>IAM Security: Granular access control for your data.</li> </ul>"},{"location":"integrations/aws/#s3-artifact-store","title":"\ud83e\udea3 S3 Artifact Store","text":"<p>Use Amazon S3 as the backend for all your pipeline artifacts.</p>"},{"location":"integrations/aws/#configuration","title":"Configuration","text":"<pre><code># Register an AWS stack\nflowyml stack register aws-prod \\\n    --artifact-store s3://my-bucket/flowyml-artifacts \\\n    --metadata-store sqlite:///flowyml.db\n</code></pre>"},{"location":"integrations/aws/#sagemaker-execution","title":"\ud83e\udde0 SageMaker Execution","text":"<p>Run your pipelines on Amazon SageMaker Processing Jobs or Training Jobs.</p>"},{"location":"integrations/aws/#real-world-pattern-scale-to-cloud","title":"Real-World Pattern: Scale to Cloud","text":"<pre><code>from flowyml import Pipeline\nfrom flowyml.integrations.aws import SageMakerOrchestrator\n\npipeline = Pipeline(\"aws_pipeline\")\n\n# Run on SageMaker\npipeline.run(\n    orchestrator=SageMakerOrchestrator(\n        role_arn=\"arn:aws:iam::123456789012:role/SageMakerRole\",\n        instance_type=\"ml.m5.xlarge\",\n        region_name=\"us-east-1\"\n    )\n)\n</code></pre>"},{"location":"integrations/aws/#authentication","title":"\ud83d\udd10 Authentication","text":"<p>flowyml uses the standard AWS credential chain. 1. Environment variables (<code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>) 2. <code>~/.aws/credentials</code> file 3. IAM Role (if running on EC2/Lambda)</p>"},{"location":"integrations/azure/","title":"Microsoft Azure \u2601\ufe0f","text":"<p>Enterprise-grade ML pipelines on Azure.</p> <p>[!NOTE] What you'll learn: How to integrate flowyml with Azure Blob Storage and Azure ML</p> <p>Key insight: Seamlessly move from local development to Azure's secure cloud environment.</p>"},{"location":"integrations/azure/#why-use-azure-with-flowyml","title":"Why Use Azure with flowyml?","text":"<ul> <li>Enterprise Security: Integrate with Azure Active Directory.</li> <li>Blob Storage: Cost-effective storage for massive datasets.</li> <li>Azure ML: Managed compute clusters for training and inference.</li> </ul>"},{"location":"integrations/azure/#azure-blob-storage","title":"\ud83d\udce6 Azure Blob Storage","text":"<p>Store artifacts in Azure Blob Storage containers.</p>"},{"location":"integrations/azure/#configuration","title":"Configuration","text":"<pre><code># Register an Azure stack\nflowyml stack register azure-prod \\\n    --artifact-store az://my-container/flowyml-artifacts \\\n    --metadata-store sqlite:///flowyml.db\n</code></pre>"},{"location":"integrations/azure/#azure-ml-execution","title":"\ud83d\ude80 Azure ML Execution","text":"<p>Execute steps as Azure ML Jobs.</p>"},{"location":"integrations/azure/#real-world-pattern-cloud-training","title":"Real-World Pattern: Cloud Training","text":"<pre><code>from flowyml import Pipeline\nfrom flowyml.integrations.azure import AzureMLOrchestrator\n\npipeline = Pipeline(\"azure_pipeline\")\n\n# Run on Azure ML\npipeline.run(\n    orchestrator=AzureMLOrchestrator(\n        subscription_id=\"&lt;subscription_id&gt;\",\n        resource_group=\"&lt;resource_group&gt;\",\n        workspace_name=\"&lt;workspace_name&gt;\",\n        compute_target=\"gpu-cluster\"\n    )\n)\n</code></pre>"},{"location":"integrations/azure/#authentication","title":"\ud83d\udd10 Authentication","text":"<p>flowyml supports: 1. <code>DefaultAzureCredential</code> (Environment vars, Managed Identity, CLI) 2. Service Principal authentication</p>"},{"location":"integrations/docker/","title":"Docker Integration \ud83d\udc33","text":"<p>Containerize your pipelines for reproducible execution anywhere.</p> <p>[!NOTE] What you'll learn: How to run pipelines in isolated Docker containers</p> <p>Key insight: Eliminate \"it works on my machine\" bugs forever.</p>"},{"location":"integrations/docker/#why-use-docker","title":"Why Use Docker?","text":"<ul> <li>Isolation: Each step runs in its own clean environment.</li> <li>Reproducibility: The exact same code and dependencies run in dev, staging, and prod.</li> <li>Portability: Move from local Docker to Kubernetes or Cloud without code changes.</li> </ul>"},{"location":"integrations/docker/#running-on-docker","title":"\ud83d\udc33 Running on Docker","text":"<p>flowyml can automatically build and run your steps in Docker containers.</p>"},{"location":"integrations/docker/#configuration","title":"Configuration","text":"<pre><code>from flowyml.integrations.docker import DockerOrchestrator\n\n# Run pipeline in Docker\npipeline.run(\n    orchestrator=DockerOrchestrator(\n        image=\"python:3.9-slim\",  # Base image\n        install_deps=True         # Auto-install requirements.txt\n    )\n)\n</code></pre>"},{"location":"integrations/docker/#custom-dockerfiles","title":"\ud83d\udee0 Custom Dockerfiles","text":"<p>For complex dependencies, provide your own Dockerfile.</p> <pre><code>orchestrator = DockerOrchestrator(\n    dockerfile=\"./Dockerfile\",\n    build_context=\".\"\n)\n</code></pre>"},{"location":"integrations/docker/#example-dockerfile","title":"Example Dockerfile","text":"<pre><code>FROM python:3.9\nRUN apt-get update &amp;&amp; apt-get install -y gcc\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\nCOPY . /app\nWORKDIR /app\n</code></pre>"},{"location":"integrations/gcp/","title":"Google Cloud Platform (GCP) \u2601\ufe0f","text":"<p>Scale your pipelines from local prototypes to production workloads on Google Cloud.</p> <p>[!NOTE] What you'll learn: How to run flowyml pipelines on Vertex AI and store data in GCS</p> <p>Key insight: Develop locally on your laptop, then flip a switch to run on a 100-GPU cluster in the cloud.</p>"},{"location":"integrations/gcp/#why-use-gcp-with-flowyml","title":"Why Use GCP with flowyml?","text":"<p>Local limitations: - Memory: \"OOM Error\" on large datasets - Compute: Training takes days on a CPU - Storage: Hard drive full of model checkpoints</p> <p>GCP advantages: - Infinite Scale: Spin up as many machines as you need - Managed Services: Vertex AI handles the infrastructure - Unified Data: Store everything in GCS, accessible from anywhere</p>"},{"location":"integrations/gcp/#gcs-artifact-store","title":"\u2601\ufe0f GCS Artifact Store","text":"<p>Store your pipeline artifacts (datasets, models) in Google Cloud Storage. This makes them accessible to your team and production systems.</p>"},{"location":"integrations/gcp/#configuration","title":"Configuration","text":"<pre><code># Register a stack that uses GCS\nflowyml stack register gcp-prod \\\n    --artifact-store gs://my-bucket/flowyml-artifacts \\\n    --metadata-store sqlite:///flowyml.db\n</code></pre>"},{"location":"integrations/gcp/#vertex-ai-execution","title":"\ud83d\ude80 Vertex AI Execution","text":"<p>Run your pipeline steps as Vertex AI Custom Jobs. flowyml handles the Dockerization and submission automatically.</p>"},{"location":"integrations/gcp/#real-world-pattern-hybrid-execution","title":"Real-World Pattern: Hybrid Execution","text":"<p>Develop locally, then deploy to Vertex AI for the heavy lifting.</p> <pre><code>from flowyml import Pipeline\nfrom flowyml.integrations.gcp import VertexAIOrchestrator\n\npipeline = Pipeline(\"training_pipeline\")\n# ... add steps ...\n\n# Option 1: Run locally for debugging\n# pipeline.run()\n\n# Option 2: Run on Vertex AI for production\npipeline.run(\n    orchestrator=VertexAIOrchestrator(\n        project=\"my-gcp-project\",\n        location=\"us-central1\",\n        machine_type=\"n1-standard-16\", # Powerful machine!\n        accelerator_type=\"NVIDIA_TESLA_T4\",\n        accelerator_count=1\n    )\n)\n</code></pre> <p>[!TIP] Cost Control: Vertex AI charges by the second. flowyml ensures resources are only provisioned while your steps are running.</p>"},{"location":"integrations/huggingface/","title":"Hugging Face Integration \ud83e\udd17","text":"<p>Build state-of-the-art NLP and Vision pipelines with Transformers and flowyml.</p> <p>[!NOTE] What you'll learn: How to manage Transformers models and datasets</p> <p>Key insight: Treat Hugging Face models as first-class citizens in your pipeline.</p>"},{"location":"integrations/huggingface/#why-hugging-face-flowyml","title":"Why Hugging Face + flowyml?","text":"<ul> <li>Model Management: Version control large Transformer models efficiently.</li> <li>Dataset Lineage: Track exactly which version of a dataset was used for fine-tuning.</li> <li>Easy Deployment: Move from fine-tuning to inference seamlessly.</li> </ul>"},{"location":"integrations/huggingface/#transformers","title":"\ud83e\udd17 Transformers","text":"<p>Fine-tune models with full lineage tracking.</p> <pre><code>from flowyml import step\nfrom transformers import Trainer, TrainingArguments\n\n@step\ndef fine_tune(model, dataset):\n    args = TrainingArguments(output_dir=\"./results\", num_train_epochs=3)\n\n    trainer = Trainer(\n        model=model,\n        args=args,\n        train_dataset=dataset[\"train\"]\n    )\n\n    trainer.train()\n    return trainer.model\n</code></pre>"},{"location":"integrations/huggingface/#datasets","title":"\ud83d\udcda Datasets","text":"<p>Load and version datasets.</p> <pre><code>from datasets import load_dataset\nfrom flowyml import step\n\n@step\ndef get_data():\n    # This dataset artifact will be versioned\n    return load_dataset(\"imdb\")\n</code></pre>"},{"location":"integrations/keras/","title":"Keras Integration \ud83e\udde0","text":"<p>Deep learning for humans, orchestrated by flowyml.</p> <p>[!NOTE] What you'll learn: How to track Keras training runs automatically</p> <p>Key insight: Add one callback, get full experiment tracking for free.</p>"},{"location":"integrations/keras/#why-keras-flowyml","title":"Why Keras + flowyml?","text":"<ul> <li>Zero-Boilerplate Tracking: No need to write <code>log_metric</code> loops.</li> <li>Model Versioning: Every <code>model.fit()</code> produces a versioned artifact.</li> <li>Reproducibility: Capture exact hyperparameters and architecture.</li> </ul>"},{"location":"integrations/keras/#flowymlkerascallback","title":"\ud83e\udde0 flowymlKerasCallback","text":"<p>The core of the integration is the <code>flowymlKerasCallback</code>. It automatically logs metrics, parameters, and model checkpoints during training.</p>"},{"location":"integrations/keras/#real-world-pattern-auto-tracking","title":"Real-World Pattern: Auto-Tracking","text":"<pre><code>import tensorflow as tf\nfrom flowyml.integrations.keras import flowymlKerasCallback\nfrom flowyml import step\n\n@step\ndef train_model(x_train, y_train):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(128, activation='relu'),\n        tf.keras.layers.Dense(10, activation='softmax')\n    ])\n\n    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n\n    # Just add the callback!\n    model.fit(\n        x_train, y_train,\n        epochs=10,\n        callbacks=[\n            flowymlKerasCallback(\n                experiment_name=\"mnist_classifier\",\n                log_model=True  # Auto-save to artifact store\n            )\n        ]\n    )\n\n    return model\n</code></pre>"},{"location":"integrations/keras/#model-management","title":"\ud83d\udce6 Model Management","text":"<p>Models trained with the callback are automatically registered in flowyml's Model Registry.</p> <pre><code>from flowyml import ModelRegistry\n\n# Load the latest champion\nregistry = ModelRegistry()\nmodel = registry.get_model(\"mnist_classifier\", version=\"latest\").load()\n</code></pre>"},{"location":"integrations/kubernetes/","title":"Kubernetes Integration \u2638\ufe0f","text":"<p>Orchestrate your pipelines on Kubernetes clusters for massive scale.</p> <p>[!NOTE] What you'll learn: How to deploy flowyml pipelines to K8s</p> <p>Key insight: Turn your K8s cluster into a powerful ML engine.</p>"},{"location":"integrations/kubernetes/#why-kubernetes","title":"Why Kubernetes?","text":"<ul> <li>Scale: Run thousands of steps in parallel.</li> <li>Resource Management: CPU/GPU quotas and limits.</li> <li>Resilience: K8s automatically restarts failed pods.</li> </ul>"},{"location":"integrations/kubernetes/#running-on-kubernetes","title":"\u2638\ufe0f Running on Kubernetes","text":"<p>flowyml submits each step as a Kubernetes Pod.</p>"},{"location":"integrations/kubernetes/#configuration","title":"Configuration","text":"<pre><code>from flowyml.integrations.kubernetes import KubernetesOrchestrator\n\npipeline.run(\n    orchestrator=KubernetesOrchestrator(\n        namespace=\"flowyml-jobs\",\n        image=\"my-registry/flowyml-app:latest\"\n    )\n)\n</code></pre>"},{"location":"integrations/kubernetes/#pod-configuration","title":"\u2699\ufe0f Pod Configuration","text":"<p>Customize resources for specific steps.</p> <pre><code>from flowyml import step, Resources\n\n@step(\n    resources=Resources(\n        cpu=\"2\",\n        memory=\"4Gi\",\n        gpu=\"1\"\n    )\n)\ndef train_model(data):\n    # This runs on a pod with 2 CPUs, 4GB RAM, and 1 GPU\n    return model.fit(data)\n</code></pre>"},{"location":"integrations/kubernetes/#secrets-env-vars","title":"\ud83d\udd10 Secrets &amp; Env Vars","text":"<p>Inject secrets safely into your pods.</p> <pre><code>orchestrator = KubernetesOrchestrator(\n    env_vars={\n        \"API_KEY\": {\"secret_name\": \"my-secret\", \"key\": \"api-key\"}\n    }\n)\n</code></pre>"},{"location":"integrations/mlflow/","title":"MLflow Integration \ud83e\uddea","text":"<p>Track experiments and manage models with the industry-standard open source platform.</p> <p>[!NOTE] What you'll learn: How to auto-log metrics and models to MLflow</p> <p>Key insight: flowyml + MLflow = Automated Experiment Tracking.</p>"},{"location":"integrations/mlflow/#why-mlflow","title":"Why MLflow?","text":"<ul> <li>Experiment Tracking: Log parameters, metrics, and artifacts.</li> <li>Model Registry: Version and manage model lifecycles.</li> <li>Universal: Works with almost any ML library.</li> </ul>"},{"location":"integrations/mlflow/#auto-logging","title":"\ud83e\uddea Auto-Logging","text":"<p>flowyml can automatically configure MLflow tracking for your pipeline.</p> <pre><code>from flowyml.integrations.mlflow import MLflowTracker\n\n# Enable MLflow tracking\npipeline.run(\n    tracker=MLflowTracker(\n        tracking_uri=\"http://localhost:5000\",\n        experiment_name=\"my_experiment\"\n    )\n)\n</code></pre>"},{"location":"integrations/mlflow/#logging-in-steps","title":"\ud83d\udcdd Logging in Steps","text":"<p>You can also log custom metrics inside your steps.</p> <pre><code>import mlflow\nfrom flowyml import step\n\n@step\ndef train_model(data):\n    mlflow.log_param(\"lr\", 0.01)\n\n    # ... training ...\n\n    mlflow.log_metric(\"accuracy\", 0.95)\n    mlflow.sklearn.log_model(model, \"model\")\n\n    return model\n</code></pre>"},{"location":"integrations/mlflow/#model-registry","title":"\ud83d\udce6 Model Registry","text":"<p>Register models automatically after successful training.</p> <pre><code>@step\ndef register_model(model):\n    mlflow.register_model(\n        \"runs:/&lt;run_id&gt;/model\",\n        \"ProductionModel\"\n    )\n</code></pre>"},{"location":"integrations/pytorch/","title":"PyTorch Integration \ud83d\udd25","text":"<p>Dynamic neural networks, orchestrated by flowyml.</p> <p>[!NOTE] What you'll learn: How to manage PyTorch training loops and models</p> <p>Key insight: flowyml handles the artifacts so you can focus on the gradients.</p>"},{"location":"integrations/pytorch/#why-pytorch-flowyml","title":"Why PyTorch + flowyml?","text":"<ul> <li>Custom Materializers: Save/load models and DataLoaders automatically.</li> <li>Reproducibility: Track random seeds and hyperparameters.</li> <li>Distributed Training: Scale to multi-GPU easily.</li> </ul>"},{"location":"integrations/pytorch/#training-pattern","title":"\ud83d\udd25 Training Pattern","text":"<p>Wrap your training loop in a step.</p> <pre><code>import torch\nfrom flowyml import step\n\n@step\ndef train(dataloader, model, epochs):\n    optimizer = torch.optim.Adam(model.parameters())\n\n    for epoch in range(epochs):\n        for batch in dataloader:\n            optimizer.zero_grad()\n            loss = model(batch)\n            loss.backward()\n            optimizer.step()\n\n    return model\n</code></pre>"},{"location":"integrations/pytorch/#saving-models","title":"\ud83d\udcbe Saving Models","text":"<p>flowyml automatically uses <code>torch.save</code> when you return a <code>nn.Module</code>.</p> <pre><code># In your materializer config (auto-detected usually)\n# It saves state_dict or full model based on preference\n</code></pre>"},{"location":"integrations/sklearn/","title":"Scikit-Learn Integration \ud83e\udde0","text":"<p>Classic ML pipelines made robust and reproducible.</p> <p>[!NOTE] What you'll learn: How to version and deploy sklearn models</p> <p>Key insight: Turn your notebook scripts into production pipelines.</p>"},{"location":"integrations/sklearn/#why-scikit-learn-flowyml","title":"Why Scikit-Learn + flowyml?","text":"<ul> <li>Pipeline Versioning: Version the entire preprocessing + model chain.</li> <li>Model Registry: Promote the best Random Forest to production.</li> <li>Easy Serving: Deploy sklearn models as APIs.</li> </ul>"},{"location":"integrations/sklearn/#pipeline-pattern","title":"\ud83e\udde0 Pipeline Pattern","text":"<p>Return a <code>sklearn.pipeline.Pipeline</code> object for automatic serialization.</p> <pre><code>from sklearn.pipeline import Pipeline as SkPipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom flowyml import step\n\n@step\ndef build_pipeline():\n    return SkPipeline([\n        ('scaler', StandardScaler()),\n        ('rf', RandomForestClassifier())\n    ])\n\n@step\ndef train(pipeline, X, y):\n    pipeline.fit(X, y)\n    return pipeline\n</code></pre>"},{"location":"integrations/slack/","title":"Slack Integration \ud83d\udcac","text":"<p>Get notified where you work. Receive alerts for pipeline successes, failures, and approvals.</p> <p>[!NOTE] What you'll learn: How to send pipeline alerts to Slack channels</p> <p>Key insight: Don't watch the terminal. Let the bot tell you when it's done.</p>"},{"location":"integrations/slack/#why-slack-alerts","title":"Why Slack Alerts?","text":"<ul> <li>Real-time: Know immediately when a production job fails.</li> <li>Visibility: Keep the whole team in the loop.</li> <li>Actionable: Links to logs and dashboards directly in the message.</li> </ul>"},{"location":"integrations/slack/#configuration","title":"\ud83d\udcac Configuration","text":"<ol> <li>Create a Slack App and get a Webhook URL.</li> <li>Configure flowyml to use it.</li> </ol> <pre><code>from flowyml import configure_notifications\n\nconfigure_notifications(\n    slack_webhook=\"https://hooks.slack.com/services/T000/B000/XXXX\"\n)\n</code></pre>"},{"location":"integrations/slack/#sending-alerts","title":"\ud83d\udd14 Sending Alerts","text":"<p>Send alerts from any step.</p> <pre><code>from flowyml import get_notifier, step\n\n@step\ndef notify_team(metrics):\n    get_notifier().notify(\n        title=\"Training Finished\",\n        message=f\"Accuracy: {metrics['acc']}\",\n        level=\"success\",\n        channels=[\"slack\"]\n    )\n</code></pre>"},{"location":"integrations/slack/#failure-alerts","title":"\ud83d\udea8 Failure Alerts","text":"<p>Automatically notify on pipeline failure.</p> <pre><code>@step(\n    on_failure=on_failure(\n        action=\"slack\",\n        recipients=[\"#ml-ops\"]\n    )\n)\ndef critical_step():\n    # ...\n</code></pre>"},{"location":"integrations/tensorflow/","title":"TensorFlow Integration \ud83e\udd16","text":"<p>Production-grade TensorFlow pipelines.</p> <p>[!NOTE] What you'll learn: How to manage TF graphs and SavedModels</p> <p>Key insight: Enterprise-scale TF management.</p>"},{"location":"integrations/tensorflow/#why-tensorflow-flowyml","title":"Why TensorFlow + flowyml?","text":"<ul> <li>SavedModel Support: First-class support for TF SavedModel format.</li> <li>TFX Compatibility: Integrate with TFX components.</li> <li>Serving: Easy export to TensorFlow Serving.</li> </ul>"},{"location":"integrations/tensorflow/#training-step","title":"\ud83e\udd16 Training Step","text":"<pre><code>import tensorflow as tf\nfrom flowyml import step\n\n@step\ndef train_tf_model(dataset):\n    model = tf.keras.Sequential([...])\n\n    # flowyml tracks this execution\n    model.fit(dataset, epochs=5)\n\n    return model\n</code></pre>"},{"location":"integrations/tensorflow/#artifacts","title":"\ud83d\udcbe Artifacts","text":"<p>flowyml saves TF models as <code>SavedModel</code> directories, preserving the graph and weights.</p>"},{"location":"integrations/wandb/","title":"Weights &amp; Biases Integration \ud83d\udc1d","text":"<p>Visualize your training runs and track artifacts with W&amp;B.</p> <p>[!NOTE] What you'll learn: How to integrate W&amp;B for rich experiment visualization</p> <p>Key insight: Beautiful dashboards for your flowyml pipelines.</p>"},{"location":"integrations/wandb/#why-weights-biases","title":"Why Weights &amp; Biases?","text":"<ul> <li>Visualization: Interactive charts for loss, accuracy, and system metrics.</li> <li>Collaboration: Share results with your team instantly.</li> <li>Artifacts: Track dataset and model versioning.</li> </ul>"},{"location":"integrations/wandb/#configuration","title":"\ud83d\udc1d Configuration","text":"<p>Enable W&amp;B tracking for your pipeline run.</p> <pre><code>from flowyml.integrations.wandb import WandBTracker\n\npipeline.run(\n    tracker=WandBTracker(\n        project=\"flowyml-demo\",\n        entity=\"my-team\"\n    )\n)\n</code></pre>"},{"location":"integrations/wandb/#logging-metrics","title":"\ud83d\udcca Logging Metrics","text":"<p>flowyml automatically captures step inputs/outputs, but you can add custom logs.</p> <pre><code>import wandb\nfrom flowyml import step\n\n@step\ndef train(data):\n    # Log custom metrics\n    wandb.log({\"loss\": 0.1, \"accuracy\": 0.9})\n\n    # Log images\n    wandb.log({\"chart\": wandb.Image(\"plot.png\")})\n\n    return model\n</code></pre>"},{"location":"integrations/wandb/#wb-artifacts","title":"\ud83d\udce6 W&amp;B Artifacts","text":"<p>Use W&amp;B Artifacts to track data lineage.</p> <pre><code>@step\ndef load_data():\n    run = wandb.init()\n    artifact = run.use_artifact('mnist:v1')\n    dir = artifact.download()\n    return load_dataset(dir)\n</code></pre>"},{"location":"plugins/creating-plugins/","title":"Creating Custom Plugins","text":"<p>flowyml's plugin system is designed to be extensible. You can integrate any Python class as a flowyml component using the Generic Bridge and a simple YAML configuration.</p>"},{"location":"plugins/creating-plugins/#configuration-structure","title":"Configuration Structure","text":"<p>Plugins are defined in a <code>plugins</code> list within your <code>flowyml.yaml</code> or a dedicated config file.</p> <pre><code>plugins:\n  - name: &lt;unique_plugin_name&gt;\n    source: &lt;python_import_path&gt;\n    type: &lt;component_type&gt;\n    adaptation:\n      method_mapping:\n        &lt;flowyml_method&gt;: &lt;external_method&gt;\n      attribute_mapping:\n        &lt;flowyml_attribute&gt;: &lt;external_attribute&gt;\n</code></pre>"},{"location":"plugins/creating-plugins/#fields","title":"Fields","text":"<ul> <li><code>name</code>: A unique identifier for the plugin (e.g., <code>my_custom_orchestrator</code>).</li> <li><code>source</code>: The full Python import path to the class (e.g., <code>airflow.providers.google.cloud.operators.bigquery.BigQueryExecuteQueryOperator</code>).</li> <li><code>type</code>: The flowyml component type. Supported values:<ul> <li><code>orchestrator</code></li> <li><code>artifact_store</code></li> <li><code>container_registry</code></li> </ul> </li> <li><code>adaptation</code>: (Optional) Rules for adapting the external class to flowyml's interface.</li> </ul>"},{"location":"plugins/creating-plugins/#adaptation-rules","title":"Adaptation Rules","text":""},{"location":"plugins/creating-plugins/#method-mapping","title":"Method Mapping","text":"<p>Map flowyml's standard methods to the methods of your external class.</p> <p>Example: Mapping flowyml's <code>run_pipeline</code> to Airflow's <code>execute</code>:</p> <pre><code>adaptation:\n  method_mapping:\n    run_pipeline: execute\n</code></pre>"},{"location":"plugins/creating-plugins/#attribute-mapping","title":"Attribute Mapping","text":"<p>Map flowyml's expected attributes to the attributes of your external class.</p> <p>Example: Mapping <code>config</code> to <code>params</code>:</p> <pre><code>adaptation:\n  attribute_mapping:\n    config: params\n</code></pre>"},{"location":"plugins/creating-plugins/#example-airflow-operator","title":"Example: Airflow Operator","text":"<p>Here is how you can wrap an Airflow BigQuery operator as a flowyml orchestrator:</p> <pre><code>plugins:\n  - name: airflow_bigquery\n    source: airflow.providers.google.cloud.operators.bigquery.BigQueryExecuteQueryOperator\n    type: orchestrator\n    adaptation:\n      method_mapping:\n        run_pipeline: execute\n      attribute_mapping:\n        config: params\n</code></pre> <p>Once defined, you can load and use this component in your flowyml code:</p> <pre><code>from flowyml.stacks.plugins import get_component_registry\n\nregistry = get_component_registry()\nregistry.load_plugins_from_config(\"flowyml.yaml\")\n\nbq_operator = registry.get_orchestrator(\"airflow_bigquery\")\n# Now use it like any flowyml orchestrator!\n</code></pre>"},{"location":"plugins/overview/","title":"flowyml Plugin System","text":"<p>flowyml features a robust, ecosystem-agnostic plugin system designed to seamlessly integrate components from various ML and data frameworks, including ZenML, Airflow, and Prefect.</p>"},{"location":"plugins/overview/#key-features","title":"Key Features","text":"<ul> <li>Generic Integration Bridge: A universal wrapper system that adapts external components (like ZenML orchestrators or Airflow operators) to flowyml's interface using rule-based configuration. No hardcoded dependencies!</li> <li>Zero-Code Integration: Use components from other frameworks just by defining a simple YAML configuration.</li> <li>Unified Management: Discover, install, and manage plugins via a consistent CLI.</li> <li>Stack Migration: Automatically migrate existing stacks (e.g., from ZenML) to flowyml.</li> </ul>"},{"location":"plugins/overview/#architecture","title":"Architecture","text":"<p>The plugin system is built on three core pillars:</p> <ol> <li>Component Registry: The central hub that manages all available components, including built-ins, plugins, and bridged components.</li> <li>Generic Bridge (<code>GenericBridge</code>): A smart adapter that uses introspection and configuration rules to \"teach\" flowyml how to talk to external components.</li> <li>Plugin Configuration: YAML-based definitions that map external classes and methods to flowyml's expected behavior.</li> </ol>"},{"location":"plugins/overview/#quick-start","title":"Quick Start","text":""},{"location":"plugins/overview/#listing-plugins","title":"Listing Plugins","text":"<p>View all installed and available plugins:</p> <pre><code>flowyml plugin list\n</code></pre>"},{"location":"plugins/overview/#searching-for-plugins","title":"Searching for Plugins","text":"<p>Find plugins for specific tools or categories:</p> <pre><code>flowyml plugin search kubernetes\nflowyml plugin search airflow\n</code></pre>"},{"location":"plugins/overview/#installing-plugins","title":"Installing Plugins","text":"<p>Install a plugin directly from PyPI:</p> <pre><code>flowyml plugin install zenml-kubernetes\n</code></pre>"},{"location":"plugins/overview/#importing-external-stacks","title":"Importing External Stacks","text":"<p>Migrate an existing ZenML stack to flowyml:</p> <pre><code>flowyml plugin import-zenml-stack my-zenml-stack\n</code></pre> <p>This will generate a <code>flowyml.yaml</code> configuration file with all the necessary plugin mappings, ready to run!</p>"},{"location":"plugins/zenml-integration/","title":"ZenML Integration","text":"<p>UniFlow provides first-class support for the entire ZenML ecosystem through its Generic Bridge system. You can use any ZenML stack component\u2014orchestrators, artifact stores, container registries\u2014directly within UniFlow pipelines.</p>"},{"location":"plugins/zenml-integration/#how-it-works","title":"How It Works","text":"<p>Unlike traditional integrations that require custom code for each component, UniFlow uses a Generic Bridge that dynamically adapts ZenML components. You simply tell UniFlow where the component lives and how to map its methods (if necessary) via configuration.</p>"},{"location":"plugins/zenml-integration/#using-zenml-components","title":"Using ZenML Components","text":"<p>To use a ZenML component, you define it in your <code>uniflow.yaml</code> (or load it via the CLI).</p>"},{"location":"plugins/zenml-integration/#example-kubernetes-orchestrator","title":"Example: Kubernetes Orchestrator","text":"<pre><code>plugins:\n  - name: zenml_k8s\n    source: zenml.integrations.kubernetes.orchestrators.KubernetesOrchestrator\n    type: orchestrator\n    adaptation:\n      method_mapping:\n        run_pipeline: run\n</code></pre>"},{"location":"plugins/zenml-integration/#example-s3-artifact-store","title":"Example: S3 Artifact Store","text":"<pre><code>plugins:\n  - name: zenml_s3\n    source: zenml.integrations.s3.artifact_stores.S3ArtifactStore\n    type: artifact_store\n</code></pre>"},{"location":"plugins/zenml-integration/#migrating-from-zenml","title":"Migrating from ZenML","text":"<p>If you already have a ZenML stack, you don't need to write configuration manually. UniFlow includes a migration tool that inspects your ZenML stack and generates the UniFlow configuration for you.</p> <pre><code># Import a specific stack\nuniflow plugin import-zenml-stack my-production-stack\n</code></pre> <p>This command will: 1.  Connect to your ZenML client. 2.  Analyze the <code>my-production-stack</code>. 3.  Identify all components (Orchestrator, Artifact Store, etc.). 4.  Generate a <code>uniflow.yaml</code> file with the correct plugin definitions.</p>"},{"location":"plugins/zenml-integration/#running-zenml-pipelines","title":"Running ZenML Pipelines","text":"<p>Once configured, you can run your UniFlow pipelines on ZenML infrastructure seamlessly:</p> <pre><code>uniflow run my_pipeline --stack my-production-stack\n</code></pre> <p>UniFlow handles the translation of pipeline steps and execution context, delegating the actual heavy lifting to the ZenML component (e.g., submitting a job to Kubernetes).</p>"},{"location":"reference/cheatsheet/","title":"flowyml Cheatsheet \ud83d\udcdd","text":"<p>A quick reference guide for common flowyml commands and patterns.</p>"},{"location":"reference/cheatsheet/#cli-commands","title":"CLI Commands \ud83d\udcbb","text":""},{"location":"reference/cheatsheet/#project-management","title":"Project Management","text":"<pre><code># Initialize a new project\nflowyml init my-project\n\n# Initialize with a specific template\nflowyml init my-project --template basic\n</code></pre>"},{"location":"reference/cheatsheet/#ui-management","title":"UI Management","text":"<pre><code># Start the UI server\nflowyml ui start\n\n# Stop the UI server\nflowyml ui stop\n\n# Check UI status\nflowyml ui status\n</code></pre>"},{"location":"reference/cheatsheet/#pipeline-execution","title":"Pipeline Execution","text":"<pre><code># Run a pipeline script\npython my_pipeline.py\n\n# Run with specific configuration\nflowyml_ENV=production python my_pipeline.py\n</code></pre>"},{"location":"reference/cheatsheet/#cache-management","title":"Cache Management","text":"<pre><code># Clear all cache\nflowyml cache clear\n\n# Clear cache for specific pipeline\nflowyml cache clear --pipeline my_pipeline\n</code></pre>"},{"location":"reference/cheatsheet/#python-api","title":"Python API \ud83d\udc0d","text":""},{"location":"reference/cheatsheet/#basic-pipeline","title":"Basic Pipeline","text":"<pre><code>from flowyml import Pipeline, step\n\n@step\ndef step_one():\n    return \"data\"\n\n@step(inputs=[\"data\"])\ndef step_two(data):\n    return f\"processed {data}\"\n\n# Declarative style\n@pipeline\ndef my_pipeline():\n    d = step_one()\n    return step_two(d)\n\nrun = my_pipeline()\n</code></pre>"},{"location":"reference/cheatsheet/#explicit-pipeline-construction","title":"Explicit Pipeline Construction","text":"<pre><code>from flowyml import Pipeline, step\n\np = Pipeline(\"explicit_pipeline\")\np.add_step(step_one)\np.add_step(step_two)\np.run()\n</code></pre>"},{"location":"reference/cheatsheet/#step-configuration","title":"Step Configuration","text":"<pre><code>@step(\n    inputs=[\"raw_data\"],       # Input asset names\n    outputs=[\"model\"],         # Output asset names\n    cache=\"code_hash\",         # Caching strategy\n    retry=3,                   # Retry attempts\n    timeout=3600,              # Timeout in seconds\n    resources={\"gpu\": 1}       # Resource requirements\n)\ndef train(raw_data):\n    ...\n</code></pre>"},{"location":"reference/cheatsheet/#context-parameters","title":"Context &amp; Parameters","text":"<pre><code>from flowyml import context, pipeline\n\n# Define context with parameters\nctx = context(\n    learning_rate=0.01,\n    batch_size=32,\n    env=\"dev\"\n)\n\n@step\ndef train(learning_rate, batch_size):\n    # Parameters are auto-injected by name!\n    ...\n\n@pipeline(context=ctx)\ndef train_pipeline():\n    return train()\n</code></pre>"},{"location":"reference/cheatsheet/#assets","title":"Assets","text":"<pre><code>from flowyml import Dataset, Model, Metrics\n\n# Create a dataset\nds = Dataset.create(\n    data=df,\n    name=\"training_data\",\n    properties={\"source\": \"s3://...\"}\n)\n\n# Create metrics\nmetrics = Metrics.create(\n    accuracy=0.95,\n    loss=0.02\n)\n</code></pre>"},{"location":"reference/cheatsheet/#directory-structure","title":"Directory Structure \ud83d\udcc2","text":"<pre><code>my-project/\n\u251c\u2500\u2500 flowyml.yaml         # Project configuration\n\u251c\u2500\u2500 .flowyml/            # Internal storage\n\u2502   \u251c\u2500\u2500 artifacts/       # Stored assets\n\u2502   \u251c\u2500\u2500 cache/           # Execution cache\n\u2502   \u2514\u2500\u2500 runs/            # Run metadata\n\u251c\u2500\u2500 src/                 # Source code\n\u2502   \u2514\u2500\u2500 pipelines/       # Pipeline definitions\n\u2514\u2500\u2500 notebooks/           # Jupyter notebooks\n</code></pre>"},{"location":"reference/cli/","title":"CLI Reference","text":""},{"location":"reference/cli/#overview","title":"Overview","text":"<p>flowyml provides a powerful CLI for managing stacks, components, and running pipelines without modifying code.</p>"},{"location":"reference/cli/#installation","title":"Installation","text":"<pre><code>pip install flowyml\n</code></pre> <p>The <code>flowyml</code> command will be available globally.</p>"},{"location":"reference/cli/#commands","title":"Commands","text":""},{"location":"reference/cli/#flowyml-init","title":"<code>flowyml init</code>","text":"<p>Initialize a new flowyml project.</p> <pre><code>flowyml init [OPTIONS]\n</code></pre> <p>Options: - <code>--output, -o TEXT</code>: Output file path (default: <code>flowyml.yaml</code>)</p> <p>Examples: <pre><code># Create flowyml.yaml\nflowyml init\n\n# Custom output path\nflowyml init --output config/flowyml.yaml\n</code></pre></p> <p>Output: Creates a <code>flowyml.yaml</code> file with default configuration including: - Local stack - Basic resource presets - Docker configuration</p>"},{"location":"reference/cli/#flowyml-run","title":"<code>flowyml run</code>","text":"<p>Run a pipeline with specified stack and configuration.</p> <pre><code>flowyml run PIPELINE_FILE [OPTIONS]\n</code></pre> <p>Arguments: - <code>PIPELINE_FILE</code>: Path to pipeline Python file</p> <p>Options: - <code>--stack, -s TEXT</code>: Stack to use (from flowyml.yaml) - <code>--resources, -r TEXT</code>: Resource configuration to use - <code>--config, -c TEXT</code>: Path to flowyml.yaml - <code>--context, -ctx TEXT</code>: Context variables (key=value), can be specified multiple times - <code>--dry-run</code>: Show what would be executed without running</p> <p>Examples: <pre><code># Run with default (local) stack\nflowyml run pipeline.py\n\n# Run on production stack\nflowyml run pipeline.py --stack production\n\n# Run with GPU resources\nflowyml run pipeline.py --stack production --resources gpu_training\n\n# Pass context variables\nflowyml run pipeline.py --context data_path=gs://bucket/data.csv --context model_id=123\n\n# Dry run to see configuration\nflowyml run pipeline.py --stack production --dry-run\n\n# Custom config file\nflowyml run pipeline.py --config custom.yaml --stack staging\n\n# Combined example\nflowyml run train.py \\\n  --stack production \\\n  --resources gpu_large \\\n  --context data_path=gs://prod/train.csv \\\n  --context epochs=100\n</code></pre></p>"},{"location":"reference/cli/#flowyml-stack","title":"<code>flowyml stack</code>","text":"<p>Manage infrastructure stacks.</p>"},{"location":"reference/cli/#flowyml-stack-list","title":"<code>flowyml stack list</code>","text":"<p>List all configured stacks.</p> <pre><code>flowyml stack list [OPTIONS]\n</code></pre> <p>Options: - <code>--config, -c TEXT</code>: Path to flowyml.yaml</p> <p>Examples: <pre><code># List stacks\nflowyml stack list\n\n# With custom config\nflowyml stack list --config custom.yaml\n</code></pre></p> <p>Output: <pre><code>Configured stacks:\n  \u2022 local (default) [local]\n  \u2022 production [gcp]\n  \u2022 staging [gcp]\n</code></pre></p>"},{"location":"reference/cli/#flowyml-stack-show","title":"<code>flowyml stack show</code>","text":"<p>Show detailed stack configuration.</p> <pre><code>flowyml stack show STACK_NAME [OPTIONS]\n</code></pre> <p>Arguments: - <code>STACK_NAME</code>: Name of stack to show</p> <p>Options: - <code>--config, -c TEXT</code>: Path to flowyml.yaml</p> <p>Examples: <pre><code># Show production stack details\nflowyml stack show production\n\n# With custom config\nflowyml stack show staging --config staging.yaml\n</code></pre></p> <p>Output: <pre><code>Stack: production\ntype: gcp\nproject_id: my-ml-project\nregion: us-central1\nartifact_store:\n  type: gcs\n  bucket: ml-artifacts-prod\n</code></pre></p>"},{"location":"reference/cli/#flowyml-stack-set-default","title":"<code>flowyml stack set-default</code>","text":"<p>Set the default stack.</p> <pre><code>flowyml stack set-default STACK_NAME [OPTIONS]\n</code></pre> <p>Arguments: - <code>STACK_NAME</code>: Name of stack to set as default</p> <p>Options: - <code>--config, -c TEXT</code>: Path to flowyml.yaml</p> <p>Examples: <pre><code># Set production as default\nflowyml stack set-default production\n\n# With custom config\nflowyml stack set-default local --config dev.yaml\n</code></pre></p>"},{"location":"reference/cli/#flowyml-component","title":"<code>flowyml component</code>","text":"<p>Manage stack components and plugins.</p>"},{"location":"reference/cli/#flowyml-component-list","title":"<code>flowyml component list</code>","text":"<p>List all registered components.</p> <pre><code>flowyml component list [OPTIONS]\n</code></pre> <p>Options: - <code>--type, -t TEXT</code>: Filter by component type (orchestrators, artifact_stores, container_registries)</p> <p>Examples: <pre><code># List all components\nflowyml component list\n\n# List only orchestrators\nflowyml component list --type orchestrators\n\n# List only artifact stores\nflowyml component list --type artifact_stores\n</code></pre></p> <p>Output: <pre><code>\ud83d\udce6 Registered Components:\n\nOrchestrators:\n  \u2022 vertex_ai\n  \u2022 airflow\n\nArtifact_stores:\n  \u2022 local\n  \u2022 gcs\n  \u2022 minio\n</code></pre></p>"},{"location":"reference/cli/#flowyml-component-load","title":"<code>flowyml component load</code>","text":"<p>Load a component from various sources.</p> <pre><code>flowyml component load SOURCE [OPTIONS]\n</code></pre> <p>Arguments: - <code>SOURCE</code>: Component source (see examples)</p> <p>Options: - <code>--name, -n TEXT</code>: Custom name for component</p> <p>Examples: <pre><code># From Python module\nflowyml component load my_flowyml_components\n\n# From file with specific class\nflowyml component load /path/to/custom.py:MyOrchestrator\n\n# From ZenML\nflowyml component load zenml:zenml.integrations.kubernetes.orchestrators.KubernetesOrchestrator\n\n# With custom name\nflowyml component load my_components --name custom\n</code></pre></p> <p>Source Formats: - <code>module.path</code> - Load from Python module - <code>/path/to/file.py:ClassName</code> - Load from file - <code>zenml:zenml.path.Class</code> - Load from ZenML</p>"},{"location":"reference/cli/#global-options","title":"Global Options","text":"<p>All commands support: - <code>--help</code>: Show help message - <code>--version</code>: Show flowyml version</p>"},{"location":"reference/cli/#configuration-files","title":"Configuration Files","text":""},{"location":"reference/cli/#search-order","title":"Search Order","text":"<p>flowyml searches for configuration in this order: 1. <code>--config</code> flag value 2. <code>flowyml.yaml</code> (current directory) 3. <code>flowyml.yml</code> 4. <code>.flowyml/config.yaml</code> 5. <code>.flowyml/config.yml</code></p>"},{"location":"reference/cli/#environment-variables","title":"Environment Variables","text":"<p>flowyml automatically expands environment variables in configuration: - <code>${VAR_NAME}</code> - Required variable (fails if not set) - <code>$VAR_NAME</code> - Required variable - <code>${VAR_NAME:-default}</code> - With default value (future)</p>"},{"location":"reference/cli/#examples","title":"Examples","text":""},{"location":"reference/cli/#development-workflow","title":"Development Workflow","text":"<pre><code># 1. Initialize project\nflowyml init\n\n# 2. Edit flowyml.yaml\nvim flowyml.yaml\n\n# 3. List available stacks\nflowyml stack list\n\n# 4. Run pipeline locally\nflowyml run pipeline.py\n\n# 5. Test on staging\nflowyml run pipeline.py --stack staging --dry-run\n\n# 6. Deploy to production\nflowyml run pipeline.py --stack production --resources gpu_training\n</code></pre>"},{"location":"reference/cli/#multi-environment-deployment","title":"Multi-Environment Deployment","text":"<pre><code># Development\nflowyml run pipeline.py --config dev.yaml\n\n# Staging\nflowyml run pipeline.py --config staging.yaml --stack staging\n\n# Production\nflowyml run pipeline.py --config prod.yaml --stack production\n</code></pre>"},{"location":"reference/cli/#custom-components","title":"Custom Components","text":"<pre><code># 1. List current components\nflowyml component list\n\n# 2. Load custom component\nflowyml component load my_custom_components\n\n# 3. Verify it's loaded\nflowyml component list\n\n# 4. Use in pipeline\nflowyml run pipeline.py --stack custom_stack\n</code></pre>"},{"location":"reference/cli/#gpu-training","title":"GPU Training","text":"<pre><code># Train with single GPU\nflowyml run train.py --resources gpu_small\n\n# Train with multiple GPUs\nflowyml run train.py --resources gpu_large\n\n# Large-scale training with A100s\nflowyml run train.py \\\n  --stack production \\\n  --resources gpu_xlarge \\\n  --context batch_size=512 \\\n  --context epochs=100\n</code></pre>"},{"location":"reference/cli/#debugging","title":"Debugging","text":"<pre><code># Dry run to see configuration\nflowyml run pipeline.py --stack production --dry-run\n\n# Show stack details\nflowyml stack show production\n\n# Validate configuration\npython -c \"from flowyml.utils.stack_config import load_config; load_config().load()\"\n</code></pre>"},{"location":"reference/cli/#exit-codes","title":"Exit Codes","text":"<ul> <li><code>0</code>: Success</li> <li><code>1</code>: General error</li> <li><code>2</code>: Configuration error</li> <li><code>3</code>: Pipeline execution error</li> </ul>"},{"location":"reference/cli/#shell-completion","title":"Shell Completion","text":""},{"location":"reference/cli/#bash","title":"Bash","text":"<pre><code>echo 'eval \"$(_flowyml_COMPLETE=bash_source flowyml)\"' &gt;&gt; ~/.bashrc\n</code></pre>"},{"location":"reference/cli/#zsh","title":"Zsh","text":"<pre><code>echo 'eval \"$(_flowyml_COMPLETE=zsh_source flowyml)\"' &gt;&gt; ~/.zshrc\n</code></pre>"},{"location":"reference/cli/#fish","title":"Fish","text":"<pre><code>echo '_flowyml_COMPLETE=fish_source flowyml | source' &gt;&gt; ~/.config/fish/completions/flowyml.fish\n</code></pre>"},{"location":"reference/cli/#tips-tricks","title":"Tips &amp; Tricks","text":""},{"location":"reference/cli/#aliases","title":"Aliases","text":"<pre><code># .bashrc or .zshrc\nalias uf='flowyml'\nalias ufr='flowyml run'\nalias ufs='flowyml stack'\nalias ufc='flowyml component'\n\n# Usage\nufr pipeline.py -s production\nufs list\nufc list\n</code></pre>"},{"location":"reference/cli/#default-stack","title":"Default Stack","text":"<p>Set in <code>flowyml.yaml</code>: <pre><code>default_stack: production\n</code></pre></p> <p>Then run without specifying stack: <pre><code>flowyml run pipeline.py\n# Uses production stack\n</code></pre></p>"},{"location":"reference/cli/#environment-specific-aliases","title":"Environment-Specific Aliases","text":"<pre><code># Development\nalias uf-dev='flowyml run --config dev.yaml'\n\n# Staging\nalias uf-stage='flowyml run --config staging.yaml --stack staging'\n\n# Production\nalias uf-prod='flowyml run --config prod.yaml --stack production'\n\n# Usage\nuf-dev pipeline.py\nuf-stage pipeline.py\nuf-prod pipeline.py --resources gpu_large\n</code></pre>"},{"location":"reference/cli/#cicd-integration","title":"CI/CD Integration","text":"<pre><code># .github/workflows/ml-pipeline.yml\nname: ML Pipeline\n\non:\n  push:\n    branches: [main]\n\njobs:\n  train:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n\n      - name: Set up Python\n        uses: actions/setup-python@v2\n        with:\n          python-version: '3.11'\n\n      - name: Install dependencies\n        run: |\n          pip install flowyml[gcp]\n\n      - name: Run pipeline\n        env:\n          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}\n          GCP_BUCKET: ${{ secrets.GCP_BUCKET }}\n          GCP_SERVICE_ACCOUNT: ${{ secrets.GCP_SERVICE_ACCOUNT }}\n        run: |\n          flowyml run training_pipeline.py \\\n            --stack production \\\n            --resources gpu_training \\\n            --context experiment_name=github-${{ github.run_id }}\n</code></pre>"},{"location":"reference/cli/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/cli/#command-not-found","title":"Command Not Found","text":"<pre><code># Check installation\npip show flowyml\n\n# Reinstall\npip install --force-reinstall flowyml\n</code></pre>"},{"location":"reference/cli/#configuration-not-found","title":"Configuration Not Found","text":"<pre><code># Specify custom path\nflowyml run pipeline.py --config /full/path/to/flowyml.yaml\n\n# Check current directory\npwd\nls -la flowyml.yaml\n</code></pre>"},{"location":"reference/cli/#component-not-found","title":"Component Not Found","text":"<pre><code># List what's registered\nflowyml component list\n\n# Load explicitly\nflowyml component load my_components\n\n# Check Python path\npython -c \"import my_components\"\n</code></pre>"},{"location":"reference/cli/#stack-validation-fails","title":"Stack Validation Fails","text":"<pre><code># Show stack configuration\nflowyml stack show STACK_NAME\n\n# Dry run\nflowyml run pipeline.py --stack STACK_NAME --dry-run\n</code></pre>"},{"location":"reference/cli/#see-also","title":"See Also","text":"<ul> <li>Configuration Guide</li> <li>Components Guide</li> <li>Quick Reference</li> <li>Stack Architecture</li> </ul>"},{"location":"tutorials/extensible-pipeline/","title":"Tutorial: Building an Extensible ML Pipeline","text":"<p>This step-by-step tutorial shows you how to build a production-ready ML pipeline with custom components, from development to deployment.</p>"},{"location":"tutorials/extensible-pipeline/#what-youll-build","title":"What You'll Build","text":"<ul> <li>Production ML pipeline with TensorFlow</li> <li>Custom MinIO artifact store component</li> <li>Multi-environment configuration</li> <li>Automated CI/CD deployment</li> </ul>"},{"location":"tutorials/extensible-pipeline/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> <li>Docker installed</li> <li>(Optional) GCP account for cloud deployment</li> <li>(Optional) MinIO server for custom storage</li> </ul>"},{"location":"tutorials/extensible-pipeline/#step-1-project-setup","title":"Step 1: Project Setup","text":""},{"location":"tutorials/extensible-pipeline/#install-flowyml","title":"Install flowyml","text":"<pre><code># Basic installation\npip install flowyml\n\n# With ML and GCP support\npip install flowyml[tensorflow,gcp]\n</code></pre>"},{"location":"tutorials/extensible-pipeline/#initialize-project","title":"Initialize Project","text":"<pre><code># Create project directory\nmkdir ml-pipeline-tutorial\ncd ml-pipeline-tutorial\n\n# Initialize flowyml\nflowyml init\n\n# Should create flowyml.yaml\n</code></pre>"},{"location":"tutorials/extensible-pipeline/#step-2-write-your-pipeline","title":"Step 2: Write Your Pipeline","text":"<p>Create <code>training_pipeline.py</code>:</p> <pre><code>\"\"\"Clean ML training pipeline - infrastructure agnostic.\"\"\"\n\nfrom flowyml import Pipeline, step, context, Dataset, Model, Metrics\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\n\n\n@step(outputs=[\"dataset\"])\ndef load_data(data_path: str):\n    \"\"\"Load and prepare training data.\"\"\"\n    # In production, load from data_path\n    # For tutorial, generate synthetic data\n    np.random.seed(42)\n    X = np.random.randn(1000, 20)\n    y = (X[:, 0] + X[:, 1] &gt; 0).astype(int)\n\n    df = pd.DataFrame(X)\n    df['label'] = y\n\n    return Dataset.create(\n        data=df,\n        name=\"training_data\",\n        properties={\n            \"rows\": len(df),\n            \"cols\": len(df.columns),\n            \"source\": data_path\n        }\n    )\n\n\n@step(inputs=[\"dataset\"], outputs=[\"train_data\", \"val_data\"])\ndef split_data(dataset: Dataset):\n    \"\"\"Split into train and validation sets.\"\"\"\n    df = dataset.data\n\n    # 80-20 split\n    split_idx = int(len(df) * 0.8)\n    train_df = df.iloc[:split_idx]\n    val_df = df.iloc[split_idx:]\n\n    train_dataset = Dataset.create(\n        data=train_df,\n        name=\"train_data\",\n        parent=dataset,\n        properties={\"split\": \"train\"}\n    )\n    val_dataset = Dataset.create(\n        data=val_df,\n        name=\"val_data\",\n        parent=dataset,\n        properties={\"split\": \"validation\"}\n    )\n\n    return train_dataset, val_dataset\n\n\n@step(inputs=[\"train_data\"], outputs=[\"model\"])\ndef train_model(train_data: Dataset, epochs: int):\n    \"\"\"Train TensorFlow model.\"\"\"\n    df = train_data.data\n\n    # Prepare data\n    X_train = df.drop('label', axis=1).values\n    y_train = df['label'].values\n\n    # Build model\n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(64, activation='relu', input_shape=(20,)),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(32, activation='relu'),\n        tf.keras.layers.Dense(1, activation='sigmoid')\n    ])\n\n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n\n    # Train\n    history = model.fit(\n        X_train, y_train,\n        epochs=epochs,\n        batch_size=32,\n        validation_split=0.2,\n        verbose=1\n    )\n\n    return Model.create(\n        data=model,\n        name=\"binary_classifier\",\n        framework=\"tensorflow\",\n        parent=train_data\n    )\n\n\n@step(inputs=[\"model\", \"val_data\"], outputs=[\"metrics\"])\ndef evaluate_model(model: Model, val_data: Dataset):\n    \"\"\"Evaluate on validation set.\"\"\"\n    df = val_data.data\n\n    X_val = df.drop('label', axis=1).values\n    y_val = df['label'].values\n\n    # Evaluate\n    tf_model = model.data\n    loss, accuracy = tf_model.evaluate(X_val, y_val, verbose=0)\n\n    return Metrics.create(\n        loss=float(loss),\n        accuracy=float(accuracy),\n        name=\"validation_metrics\",\n        parent=model\n    )\n\n\n# Create pipeline - NO infrastructure code!\nif __name__ == \"__main__\":\n    ctx = context(\n        data_path=\"data/train.csv\",\n        epochs=10\n    )\n\n    pipeline = Pipeline(\"ml_training\", context=ctx)\n    pipeline.add_step(load_data)\n    pipeline.add_step(split_data)\n    pipeline.add_step(train_model)\n    pipeline.add_step(evaluate_model)\n\n    result = pipeline.run()\n\n    if result.success:\n        print(f\"\u2705 Training complete!\")\n        print(f\"Accuracy: {result.outputs['metrics'].accuracy:.2%}\")\n    else:\n        print(f\"\u274c Training failed\")\n</code></pre>"},{"location":"tutorials/extensible-pipeline/#step-3-test-locally","title":"Step 3: Test Locally","text":""},{"location":"tutorials/extensible-pipeline/#run-with-default-local-stack","title":"Run with default (local) stack","text":"<pre><code>flowyml run training_pipeline.py\n</code></pre> <p>Output: <pre><code>\ud83d\ude80 Running pipeline: training_pipeline.py\n\ud83d\udce6 Stack: local\n\u2699\ufe0f  Loading pipeline...\n\ud83c\udfc3 Running pipeline...\n\nEpoch 1/10\n...\n\u2705 Pipeline completed successfully!\n</code></pre></p>"},{"location":"tutorials/extensible-pipeline/#verify-artifacts","title":"Verify artifacts","text":"<pre><code># Check artifact storage\nls -R .flowyml/artifacts/\n\n# Check metadata\nsqlite3 .flowyml/metadata.db \"SELECT * FROM runs;\"\n</code></pre>"},{"location":"tutorials/extensible-pipeline/#step-4-create-custom-component","title":"Step 4: Create Custom Component","text":"<p>For this tutorial, we'll create a MinIO artifact store.</p> <p>Create <code>custom_components/minio_store.py</code>:</p> <pre><code>\"\"\"Custom MinIO artifact store for tutorial.\"\"\"\n\nfrom flowyml.stacks.components import ArtifactStore\nfrom flowyml.stacks.plugins import register_component\nfrom typing import Any\nimport pickle\nimport io\n\n\n@register_component\nclass MinIOArtifactStore(ArtifactStore):\n    \"\"\"MinIO object storage integration.\"\"\"\n\n    def __init__(\n        self,\n        name: str = \"minio\",\n        endpoint: str = \"localhost:9000\",\n        bucket: str = \"ml-artifacts\",\n        access_key: str = \"minioadmin\",\n        secret_key: str = \"minioadmin\",\n        secure: bool = False,\n    ):\n        super().__init__(name)\n        self.endpoint = endpoint\n        self.bucket = bucket\n        self.access_key = access_key\n        self.secret_key = secret_key\n        self.secure = secure\n        self._client = None\n\n    @property\n    def client(self):\n        if self._client is None:\n            from minio import Minio\n\n            self._client = Minio(\n                self.endpoint,\n                access_key=self.access_key,\n                secret_key=self.secret_key,\n                secure=self.secure,\n            )\n\n            # Create bucket if needed\n            if not self._client.bucket_exists(self.bucket):\n                self._client.make_bucket(self.bucket)\n\n        return self._client\n\n    def validate(self) -&gt; bool:\n        try:\n            from minio import Minio\n            _ = self.client  # Test connection\n            return True\n        except ImportError:\n            raise ImportError(\"pip install minio\")\n        except Exception as e:\n            raise ConnectionError(f\"Cannot connect to MinIO: {e}\")\n\n    def save(self, artifact: Any, path: str) -&gt; str:\n        data = pickle.dumps(artifact)\n        stream = io.BytesIO(data)\n\n        self.client.put_object(\n            self.bucket,\n            path,\n            stream,\n            length=len(data)\n        )\n\n        return f\"s3://{self.bucket}/{path}\"\n\n    def load(self, path: str) -&gt; Any:\n        if path.startswith(\"s3://\"):\n            path = path.replace(f\"s3://{self.bucket}/\", \"\")\n\n        response = self.client.get_object(self.bucket, path)\n        return pickle.loads(response.read())\n\n    def exists(self, path: str) -&gt; bool:\n        try:\n            self.client.stat_object(self.bucket, path)\n            return True\n        except:\n            return False\n\n    def to_dict(self):\n        return {\n            \"type\": \"minio\",\n            \"endpoint\": self.endpoint,\n            \"bucket\": self.bucket\n        }\n</code></pre>"},{"location":"tutorials/extensible-pipeline/#test-custom-component","title":"Test Custom Component","text":"<pre><code># Install MinIO client\npip install minio\n\n# Start MinIO (Docker)\ndocker run -d \\\n  -p 9000:9000 \\\n  -p 9001:9001 \\\n  --name minio \\\n  minio/minio server /data --console-address \":9001\"\n\n# Load component\nflowyml component load custom_components.minio_store\n\n# Verify\nflowyml component list\n</code></pre>"},{"location":"tutorials/extensible-pipeline/#step-5-multi-environment-configuration","title":"Step 5: Multi-Environment Configuration","text":"<p>Update <code>flowyml.yaml</code>:</p> <pre><code># Multi-environment configuration\n\nstacks:\n  # Local development\n  local:\n    type: local\n    artifact_store:\n      path: .flowyml/artifacts\n    metadata_store:\n      path: .flowyml/metadata.db\n\n  # Development with MinIO\n  dev_minio:\n    type: local\n    artifact_store:\n      type: minio\n      endpoint: localhost:9000\n      bucket: ml-dev\n    metadata_store:\n      path: .flowyml/metadata.db\n\n  # Staging on GCP\n  staging:\n    type: gcp\n    project_id: ${GCP_PROJECT_ID}\n    region: us-central1\n    artifact_store:\n      type: gcs\n      bucket: ${GCP_STAGING_BUCKET}\n    orchestrator:\n      type: vertex_ai\n\n  # Production on GCP\n  production:\n    type: gcp\n    project_id: ${GCP_PROJECT_ID}\n    region: us-central1\n    artifact_store:\n      type: gcs\n      bucket: ${GCP_PROD_BUCKET}\n    orchestrator:\n      type: vertex_ai\n      service_account: ${GCP_SERVICE_ACCOUNT}\n\ndefault_stack: local\n\nresources:\n  default:\n    cpu: \"2\"\n    memory: \"8Gi\"\n\n  training:\n    cpu: \"8\"\n    memory: \"32Gi\"\n    gpu: \"nvidia-tesla-v100\"\n    gpu_count: 2\n\ndocker:\n  use_poetry: true\n  base_image: python:3.11-slim\n\ncomponents:\n  - module: custom_components.minio_store\n</code></pre>"},{"location":"tutorials/extensible-pipeline/#test-different-stacks","title":"Test Different Stacks","text":"<pre><code># Local\nflowyml run training_pipeline.py\n\n# With MinIO\nflowyml run training_pipeline.py --stack dev_minio\n\n# Staging (dry run)\nflowyml run training_pipeline.py --stack staging --dry-run\n\n# Production with GPUs\nflowyml run training_pipeline.py \\\n  --stack production \\\n  --resources training \\\n  --context epochs=50\n</code></pre>"},{"location":"tutorials/extensible-pipeline/#step-6-create-dockerfile","title":"Step 6: Create Dockerfile","text":"<p>Create <code>Dockerfile</code>:</p> <pre><code>FROM python:3.11-slim\n\n# Install system dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    git \\\n    build-essential \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Set working directory\nWORKDIR /app\n\n# Copy requirements\nCOPY pyproject.toml* requirements.txt* ./\n\n# Install Python dependencies\nRUN if [ -f pyproject.toml ]; then \\\n        pip install poetry &amp;&amp; poetry install --no-dev; \\\n    elif [ -f requirements.txt ]; then \\\n        pip install -r requirements.txt; \\\n    fi\n\n# Install flowyml\nRUN pip install flowyml[tensorflow,gcp]\n\n# Copy code\nCOPY . .\n\n# Set entrypoint\nENTRYPOINT [\"python\"]\nCMD [\"training_pipeline.py\"]\n</code></pre>"},{"location":"tutorials/extensible-pipeline/#build-and-test","title":"Build and Test","text":"<pre><code># Build image\ndocker build -t ml-pipeline:latest .\n\n# Test locally\ndocker run ml-pipeline:latest\n\n# Push to registry\ndocker tag ml-pipeline:latest gcr.io/my-project/ml-pipeline:v1\ndocker push gcr.io/my-project/ml-pipeline:v1\n</code></pre>"},{"location":"tutorials/extensible-pipeline/#step-7-cicd-setup","title":"Step 7: CI/CD Setup","text":"<p>Create <code>.github/workflows/ml-pipeline.yml</code>:</p> <pre><code>name: ML Training Pipeline\n\non:\n  push:\n    branches: [main]\n  workflow_dispatch:\n    inputs:\n      environment:\n        description: 'Environment to deploy to'\n        required: true\n        default: 'staging'\n        type: choice\n        options:\n          - staging\n          - production\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.11'\n\n      - name: Install dependencies\n        run: |\n          pip install flowyml[tensorflow]\n          pip install minio  # For custom component\n\n      - name: Run tests\n        run: |\n          flowyml run training_pipeline.py --dry-run\n\n  deploy:\n    needs: test\n    runs-on: ubuntu-latest\n    if: github.ref == 'refs/heads/main'\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.11'\n\n      - name: Install flowyml\n        run: |\n          pip install flowyml[tensorflow,gcp]\n\n      - name: Authenticate to Google Cloud\n        uses: google-github-actions/auth@v1\n        with:\n          credentials_json: ${{ secrets.GCP_SA_KEY }}\n\n      - name: Run pipeline on GCP\n        env:\n          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}\n          GCP_STAGING_BUCKET: ${{ secrets.GCP_STAGING_BUCKET }}\n          GCP_PROD_BUCKET: ${{ secrets.GCP_PROD_BUCKET }}\n          GCP_SERVICE_ACCOUNT: ${{ secrets.GCP_SERVICE_ACCOUNT }}\n        run: |\n          ENV=${{ github.event.inputs.environment || 'staging' }}\n\n          flowyml run training_pipeline.py \\\n            --stack $ENV \\\n            --resources training \\\n            --context experiment_name=github-${{ github.run_id }}\n</code></pre>"},{"location":"tutorials/extensible-pipeline/#step-8-production-deployment","title":"Step 8: Production Deployment","text":""},{"location":"tutorials/extensible-pipeline/#verify-configuration","title":"Verify Configuration","text":"<pre><code># Check stack configuration\nflowyml stack show production\n\n# Dry run\nflowyml run training_pipeline.py \\\n  --stack production \\\n  --resources training \\\n  --dry-run\n</code></pre>"},{"location":"tutorials/extensible-pipeline/#deploy","title":"Deploy","text":"<pre><code># Run on production\nflowyml run training_pipeline.py \\\n  --stack production \\\n  --resources training \\\n  --context epochs=100 \\\n  --context experiment_name=prod-v1\n</code></pre>"},{"location":"tutorials/extensible-pipeline/#what-youve-learned","title":"What You've Learned","text":"<p>\u2705 Clean pipeline code - no infrastructure coupling \u2705 Custom components - MinIO artifact store \u2705 Multi-environment setup - dev, staging, production \u2705 Configuration-driven - same code, different infra \u2705 Docker integration - containerized execution \u2705 CI/CD automation - GitHub Actions deployment</p>"},{"location":"tutorials/extensible-pipeline/#next-steps","title":"Next Steps","text":"<ol> <li>Add more custom components</li> <li>Airflow orchestrator</li> <li>Redis cache</li> <li> <p>Custom metrics tracker</p> </li> <li> <p>Enhance pipeline</p> </li> <li>Hyperparameter tuning</li> <li>Model registry integration</li> <li> <p>A/B testing</p> </li> <li> <p>Monitor and optimize</p> </li> <li>Add logging</li> <li>Track metrics</li> <li> <p>Optimize resources</p> </li> <li> <p>Share components</p> </li> <li>Package as pip installable</li> <li>Publish to PyPI</li> <li>Contribute to community</li> </ol>"},{"location":"tutorials/extensible-pipeline/#resources","title":"Resources","text":"<ul> <li>Components Guide</li> <li>Configuration Guide</li> <li>CLI Reference</li> <li>Example Code</li> </ul>"},{"location":"tutorials/extensible-pipeline/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/extensible-pipeline/#minio-connection-issues","title":"MinIO Connection Issues","text":"<pre><code># Check MinIO is running\ndocker ps | grep minio\n\n# Test connection\npython -c \"from minio import Minio; Minio('localhost:9000', 'minioadmin', 'minioadmin').list_buckets()\"\n</code></pre>"},{"location":"tutorials/extensible-pipeline/#gcp-authentication-issues","title":"GCP Authentication Issues","text":"<pre><code># Check authentication\ngcloud auth list\n\n# Re-authenticate\ngcloud auth login\ngcloud auth application-default login\n</code></pre>"},{"location":"tutorials/extensible-pipeline/#component-not-loading","title":"Component Not Loading","text":"<pre><code># Check Python path\npython -c \"import sys; print('\\n'.join(sys.path))\"\n\n# Load explicitly\nflowyml component load custom_components.minio_store\n\n# Verify\nflowyml component list\n</code></pre> <p>Congratulations! You've built a production-ready, extensible ML pipeline! \ud83c\udf89</p>"},{"location":"user-guide/api-execution/","title":"API-Based Execution &amp; Token Management \ud83d\ude80","text":"<p>flowyml provides a robust REST API for executing pipelines, managing tokens, and interacting with the system programmatically. This allows you to integrate flowyml into your existing infrastructure, such as CI/CD pipelines or custom dashboards.</p>"},{"location":"user-guide/api-execution/#token-management","title":"\ud83d\udd11 Token Management","text":"<p>Secure access to the flowyml API is managed via API tokens.</p>"},{"location":"user-guide/api-execution/#creating-tokens","title":"Creating Tokens","text":"<p>You can generate tokens via the UI or CLI.</p> <p>Using CLI: <pre><code># Generate a new token\nflowyml token create --name \"ci-cd-token\" --role \"admin\"\n\n# List tokens\nflowyml token list\n\n# Revoke a token\nflowyml token revoke --token-id &lt;token_id&gt;\n</code></pre></p> <p>Using UI: 1. Navigate to Settings &gt; API Tokens. 2. Click Generate New Token. 3. Copy the token immediately; it won't be shown again.</p>"},{"location":"user-guide/api-execution/#using-tokens","title":"Using Tokens","text":"<p>Include the token in the <code>Authorization</code> header of your HTTP requests:</p> <pre><code>Authorization: Bearer &lt;your_token&gt;\n</code></pre>"},{"location":"user-guide/api-execution/#api-based-pipeline-execution","title":"\ud83d\ude80 API-Based Pipeline Execution","text":"<p>You can trigger pipelines remotely using the REST API.</p>"},{"location":"user-guide/api-execution/#trigger-a-run","title":"Trigger a Run","text":"<p>Endpoint: <code>POST /api/v1/runs</code></p> <p>Request Body: <pre><code>{\n  \"pipeline_name\": \"training_pipeline\",\n  \"project\": \"default\",\n  \"parameters\": {\n    \"learning_rate\": 0.01,\n    \"epochs\": 20\n  }\n}\n</code></pre></p> <p>Example (curl): <pre><code>curl -X POST http://localhost:8080/api/v1/runs \\\n  -H \"Authorization: Bearer &lt;your_token&gt;\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"pipeline_name\": \"training_pipeline\",\n    \"parameters\": {\"epochs\": 50}\n  }'\n</code></pre></p>"},{"location":"user-guide/api-execution/#check-run-status","title":"Check Run Status","text":"<p>Endpoint: <code>GET /api/v1/runs/{run_id}</code></p> <p>Response: <pre><code>{\n  \"run_id\": \"run_12345\",\n  \"status\": \"running\",\n  \"created_at\": \"2023-10-27T10:00:00Z\",\n  \"pipeline_name\": \"training_pipeline\"\n}\n</code></pre></p>"},{"location":"user-guide/api-execution/#list-runs","title":"List Runs","text":"<p>Endpoint: <code>GET /api/v1/runs</code></p> <p>Query Parameters: - <code>pipeline_name</code>: Filter by pipeline. - <code>status</code>: Filter by status (e.g., <code>success</code>, <code>failed</code>). - <code>limit</code>: Number of results to return.</p>"},{"location":"user-guide/api-execution/#api-reference","title":"\ud83d\udcda API Reference","text":"<p>For a complete list of endpoints, visit the interactive API documentation (Swagger UI) at:</p> <p><code>http://localhost:8080/docs</code></p>"},{"location":"user-guide/cli/","title":"CLI Reference \ud83d\udcbb","text":"<p>The flowyml Command Line Interface (CLI) is your primary tool for managing projects, running pipelines, and controlling the UI.</p>"},{"location":"user-guide/cli/#installation","title":"Installation","text":"<p>The CLI is installed automatically with the package:</p> <pre><code>pip install flowyml\n</code></pre> <p>Verify installation:</p> <pre><code>flowyml --version\n</code></pre>"},{"location":"user-guide/cli/#command-structure","title":"Command Structure","text":"<pre><code>flowyml [COMMAND] [SUBCOMMAND] [OPTIONS]\n</code></pre>"},{"location":"user-guide/cli/#commands","title":"Commands","text":""},{"location":"user-guide/cli/#init","title":"<code>init</code> \ud83c\udf31","text":"<p>Initialize a new flowyml project.</p> <pre><code>flowyml init [PROJECT_NAME]\n</code></pre> <p>Options: - <code>--template [NAME]</code>: Use a specific template (default: <code>basic</code>). Available templates: <code>basic</code>, <code>ml</code>, <code>cv</code>. - <code>--force</code>: Overwrite existing directory.</p> <p>Example: <pre><code>flowyml init my-ml-project --template ml\n</code></pre></p>"},{"location":"user-guide/cli/#ui","title":"<code>ui</code> \ud83d\udda5\ufe0f","text":"<p>Manage the flowyml UI server.</p>"},{"location":"user-guide/cli/#ui-start","title":"<code>ui start</code>","text":"<p>Start the UI server.</p> <pre><code>flowyml ui start\n</code></pre> <p>Options: - <code>--port [PORT]</code>: Port for the frontend (default: 8080). - <code>--backend-port [PORT]</code>: Port for the backend API (default: 8000). - <code>--host [HOST]</code>: Host to bind to (default: 127.0.0.1). - <code>--daemon</code>: Run in background (daemon mode).</p>"},{"location":"user-guide/cli/#ui-stop","title":"<code>ui stop</code>","text":"<p>Stop the running UI server.</p> <pre><code>flowyml ui stop\n</code></pre>"},{"location":"user-guide/cli/#ui-status","title":"<code>ui status</code>","text":"<p>Check if the UI server is running.</p> <pre><code>flowyml ui status\n</code></pre>"},{"location":"user-guide/cli/#run","title":"<code>run</code> \u25b6\ufe0f","text":"<p>Execute a pipeline or script.</p> <pre><code>flowyml run [SCRIPT_PATH]\n</code></pre> <p>Options: - <code>--pipeline [NAME]</code>: Name of the pipeline to run (if script contains multiple). - <code>--param [KEY=VALUE]</code>: Override context parameters.</p> <p>Example: <pre><code>flowyml run src/pipelines/training.py --param epochs=50\n</code></pre></p>"},{"location":"user-guide/cli/#cache","title":"<code>cache</code> \ud83e\uddf9","text":"<p>Manage the execution cache.</p>"},{"location":"user-guide/cli/#cache-clear","title":"<code>cache clear</code>","text":"<p>Clear the cache.</p> <pre><code>flowyml cache clear\n</code></pre> <p>Options: - <code>--pipeline [NAME]</code>: Clear cache only for a specific pipeline. - <code>--days [N]</code>: Clear cache entries older than N days.</p>"},{"location":"user-guide/cli/#config","title":"<code>config</code> \u2699\ufe0f","text":"<p>View or modify configuration.</p> <pre><code>flowyml config\n</code></pre>"},{"location":"user-guide/cli/#config-list","title":"<code>config list</code>","text":"<p>List all current configuration values.</p>"},{"location":"user-guide/cli/#config-set","title":"<code>config set</code>","text":"<p>Set a configuration value.</p> <pre><code>flowyml config set ui.port 3000\n</code></pre>"},{"location":"user-guide/cli/#environment-variables","title":"Environment Variables \ud83c\udf10","text":"<p>You can also configure flowyml using environment variables. All variables are prefixed with <code>flowyml_</code>.</p> <ul> <li><code>flowyml_HOME</code>: Path to the flowyml home directory (default: <code>~/.flowyml</code>).</li> <li><code>flowyml_ENV</code>: Environment name (e.g., <code>dev</code>, <code>prod</code>).</li> <li><code>flowyml_UI_PORT</code>: Port for the UI.</li> <li><code>flowyml_LOG_LEVEL</code>: Logging level (DEBUG, INFO, WARNING, ERROR).</li> </ul>"},{"location":"user-guide/components/","title":"Stack Components and Extensibility","text":""},{"location":"user-guide/components/#overview","title":"Overview","text":"<p>flowyml's stack system is built on a powerful plugin architecture that makes it easy to extend with custom components, integrate with existing tools, and even reuse components from the ZenML ecosystem.</p>"},{"location":"user-guide/components/#table-of-contents","title":"\ud83d\udcda Table of Contents","text":"<ul> <li>Core Concepts</li> <li>Built-in Components</li> <li>Creating Custom Components</li> <li>Component Registration</li> <li>Using Custom Components</li> <li>Publishing Components</li> <li>ZenML Integration</li> <li>API Reference</li> </ul>"},{"location":"user-guide/components/#core-concepts","title":"Core Concepts","text":""},{"location":"user-guide/components/#what-is-a-stack-component","title":"What is a Stack Component?","text":"<p>A stack component is a modular piece of infrastructure that performs a specific function in your ML pipeline:</p> <ul> <li>Orchestrator: Manages pipeline execution and scheduling</li> <li>Artifact Store: Stores pipeline artifacts and outputs</li> <li>Container Registry: Manages Docker images</li> <li>Metadata Store: Tracks pipeline runs and lineage</li> </ul>"},{"location":"user-guide/components/#component-hierarchy","title":"Component Hierarchy","text":"<pre><code>StackComponent (Base Class)\n\u251c\u2500\u2500 Orchestrator\n\u2502   \u251c\u2500\u2500 VertexAIOrchestrator\n\u2502   \u251c\u2500\u2500 AirflowOrchestrator (custom)\n\u2502   \u2514\u2500\u2500 KubernetesOrchestrator (custom)\n\u2502\n\u251c\u2500\u2500 ArtifactStore\n\u2502   \u251c\u2500\u2500 LocalArtifactStore\n\u2502   \u251c\u2500\u2500 GCSArtifactStore\n\u2502   \u251c\u2500\u2500 S3ArtifactStore (custom)\n\u2502   \u2514\u2500\u2500 MinIOArtifactStore (custom)\n\u2502\n\u2514\u2500\u2500 ContainerRegistry\n    \u251c\u2500\u2500 GCRContainerRegistry\n    \u251c\u2500\u2500 ECRContainerRegistry (custom)\n    \u2514\u2500\u2500 DockerHubRegistry (custom)\n</code></pre>"},{"location":"user-guide/components/#built-in-components","title":"Built-in Components","text":""},{"location":"user-guide/components/#local-stack-components","title":"Local Stack Components","text":"<p>LocalExecutor - Runs steps in the current process - Perfect for development and testing - No external dependencies</p> <p>LocalArtifactStore - Stores artifacts on local filesystem - Fast and simple - Good for prototyping</p> <p>SQLiteMetadataStore - Tracks runs in SQLite database - Lightweight and portable - No server required</p>"},{"location":"user-guide/components/#gcp-stack-components","title":"GCP Stack Components","text":"<p>VertexAIOrchestrator - Managed ML platform on Google Cloud - Scalable and reliable - Integrated with GCP services</p> <p>GCSArtifactStore - Google Cloud Storage integration - Durable and scalable - Global availability</p> <p>GCRContainerRegistry - Google Container Registry - Integrated with GCP - Automated builds</p>"},{"location":"user-guide/components/#creating-custom-components","title":"Creating Custom Components","text":""},{"location":"user-guide/components/#basic-component-structure","title":"Basic Component Structure","text":"<p>Every component must: 1. Inherit from the appropriate base class 2. Implement required methods 3. Register itself (optionally via decorator)</p>"},{"location":"user-guide/components/#example-custom-orchestrator","title":"Example: Custom Orchestrator","text":"<pre><code>from flowyml.stacks.components import Orchestrator, ResourceConfig, DockerConfig\nfrom flowyml.stacks.plugins import register_component\nfrom typing import Any\n\n@register_component\nclass AirflowOrchestrator(Orchestrator):\n    \"\"\"\n    Apache Airflow orchestrator for flowyml.\n\n    Converts flowyml pipelines to Airflow DAGs and manages execution.\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str = \"airflow\",\n        airflow_home: str = \"~/airflow\",\n        dag_folder: str = \"~/airflow/dags\",\n    ):\n        \"\"\"Initialize Airflow orchestrator.\"\"\"\n        super().__init__(name)\n        self.airflow_home = airflow_home\n        self.dag_folder = dag_folder\n\n    def validate(self) -&gt; bool:\n        \"\"\"Validate Airflow is installed and configured.\"\"\"\n        try:\n            import airflow\n            from pathlib import Path\n\n            # Check DAG folder exists\n            dag_path = Path(self.dag_folder).expanduser()\n            if not dag_path.exists():\n                dag_path.mkdir(parents=True)\n\n            return True\n        except ImportError:\n            raise ImportError(\n                \"Apache Airflow not installed. \"\n                \"Install with: pip install apache-airflow\"\n            )\n\n    def run_pipeline(\n        self,\n        pipeline: Any,\n        resources: ResourceConfig = None,\n        docker_config: DockerConfig = None,\n        **kwargs\n    ) -&gt; str:\n        \"\"\"\n        Convert pipeline to Airflow DAG and execute.\n\n        Args:\n            pipeline: flowyml pipeline to execute\n            resources: Resource configuration (optional)\n            docker_config: Docker configuration (optional)\n            **kwargs: Additional arguments\n\n        Returns:\n            DAG run ID\n        \"\"\"\n        from airflow import DAG\n        from airflow.operators.python import PythonOperator\n        from datetime import datetime\n\n        # Create Airflow DAG\n        dag = DAG(\n            dag_id=pipeline.name,\n            default_args={'owner': 'flowyml'},\n            start_date=datetime.now(),\n            schedule_interval=None,\n        )\n\n        # Convert steps to tasks\n        tasks = {}\n        for step in pipeline.steps:\n            task = PythonOperator(\n                task_id=step.name,\n                python_callable=step.func,\n                dag=dag,\n            )\n            tasks[step.name] = task\n\n        # Set dependencies\n        for i in range(len(pipeline.steps) - 1):\n            tasks[pipeline.steps[i].name] &gt;&gt; tasks[pipeline.steps[i+1].name]\n\n        # Trigger DAG run\n        run_id = f\"flowyml_{pipeline.run_id}\"\n        dag.create_dagrun(run_id=run_id, state='running')\n\n        return run_id\n\n    def get_run_status(self, run_id: str) -&gt; str:\n        \"\"\"Get DAG run status.\"\"\"\n        from airflow.models import DagRun\n\n        dagrun = DagRun.find(run_id=run_id)\n        return dagrun[0].state if dagrun else \"UNKNOWN\"\n\n    def to_dict(self):\n        \"\"\"Serialize configuration.\"\"\"\n        return {\n            \"type\": \"airflow\",\n            \"airflow_home\": self.airflow_home,\n            \"dag_folder\": self.dag_folder,\n        }\n</code></pre>"},{"location":"user-guide/components/#example-custom-artifact-store","title":"Example: Custom Artifact Store","text":"<pre><code>from flowyml.stacks.components import ArtifactStore\nfrom flowyml.stacks.plugins import register_component\nfrom typing import Any\n\n@register_component\nclass MinIOArtifactStore(ArtifactStore):\n    \"\"\"\n    MinIO object storage integration.\n\n    MinIO is an S3-compatible object storage system that can run\n    on-premises or in the cloud.\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str = \"minio\",\n        endpoint: str = \"localhost:9000\",\n        bucket: str = \"flowyml\",\n        access_key: str = \"\",\n        secret_key: str = \"\",\n        secure: bool = False,\n    ):\n        \"\"\"Initialize MinIO artifact store.\"\"\"\n        super().__init__(name)\n        self.endpoint = endpoint\n        self.bucket = bucket\n        self.access_key = access_key\n        self.secret_key = secret_key\n        self.secure = secure\n        self._client = None\n\n    @property\n    def client(self):\n        \"\"\"Lazy-load MinIO client.\"\"\"\n        if self._client is None:\n            from minio import Minio\n\n            self._client = Minio(\n                self.endpoint,\n                access_key=self.access_key,\n                secret_key=self.secret_key,\n                secure=self.secure,\n            )\n\n            # Ensure bucket exists\n            if not self._client.bucket_exists(self.bucket):\n                self._client.make_bucket(self.bucket)\n\n        return self._client\n\n    def validate(self) -&gt; bool:\n        \"\"\"Validate MinIO connection.\"\"\"\n        try:\n            from minio import Minio\n            # Try to connect\n            _ = self.client\n            return True\n        except ImportError:\n            raise ImportError(\n                \"MinIO client not installed. \"\n                \"Install with: pip install minio\"\n            )\n        except Exception as e:\n            raise ConnectionError(f\"Cannot connect to MinIO: {e}\")\n\n    def save(self, artifact: Any, path: str) -&gt; str:\n        \"\"\"Save artifact to MinIO.\"\"\"\n        import pickle\n        import io\n\n        # Serialize artifact\n        data = pickle.dumps(artifact)\n        data_stream = io.BytesIO(data)\n\n        # Upload\n        self.client.put_object(\n            self.bucket,\n            path,\n            data_stream,\n            length=len(data),\n        )\n\n        return f\"s3://{self.bucket}/{path}\"\n\n    def load(self, path: str) -&gt; Any:\n        \"\"\"Load artifact from MinIO.\"\"\"\n        import pickle\n\n        # Handle s3:// URIs\n        if path.startswith(\"s3://\"):\n            path = path.replace(f\"s3://{self.bucket}/\", \"\")\n\n        # Download\n        response = self.client.get_object(self.bucket, path)\n        data = response.read()\n\n        return pickle.loads(data)\n\n    def exists(self, path: str) -&gt; bool:\n        \"\"\"Check if artifact exists.\"\"\"\n        try:\n            self.client.stat_object(self.bucket, path)\n            return True\n        except:\n            return False\n\n    def to_dict(self):\n        \"\"\"Serialize configuration.\"\"\"\n        return {\n            \"type\": \"minio\",\n            \"endpoint\": self.endpoint,\n            \"bucket\": self.bucket,\n            \"secure\": self.secure,\n        }\n</code></pre>"},{"location":"user-guide/components/#required-methods","title":"Required Methods","text":"<p>All components must implement:</p> <p><code>validate() -&gt; bool</code> - Verify component is properly configured - Check dependencies are installed - Test connections if applicable - Raise descriptive errors if validation fails</p> <p><code>to_dict() -&gt; Dict[str, Any]</code> - Serialize component configuration - Used for persistence and display - Should include all important settings</p> <p>Component-specific methods:</p> <p>For Orchestrator: - <code>run_pipeline(pipeline, **kwargs) -&gt; str</code>: Execute pipeline, return run ID - <code>get_run_status(run_id: str) -&gt; str</code>: Get execution status</p> <p>For ArtifactStore: - <code>save(artifact: Any, path: str) -&gt; str</code>: Save artifact, return URI - <code>load(path: str) -&gt; Any</code>: Load and return artifact - <code>exists(path: str) -&gt; bool</code>: Check if artifact exists</p> <p>For ContainerRegistry: - <code>push_image(image_name: str, tag: str) -&gt; str</code>: Push image, return URI - <code>pull_image(image_name: str, tag: str)</code>: Pull image - <code>get_image_uri(image_name: str, tag: str) -&gt; str</code>: Get full image URI</p>"},{"location":"user-guide/components/#component-registration","title":"Component Registration","text":""},{"location":"user-guide/components/#method-1-decorator-recommended","title":"Method 1: Decorator (Recommended)","text":"<pre><code>from flowyml.stacks.plugins import register_component\n\n@register_component\nclass MyComponent(Orchestrator):\n    pass\n\n# Or with custom name\n@register_component(name=\"my_custom_name\")\nclass MyComponent(Orchestrator):\n    pass\n</code></pre> <p>Advantages: - Clean and declarative - Auto-registration on import - No additional code needed</p>"},{"location":"user-guide/components/#method-2-manual-registration","title":"Method 2: Manual Registration","text":"<pre><code>from flowyml.stacks.plugins import get_component_registry\n\nclass MyComponent(Orchestrator):\n    pass\n\n# Register manually\nregistry = get_component_registry()\nregistry.register(MyComponent, \"my_component\")\n</code></pre> <p>Advantages: - More control over registration - Can register at runtime - Useful for dynamic components</p>"},{"location":"user-guide/components/#method-3-entry-points-best-for-packages","title":"Method 3: Entry Points (Best for Packages)","text":"<pre><code># pyproject.toml\n[project.entry-points.\"flowyml.stack_components\"]\nmy_orchestrator = \"my_package.components:MyOrchestrator\"\nmy_store = \"my_package.stores:MyArtifactStore\"\n</code></pre> <p>Advantages: - Auto-discovery on package installation - No import needed - Standard Python packaging mechanism - Discoverable by tools</p>"},{"location":"user-guide/components/#method-4-dynamic-loading","title":"Method 4: Dynamic Loading","text":"<pre><code>from flowyml.stacks.plugins import load_component\n\n# From module\nload_component(\"my_package.components\")\n\n# From file\nload_component(\"/path/to/component.py:MyClass\")\n\n# From ZenML\nload_component(\"zenml:zenml.orchestrators.kubernetes.KubernetesOrchestrator\")\n</code></pre> <p>Advantages: - Load components on demand - No code changes - Support for external sources - CLI-friendly</p>"},{"location":"user-guide/components/#using-custom-components","title":"Using Custom Components","text":""},{"location":"user-guide/components/#in-configuration-files","title":"In Configuration Files","text":"<pre><code># flowyml.yaml\nstacks:\n  custom_stack:\n    type: local\n    orchestrator:\n      type: airflow  # Your custom orchestrator\n      dag_folder: ~/airflow/dags\n\n    artifact_store:\n      type: minio  # Your custom artifact store\n      endpoint: localhost:9000\n      bucket: ml-artifacts\n      access_key: ${MINIO_ACCESS_KEY}\n      secret_key: ${MINIO_SECRET_KEY}\n\nresources:\n  default:\n    cpu: \"2\"\n    memory: \"8Gi\"\n</code></pre>"},{"location":"user-guide/components/#programmatically","title":"Programmatically","text":"<pre><code>from my_components import AirflowOrchestrator, MinIOArtifactStore\nfrom flowyml.stacks import Stack\nfrom flowyml.storage.metadata import SQLiteMetadataStore\n\n# Create components\norchestrator = AirflowOrchestrator(dag_folder=\"~/airflow/dags\")\nartifact_store = MinIOArtifactStore(\n    endpoint=\"localhost:9000\",\n    bucket=\"ml-artifacts\"\n)\nmetadata_store = SQLiteMetadataStore()\n\n# Create stack\nstack = Stack(\n    name=\"custom\",\n    executor=None,  # Airflow handles execution\n    artifact_store=artifact_store,\n    metadata_store=metadata_store,\n    orchestrator=orchestrator,\n)\n\n# Use with pipeline\nfrom flowyml import Pipeline\n\npipeline = Pipeline(\"my_pipeline\", stack=stack)\nresult = pipeline.run()\n</code></pre>"},{"location":"user-guide/components/#via-cli","title":"Via CLI","text":"<pre><code># Load custom component\nflowyml component load my_components\n\n# List available\nflowyml component list\n\n# Run with custom stack\nflowyml run pipeline.py --stack custom_stack\n</code></pre>"},{"location":"user-guide/components/#publishing-components","title":"Publishing Components","text":""},{"location":"user-guide/components/#package-structure","title":"Package Structure","text":"<pre><code>flowyml-airflow/\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 tests/\n\u2502   \u2514\u2500\u2500 test_orchestrator.py\n\u2514\u2500\u2500 flowyml_airflow/\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 orchestrator.py\n</code></pre>"},{"location":"user-guide/components/#pyprojecttoml","title":"pyproject.toml","text":"<pre><code>[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n[project]\nname = \"flowyml-airflow\"\nversion = \"0.1.0\"\ndescription = \"Apache Airflow orchestrator for flowyml\"\nauthors = [{name = \"Your Name\", email = \"you@example.com\"}]\nreadme = \"README.md\"\nlicense = {text = \"Apache-2.0\"}\nrequires-python = \"&gt;=3.8\"\ndependencies = [\n    \"flowyml&gt;=0.1.0\",\n    \"apache-airflow&gt;=2.5.0\",\n]\n\nkeywords = [\"flowyml\", \"airflow\", \"ml\", \"orchestration\", \"plugin\"]\nclassifiers = [\n    \"Development Status :: 3 - Alpha\",\n    \"Intended Audience :: Developers\",\n    \"Topic :: Software Development :: Libraries\",\n]\n\n[project.urls]\nHomepage = \"https://github.com/yourusername/flowyml-airflow\"\nDocumentation = \"https://flowyml-airflow.readthedocs.io\"\n\n# Entry point registration\n[project.entry-points.\"flowyml.stack_components\"]\nairflow = \"flowyml_airflow.orchestrator:AirflowOrchestrator\"\n\n[tool.hatch.build.targets.wheel]\npackages = [\"flowyml_airflow\"]\n</code></pre>"},{"location":"user-guide/components/#publishing-workflow","title":"Publishing Workflow","text":"<ol> <li> <p>Build package: <pre><code>python -m build\n</code></pre></p> </li> <li> <p>Test locally: <pre><code>pip install -e .\nflowyml component list  # Should show your component\n</code></pre></p> </li> <li> <p>Upload to PyPI: <pre><code>python -m twine upload dist/*\n</code></pre></p> </li> <li> <p>Users install: <pre><code>pip install flowyml-airflow\n# Component auto-available!\n</code></pre></p> </li> </ol>"},{"location":"user-guide/components/#readme-template","title":"README Template","text":"<pre><code># flowyml Airflow Orchestrator\n\nApache Airflow orchestrator plugin for flowyml.\n\n## Installation\n\n```bash\npip install flowyml-airflow\n</code></pre>"},{"location":"user-guide/components/#usage","title":"Usage","text":"<pre><code># flowyml.yaml\nstacks:\n  airflow_stack:\n    orchestrator:\n      type: air flow\n      dag_folder: ~/airflow/dags\n</code></pre> <pre><code>flowyml run pipeline.py --stack airflow_stack\n</code></pre>"},{"location":"user-guide/components/#configuration","title":"Configuration","text":"<ul> <li><code>dag_folder</code>: Path to Airflow DAGs folder</li> <li><code>airflow_home</code>: Airflow home directory (optional)</li> </ul>"},{"location":"user-guide/components/#license","title":"License","text":"<p>Apache-2.0 <pre><code>## ZenML Integration\n\n### Wrapping ZenML Components\n\n```python\nfrom flowyml.stacks.plugins import get_component_registry\n\n# Import ZenML component\nfrom zenml.integrations.kubernetes.orchestrators import KubernetesOrchestrator\n\n# Wrap it\nregistry = get_component_registry()\nregistry.wrap_zenml_component(\n    KubernetesOrchestrator,\n    name=\"k8s\"\n)\n\n# Use immediately!\n</code></pre></p>"},{"location":"user-guide/components/#via-configuration","title":"Via Configuration","text":"<pre><code># flowyml.yaml\ncomponents:\n  - zenml: zenml.integrations.kubernetes.orchestrators.KubernetesOrchestrator\n    name: k8s\n\n  - zenml: zenml.integrations.aws.artifact_stores.S3ArtifactStore\n    name: s3\n\nstacks:\n  zenml_stack:\n    orchestrator:\n      type: k8s\n    artifact_store:\n      type: s3\n</code></pre>"},{"location":"user-guide/components/#complete-stack-migration","title":"Complete Stack Migration","text":"<pre><code>from zenml.client import Client\nfrom flowyml.stacks.plugins import get_component_registry\nfrom flowyml.stacks import Stack\n\n# Get ZenML stack\nzenml_client = Client()\nzenml_stack = zenml_client.active_stack\n\n# Wrap all components\nregistry = get_component_registry()\nregistry.wrap_zenml_component(zenml_stack.orchestrator, \"orch\")\nregistry.wrap_zenml_component(zenml_stack.artifact_store, \"store\")\n\n# Create flowyml stack\nflowyml_stack = Stack(\n    name=f\"migrated_{zenml_stack.name}\",\n    orchestrator=registry.get_orchestrator(\"orch\"),\n    artifact_store=registry.get_artifact_store(\"store\"),\n    metadata_store=None,  # Use local\n)\n\n# Use with flowyml pipelines!\n</code></pre>"},{"location":"user-guide/components/#api-reference","title":"API Reference","text":""},{"location":"user-guide/components/#componentregistry","title":"ComponentRegistry","text":"<p><code>register(component_class, name=None)</code> Register a component class.</p> <p><code>get_orchestrator(name) -&gt; Type[Orchestrator]</code> Get orchestrator class by name.</p> <p><code>get_artifact_store(name) -&gt; Type[ArtifactStore]</code> Get artifact store class by name.</p> <p><code>list_all() -&gt; Dict[str, List[str]]</code> List all registered components.</p> <p><code>load_from_module(module_path)</code> Load all components from a module.</p> <p><code>wrap_zenml_component(zenml_class, name)</code> Wrap a ZenML component for flowyml.</p>"},{"location":"user-guide/components/#decorators","title":"Decorators","text":"<p><code>@register_component</code> Auto-register a component class.</p> <p><code>@register_component(name=\"custom\")</code> Register with custom name.</p>"},{"location":"user-guide/components/#functions","title":"Functions","text":"<p><code>get_component_registry() -&gt; ComponentRegistry</code> Get global registry instance.</p> <p><code>load_component(source, name=None)</code> Load component from various sources.</p>"},{"location":"user-guide/components/#best-practices","title":"Best Practices","text":"<ol> <li>\u2705 Use type hints for better IDE support</li> <li>\u2705 Add comprehensive docstrings</li> <li>\u2705 Implement proper validation</li> <li>\u2705 Handle errors gracefully</li> <li>\u2705 Write tests for your components</li> <li>\u2705 Document configuration options</li> <li>\u2705 Follow naming conventions</li> <li>\u2705 Use semantic versioning</li> <li>\u2705 Publish to PyPI for easy sharing</li> <li>\u2705 Add examples to README</li> </ol>"},{"location":"user-guide/components/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/components/#component-not-found","title":"Component Not Found","text":"<pre><code># List registered components\nflowyml component list\n\n# Load explicitly\nflowyml component load my_package.components\n</code></pre>"},{"location":"user-guide/components/#import-errors","title":"Import Errors","text":"<pre><code># Check if component module is importable\npython -c \"import my_package.components\"\n\n# Check entry points\npython -c \"from importlib.metadata import entry_points; print(entry_points(group='flowyml.stack_components'))\"\n</code></pre>"},{"location":"user-guide/components/#validation-failures","title":"Validation Failures","text":"<pre><code># Test component validation\nfrom my_components import MyOrchestrator\n\norch = MyOrchestrator()\ntry:\n    orch.validate()\n    print(\"\u2705 Validation passed\")\nexcept Exception as e:\n    print(f\"\u274c Validation failed: {e}\")\n</code></pre>"},{"location":"user-guide/components/#examples","title":"Examples","text":"<p>See: - <code>examples/custom_components/my_components.py</code> - <code>examples/custom_components/zenml_integration.py</code> - <code>examples/custom_components/PACKAGE_TEMPLATE.md</code></p>"},{"location":"user-guide/components/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Guide</li> <li>CLI Reference</li> </ul>"},{"location":"user-guide/configuration/","title":"Configuration: YAML vs. Class-Based \u2699\ufe0f","text":"<p>flowyml offers flexible configuration options to suit different development styles and deployment needs. You can define your pipelines and settings using either Python classes (for maximum flexibility and IDE support) or YAML files (for separation of concerns and easy modification without code changes).</p>"},{"location":"user-guide/configuration/#class-based-configuration-python","title":"\ud83d\udc0d Class-Based Configuration (Python)","text":"<p>This is the default and most powerful way to configure flowyml. It leverages Python's type system and allows for dynamic configuration logic.</p>"},{"location":"user-guide/configuration/#example","title":"Example","text":"<pre><code>from flowyml import context, Pipeline\n\n# Define configuration using Python objects\nctx = context(\n    learning_rate=0.001,\n    batch_size=64,\n    optimizer=\"adam\",\n    layers=[128, 64, 10]\n)\n\npipeline = Pipeline(\"my_pipeline\", context=ctx)\n</code></pre> <p>Pros: - Full power of Python (loops, conditionals, imports). - IDE autocompletion and type checking. - Easy to debug.</p>"},{"location":"user-guide/configuration/#yaml-based-configuration","title":"\ud83d\udcc4 YAML-Based Configuration","text":"<p>YAML configuration is ideal for production deployments where you want to change parameters without modifying code, or for defining pipeline structures declaratively.</p>"},{"location":"user-guide/configuration/#example-pipelineyaml","title":"Example <code>pipeline.yaml</code>","text":"<pre><code>name: training_pipeline\nproject: default\ncontext:\n  learning_rate: 0.001\n  batch_size: 64\n  optimizer: adam\n  layers:\n    - 128\n    - 64\n    - 10\n\nsteps:\n  - name: load_data\n    function: my_module.load_data\n  - name: train\n    function: my_module.train\n    inputs: [load_data]\n</code></pre>"},{"location":"user-guide/configuration/#loading-yaml-configuration","title":"Loading YAML Configuration","text":"<pre><code>from flowyml import Pipeline\n\n# Load pipeline from YAML\npipeline = Pipeline.from_yaml(\"pipeline.yaml\")\n\n# Run it\npipeline.run()\n</code></pre> <p>Pros: - Language-agnostic format. - Easy to inject into CI/CD pipelines. - Clear separation of code and configuration.</p>"},{"location":"user-guide/configuration/#hybrid-approach","title":"\ud83d\udd04 Hybrid Approach","text":"<p>You can also mix both approaches. For example, define the pipeline structure in Python but load specific parameters from a YAML file.</p> <pre><code>import yaml\nfrom flowyml import context, Pipeline\n\n# Load params\nwith open(\"config.yaml\") as f:\n    config = yaml.safe_load(f)\n\n# Inject into context\nctx = context(**config)\n\npipeline = Pipeline(\"hybrid_pipeline\", context=ctx)\n</code></pre>"},{"location":"user-guide/model-registry/","title":"Model Registry \ud83c\udfdb\ufe0f","text":"<p>The Model Registry is a centralized repository for managing the lifecycle of your machine learning models. It allows you to version, tag, and promote models through different stages (Development, Staging, Production).</p>"},{"location":"user-guide/model-registry/#key-concepts","title":"Key Concepts \ud83d\udddd\ufe0f","text":"<ul> <li>Model Version: A specific iteration of a model, including its artifacts, metrics, and metadata.</li> <li>Stage: The lifecycle state of a model version (<code>Development</code>, <code>Staging</code>, <code>Production</code>, <code>Archived</code>).</li> <li>Promotion: Moving a model version from one stage to another.</li> </ul>"},{"location":"user-guide/model-registry/#using-the-registry","title":"Using the Registry \ud83d\udee0\ufe0f","text":""},{"location":"user-guide/model-registry/#registering-a-model","title":"Registering a Model","text":"<p>You can register a model directly from your pipeline or script.</p> <pre><code>from flowyml import ModelRegistry, ModelStage\n\nregistry = ModelRegistry()\n\n# Register a trained model\nversion = registry.register(\n    model=my_model,\n    name=\"sentiment_classifier\",\n    version=\"v1.0.0\",\n    framework=\"pytorch\",\n    metrics={\"accuracy\": 0.95, \"f1\": 0.94},\n    tags={\"language\": \"en\", \"architecture\": \"bert\"}\n)\n\nprint(f\"Registered model: {version.name} version {version.version}\")\n</code></pre>"},{"location":"user-guide/model-registry/#loading-a-model","title":"Loading a Model \ud83d\udce5","text":"<p>You can load a model by name and version, or by stage.</p> <pre><code># Load specific version\nmodel = registry.load(\"sentiment_classifier\", version=\"v1.0.0\")\n\n# Load latest production model\nprod_model = registry.load(\"sentiment_classifier\", stage=ModelStage.PRODUCTION)\n</code></pre>"},{"location":"user-guide/model-registry/#promoting-a-model","title":"Promoting a Model \ud83d\ude80","text":"<p>Move a model through its lifecycle stages.</p> <pre><code># Promote to Staging\nregistry.promote(\"sentiment_classifier\", \"v1.0.0\", ModelStage.STAGING)\n\n# Promote to Production\nregistry.promote(\"sentiment_classifier\", \"v1.0.0\", ModelStage.PRODUCTION)\n</code></pre>"},{"location":"user-guide/model-registry/#comparing-versions","title":"Comparing Versions \ud83d\udcca","text":"<p>Compare metrics and metadata across different versions.</p> <pre><code>comparison = registry.compare_versions(\n    \"sentiment_classifier\",\n    [\"v1.0.0\", \"v1.1.0\"]\n)\n\nprint(comparison)\n</code></pre>"},{"location":"user-guide/model-registry/#cli-commands","title":"CLI Commands \ud83d\udcbb","text":"<p>You can also manage models via the CLI:</p> <pre><code># List all models\nflowyml models list\n\n# List versions of a model\nflowyml models list sentiment_classifier\n\n# Promote a model\nflowyml models promote sentiment_classifier v1.0.0 --to production\n</code></pre>"},{"location":"user-guide/model-registry/#integration-with-pipelines","title":"Integration with Pipelines \ud83d\udd0c","text":"<p>The Model Registry integrates seamlessly with flowyml pipelines. You can use the <code>Model</code> asset type to automatically register models produced by steps.</p> <pre><code>from flowyml import step, Model\n\n@step\ndef train():\n    # ... training logic ...\n    return Model(\n        data=trained_model,\n        name=\"my_model\",\n        register=True  # Automatically register in Model Registry\n    )\n</code></pre>"},{"location":"user-guide/monitoring/","title":"Monitoring &amp; Alerts \ud83d\udea8","text":"<p>Know when pipelines fail before your users do.</p> <p>[!NOTE] What you'll learn: How to monitor system health and get instant alerts</p> <p>Key insight: Silent failures are the worst kind. Alerts turn invisible problems into actionable notifications.</p>"},{"location":"user-guide/monitoring/#why-monitoring-matters","title":"Why Monitoring Matters","text":"<p>Without monitoring: - Silent failures: A nightly job fails, and you find out at the next standup - Resource waste: Pipelines consume 100% CPU and you don't know why - Slow debugging: \"When did this start failing?\"</p> <p>With flowyml monitoring: - Instant alerts: Slack notification the moment a pipeline fails - Resource visibility: See CPU/memory usage in real-time - Historical data: Track success rates and failure patterns</p>"},{"location":"user-guide/monitoring/#system-monitor","title":"System Monitor \ud83d\udda5\ufe0f","text":"<p>The <code>SystemMonitor</code> tracks CPU and memory usage.</p> <pre><code>from flowyml.monitoring.monitor import SystemMonitor\n\nmonitor = SystemMonitor(\"sys_mon\")\n\n# Check system health\nis_healthy = monitor.check()\n\nif not is_healthy:\n    print(\"System is under high load!\")\n</code></pre>"},{"location":"user-guide/monitoring/#pipeline-monitor","title":"Pipeline Monitor \u26a1","text":"<p>The <code>PipelineMonitor</code> tracks the health of your pipelines, such as consecutive failures.</p> <pre><code>from flowyml.monitoring.monitor import PipelineMonitor\n\nmonitor = PipelineMonitor(\"training_pipeline\")\nmonitor.check()\n</code></pre>"},{"location":"user-guide/monitoring/#alerting","title":"Alerting \ud83d\udd14","text":"<p>flowyml uses an <code>AlertManager</code> to dispatch alerts to configured handlers. By default, alerts are printed to the console, but you can add custom handlers (e.g., Slack, Email).</p>"},{"location":"user-guide/monitoring/#sending-alerts","title":"Sending Alerts","text":"<pre><code>from flowyml.monitoring.alerts import alert_manager, AlertLevel\n\nalert_manager.send_alert(\n    title=\"Model Drift Detected\",\n    message=\"Accuracy dropped below 90%\",\n    level=AlertLevel.WARNING\n)\n</code></pre>"},{"location":"user-guide/monitoring/#real-world-pattern-production-alert-setup","title":"Real-World Pattern: Production Alert Setup","text":"<p>Send critical alerts to Slack, warnings to email.</p> <pre><code>from flowyml.monitoring.alerts import AlertHandler, Alert, AlertLevel\nimport requests\n\nclass SlackAlertHandler(AlertHandler):\n    def handle(self, alert: Alert):\n        # Only send CRITICAL/ERROR to Slack (avoid noise)\n        if alert.level in [AlertLevel.CRITICAL, AlertLevel.ERROR]:\n            requests.post(\n                \"https://hooks.slack.com/services/...\",\n                json={\"text\": f\"\ud83d\udea8 {alert.title}: {alert.message}\"}\n            )\n\n# Register the handler\nalert_manager.add_handler(SlackAlertHandler())\n\n# Now all pipeline failures will ping Slack\n</code></pre> <p>[!TIP] Pro Tip: Use different alert levels to avoid alert fatigue. Reserve CRITICAL for production outages only.</p>"},{"location":"user-guide/monitoring/#cli-monitoring","title":"CLI Monitoring \ud83d\udcbb","text":"<p>You can check system status via the CLI:</p> <pre><code>flowyml monitor status\n</code></pre> <p>Beta Feature</p> <p>CLI monitoring commands are currently in beta and may change in future releases.</p>"},{"location":"user-guide/performance/","title":"Performance Optimization","text":"<p>Tools and techniques for optimizing pipeline performance.</p>"},{"location":"user-guide/performance/#overview","title":"Overview","text":"<p>flowyml provides several performance optimization utilities: - Lazy Evaluation: Defer computations until needed - Parallel Execution: Run steps concurrently - Incremental Computation: Recompute only what changed - GPU Management: Efficient GPU resource allocation - DataFrame Optimization: Reduce memory usage</p>"},{"location":"user-guide/performance/#lazy-evaluation","title":"Lazy Evaluation","text":"<p>Defer expensive computations until their results are actually needed.</p> <pre><code>from flowyml.utils.performance import LazyValue, lazy_property\n\n# Lazy value\nexpensive_data = LazyValue(lambda: load_huge_dataset())\n\n# Not loaded yet!\nprint(\"Created lazy value\")\n\n# Loaded only when accessed\ndata = expensive_data.value  # Triggers loading\n\n# Subsequent accesses use cached value\ndata2 = expensive_data.value  # Instant!\n</code></pre>"},{"location":"user-guide/performance/#lazy-properties","title":"Lazy Properties","text":"<pre><code>class DataProcessor:\n    def __init__(self, path):\n        self.path = path\n\n    @lazy_property\n    def data(self):\n        \"\"\"Loaded only when first accessed.\"\"\"\n        print(\"Loading data...\")\n        return pd.read_csv(self.path)\n\n    @lazy_property\n    def statistics(self):\n        \"\"\"Computed only when needed.\"\"\"\n        print(\"Computing statistics...\")\n        return self.data.describe()\n\nprocessor = DataProcessor(\"data.csv\")\n# Nothing loaded yet!\n\nstats = processor.statistics  # Triggers data loading + computation\nstats2 = processor.statistics  # Uses cached value\n</code></pre>"},{"location":"user-guide/performance/#parallel-execution","title":"Parallel Execution","text":"<p>Execute independent steps concurrently for faster pipelines.</p> <pre><code>from flowyml.utils.performance import ParallelExecutor\n\nexecutor = ParallelExecutor(max_workers=4)\n\n# Execute multiple functions in parallel\nresults = executor.map(\n    func=process_chunk,\n    iterables=[[chunk1], [chunk2], [chunk3], [chunk4]]\n)\n\n# Or execute different functions\nfutures = {\n    'data1': executor.submit(load_dataset, 'data1.csv'),\n    'data2': executor.submit(load_dataset, 'data2.csv'),\n    'model': executor.submit(load_model, 'model.pkl')\n}\n\n# Wait for all to complete\nresults = executor.wait_all(futures)\n</code></pre>"},{"location":"user-guide/performance/#parallel-pipeline-steps","title":"Parallel Pipeline Steps","text":"<pre><code>from flowyml import Pipeline, step\n\n@step(outputs=[\"chunk1\"])\ndef process_chunk1(data):\n    return process(data[:1000])\n\n@step(outputs=[\"chunk2\"])\ndef process_chunk2(data):\n    return process(data[1000:2000])\n\n@step(outputs=[\"chunk3\"])\ndef process_chunk3(data):\n    return process(data[2000:])\n\n# These steps can run in parallel (no dependencies)\npipeline = Pipeline(\"parallel_processing\")\npipeline.add_step(process_chunk1)\npipeline.add_step(process_chunk2)\npipeline.add_step(process_chunk3)\n\n# Enable parallel execution\nexecutor = ParallelExecutor(max_workers=3)\nresult = pipeline.run(executor=executor)\n</code></pre>"},{"location":"user-guide/performance/#incremental-computation","title":"Incremental Computation","text":"<p>Recompute only what changed, not everything.</p> <pre><code>from flowyml.utils.performance import IncrementalComputation\n\n# Track dependencies\ninc = IncrementalComputation()\n\n# Register computations\ninc.register(\"load_data\", lambda: load_csv(\"data.csv\"))\ninc.register(\"clean_data\", lambda: clean(inc.get(\"load_data\")), deps=[\"load_data\"])\ninc.register(\"train_model\", lambda: train(inc.get(\"clean_data\")), deps=[\"clean_data\"])\n\n# Compute all\nresults = inc.compute_all()\n\n# Update source data\nupdate_csv(\"data.csv\")\n\n# Mark as changed\ninc.invalidate(\"load_data\")\n\n# Only load_data + dependent steps recompute\nresults = inc.compute_all()  # clean_data and train_model also rerun\n</code></pre>"},{"location":"user-guide/performance/#dependency-tracking","title":"Dependency Tracking","text":"<pre><code># Complex dependency graph\ninc = IncrementalComputation()\n\ninc.register(\"A\", compute_a)\ninc.register(\"B\", compute_b)\ninc.register(\"C\", lambda: compute_c(inc.get(\"A\"), inc.get(\"B\")), deps=[\"A\", \"B\"])\ninc.register(\"D\", lambda: compute_d(inc.get(\"C\")), deps=[\"C\"])\n\n# If A changes, C and D recompute (but not B)\ninc.invalidate(\"A\")\ninc.compute_all()\n</code></pre>"},{"location":"user-guide/performance/#gpu-resource-management","title":"GPU Resource Management","text":"<p>Efficiently manage GPU memory and allocation.</p> <pre><code>from flowyml.utils.performance import GPUResourceManager\n\ngpu = GPUResourceManager()\n\n# Check availability\nif gpu.is_available():\n    print(f\"GPUs available: {gpu.get_device_count()}\")\n\n    #Get current usage\n    usage = gpu.get_memory_usage(device=0)\n    print(f\"GPU 0: {usage['used_mb']}/{usage['total_mb']} MB\")\n\n# Auto-select best GPU\ndevice = gpu.get_best_device()\nprint(f\"Using GPU: {device}\")\n\n# Allocate tensors\nimport torch\ntensor = torch.randn(1000, 1000).to(device)\n\n# Monitor usage\ngpu.print_memory_summary()\n</code></pre>"},{"location":"user-guide/performance/#automatic-gpu-selection","title":"Automatic GPU Selection","text":"<pre><code>@step(outputs=[\"model\"])\ndef train_on_best_gpu(data):\n    gpu = GPUResourceManager()\n    device = gpu.get_best_device()  # Least loaded GPU\n\n    model = Model().to(device)\n    model.fit(data)\n    return model\n</code></pre>"},{"location":"user-guide/performance/#dataframe-optimization","title":"DataFrame Optimization","text":"<p>Reduce pandas DataFrame memory usage.</p> <pre><code>from flowyml.utils.performance import optimize_dataframe\n\n# Original DataFrame\ndf = pd.read_csv(\"large_file.csv\")\nprint(f\"Original: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n\n# Optimize\ndf_optimized = optimize_dataframe(df)\nprint(f\"Optimized: {df_optimized.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n\n# Typical savings: 50-80%!\n</code></pre>"},{"location":"user-guide/performance/#optimization-techniques","title":"Optimization Techniques","text":"<pre><code># Manual optimization\ndf['category_col'] = df['category_col'].astype('category')  # String \u2192 Category\ndf['int_col'] = df['int_col'].astype('int32')  # int64 \u2192 int32\ndf['float_col'] = pd.to_numeric(df['float_col'], downcast='float')  # float64 \u2192 float32\n</code></pre>"},{"location":"user-guide/performance/#batch-processing","title":"Batch Processing","text":"<p>Process large datasets in batches.</p> <pre><code>from flowyml.utils.performance import batch_iterator\n\n# Process in batches\nfor batch in batch_iterator(large_dataset, batch_size=1000):\n    process_batch(batch)\n\n# With progress tracking\nfrom tqdm import tqdm\n\nfor batch in tqdm(batch_iterator(large_dataset, batch_size=1000)):\n    process_batch(batch)\n</code></pre>"},{"location":"user-guide/performance/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/performance/#1-profile-before-optimizing","title":"1. Profile Before Optimizing","text":"<pre><code>import time\n\n@step(outputs=[\"result\"])\ndef slow_step(data):\n    start = time.time()\n    result = expensive_operation(data)\n    print(f\"\u23f1\ufe0f Took {time.time() - start:.2f}s\")\n    return result\n\n# Find the bottleneck first!\n</code></pre>"},{"location":"user-guide/performance/#2-use-lazy-loading-for-large-data","title":"2. Use Lazy Loading for Large Data","text":"<pre><code>class Pipeline:\n    def __init__(self):\n        # Don't load immediately\n        self.data = LazyValue(lambda: load_huge_dataset())\n\n    def process(self):\n        # Load only when needed\n        return process(self.data.value)\n</code></pre>"},{"location":"user-guide/performance/#3-parallelize-independent-steps","title":"3. Parallelize Independent Steps","text":"<pre><code># \u2705 Good - independent steps\n@step(outputs=[\"A\"])\ndef compute_a():\n    ...\n\n@step(outputs=[\"B\"])\ndef compute_b():\n    ...\n\n# Can run in parallel!\n\n# \u274c Bad - sequential dependency\n@step(inputs=[\"A\"], outputs=[\"B\"])\ndef compute_b(a):\n    ...\n\n# Must run after A\n</code></pre>"},{"location":"user-guide/performance/#4-batch-for-memory-efficiency","title":"4. Batch for Memory Efficiency","text":"<pre><code># \u274c Bad - loads everything\ndata = pd.read_csv(\"huge_file.csv\")\nprocess(data)  # OOM!\n\n# \u2705 Good - process in batches\nfor chunk in pd.read_csv(\"huge_file.csv\", chunksize=10000):\n    process(chunk)\n</code></pre>"},{"location":"user-guide/performance/#5-monitor-gpu-usage","title":"5. Monitor GPU Usage","text":"<pre><code>gpu = GPUResourceManager()\n\n# Before training\nprint(f\"Free memory: {gpu.get_free_memory(0)} MB\")\n\n# Train\nmodel.fit(data)\n\n# After training\nprint(f\"Free memory: {gpu.get_free_memory(0)} MB\")\n\n# Clean up if needed\nif gpu.get_free_memory(0) &lt; 1000:  # &lt; 1GB\n    torch.cuda.empty_cache()\n</code></pre>"},{"location":"user-guide/performance/#performance-patterns","title":"Performance Patterns","text":""},{"location":"user-guide/performance/#pattern-1-lazy-cache","title":"Pattern 1: Lazy + Cache","text":"<pre><code>from functools import lru_cache\n\nclass DataPipeline:\n    @lazy_property\n    @lru_cache(maxsize=1)\n    def processed_data(self):\n        # Loaded and cached lazily\n        data = self.load_data()\n        return self.process(data)\n</code></pre>"},{"location":"user-guide/performance/#pattern-2-parallel-batch","title":"Pattern 2: Parallel + Batch","text":"<pre><code>from flowyml.utils.performance import ParallelExecutor, batch_iterator\n\nexecutor = ParallelExecutor(max_workers=4)\n\n# Process batches in parallel\nbatches = list(batch_iterator(dataset, batch_size=1000))\nresults = executor.map(process_batch, [[b] for b in batches])\n</code></pre>"},{"location":"user-guide/performance/#pattern-3-incremental-cache","title":"Pattern 3: Incremental + Cache","text":"<pre><code>from flowyml.utils.performance import IncrementalComputation\nfrom flowyml import SmartCache\n\ncache = SmartCache(ttl_seconds=3600)\ninc = IncrementalComputation()\n\ndef cached_compute(key):\n    cached = cache.get(key)\n    if cached:\n        return cached\n\n    result = expensive_computation()\n    cache.set(key, result)\n    return result\n\ninc.register(\"step1\", lambda: cached_compute(\"step1\"))\n</code></pre>"},{"location":"user-guide/performance/#api-reference","title":"API Reference","text":""},{"location":"user-guide/performance/#lazyvalue","title":"LazyValue","text":"<pre><code>LazyValue(fn: Callable)\n</code></pre> <p>Properties: - <code>value</code> - Gets computed value (computes on first access)</p>"},{"location":"user-guide/performance/#parallelexecutor","title":"ParallelExecutor","text":"<pre><code>ParallelExecutor(max_workers: int = None)\n</code></pre> <p>Methods: - <code>submit(fn, *args, **kwargs) -&gt; Future</code> - <code>map(fn, iterables) -&gt; List</code> - <code>wait_all(futures: Dict) -&gt; Dict</code></p>"},{"location":"user-guide/performance/#incrementalcomputation","title":"IncrementalComputation","text":"<pre><code>IncrementalComputation()\n</code></pre> <p>Methods: - <code>register(name, fn, deps=None)</code> - <code>compute(name) -&gt; Any</code> - <code>compute_all() -&gt; Dict</code> - <code>invalidate(name)</code> - <code>get(name) -&gt; Any</code></p>"},{"location":"user-guide/performance/#gpuresourcemanager","title":"GPUResourceManager","text":"<pre><code>GPUResourceManager()\n</code></pre> <p>Methods: - <code>is_available() -&gt; bool</code> - <code>get_device_count() -&gt; int</code> - <code>get_best_device() -&gt; str</code> - <code>get_memory_usage(device) -&gt; Dict</code> - <code>get_free_memory(device) -&gt; int</code> - <code>print_memory_summary()</code></p>"},{"location":"user-guide/performance/#optimize_dataframe","title":"optimize_dataframe","text":"<pre><code>optimize_dataframe(df: pd.DataFrame) -&gt; pd.DataFrame\n</code></pre>"},{"location":"user-guide/performance/#batch_iterator","title":"batch_iterator","text":"<pre><code>batch_iterator(iterable, batch_size: int)\n</code></pre>"},{"location":"user-guide/performance/#benchmarks","title":"Benchmarks","text":""},{"location":"user-guide/performance/#lazy-evaluation_1","title":"Lazy Evaluation","text":"<pre><code># Without lazy\nstart = time.time()\ndata1 = load_data()  # 5s\ndata2 = load_data()  # 5s\n# Might not use data2!\nprint(f\"Time: {time.time()-start}s\")  # 10s\n\n# With lazy\nstart = time.time()\ndata1 = LazyValue(load_data)\ndata2 = LazyValue(load_data)  # Instant\nprint(f\"Time: {time.time()-start}s\")  # 0s\n</code></pre>"},{"location":"user-guide/performance/#parallel-execution_1","title":"Parallel Execution","text":"<pre><code># Sequential\nfor item in items:  # 100 items, 0.1s each\n    process(item)\n# Total: 10s\n\n# Parallel (4 workers)\nexecutor.map(process, [[i] for i in items])\n# Total: 2.5s (4x faster!)\n</code></pre>"},{"location":"user-guide/performance/#dataframe-optimization_1","title":"DataFrame Optimization","text":"<p>Typical memory reduction: - int columns: 50% (int64 \u2192 int32) - float columns: 50% (float64 \u2192 float32) - string columns: 80% (object \u2192 category)</p> <p>Overall: 50-80% memory savings</p>"},{"location":"user-guide/projects/","title":"Projects &amp; Multi-Tenancy \ud83c\udfe2","text":"<p>Organize pipelines, runs, and artifacts into isolated projects for multi-tenant deployments.</p>"},{"location":"user-guide/projects/#overview-i","title":"Overview \u2139\ufe0f","text":"<p>The <code>Project</code> and <code>ProjectManager</code> classes provide: - Isolation: Each project has its own metadata store and artifact storage - Organization: Group related pipelines and runs together - Multi-tenancy: Support multiple teams/clients in one deployment - Resource management: Track pipelines, runs, and artifacts per project</p>"},{"location":"user-guide/projects/#quick-start","title":"Quick Start \ud83d\ude80","text":"<pre><code>from flowyml import Project\n\n# Create a project\nproject = Project(\"recommendation_system\")\n\n# Create pipelines within the project\npipeline = project.create_pipeline(\"training_v1\")\n\n# Add steps and run\npipeline.add_step(load_data)\npipeline.add_step(train_model)\nresult = pipeline.run()\n\n# Get project statistics\nstats = project.get_stats()\nprint(f\"Total runs: {stats['total_runs']}\")\nprint(f\"Total artifacts: {stats['total_artifacts']}\")\n</code></pre>"},{"location":"user-guide/projects/#project-management","title":"Project Management \ud83c\udfd7\ufe0f","text":""},{"location":"user-guide/projects/#creating-projects","title":"Creating Projects","text":"<pre><code>from flowyml import ProjectManager\n\nmanager = ProjectManager()\n\n# Create a new project\nproject = manager.create_project(\n    \"ml_platform\",\n    description=\"Main ML platform for product recommendations\"\n)\n\n# List all projects\nprojects = manager.list_projects()\nfor proj in projects:\n    print(f\"- {proj['name']}: {proj['description']}\")\n\n# Get existing project\nproject = manager.get_project(\"ml_platform\")\n</code></pre>"},{"location":"user-guide/projects/#project-structure","title":"Project Structure","text":"<p>Each project has its own directory structure:</p> <pre><code>projects/\n\u2514\u2500\u2500 my_project/\n    \u251c\u2500\u2500 project.json          # Project metadata\n    \u251c\u2500\u2500 runs/                 # Pipeline run data\n    \u251c\u2500\u2500 artifacts/            # Stored artifacts\n    \u2502   \u2514\u2500\u2500 cache/           # Artifact cache\n    \u2514\u2500\u2500 metadata.db          # SQLite metadata store\n</code></pre>"},{"location":"user-guide/projects/#working-with-projects","title":"Working with Projects \ud83d\udee0\ufe0f","text":""},{"location":"user-guide/projects/#creating-pipelines","title":"Creating Pipelines","text":"<pre><code>project = Project(\"analytics\")\n\n# Create multiple pipelines\netl_pipeline = project.create_pipeline(\"daily_etl\")\nreporting_pipeline = project.create_pipeline(\"weekly_reports\")\nml_pipeline = project.create_pipeline(\"model_training\")\n\n# Each pipeline uses the project's metadata store\netl_pipeline.add_step(extract_data)\netl_pipeline.run()\n</code></pre>"},{"location":"user-guide/projects/#querying-project-data","title":"Querying Project Data","text":"<pre><code># List all runs in the project\nruns = project.list_runs()\nfor run in runs:\n    print(f\"{run['pipeline_name']}: {run['status']}\")\n\n# Filter runs by pipeline\ntraining_runs = project.list_runs(pipeline_name=\"model_training\")\n\n# Get artifacts\nartifacts = project.get_artifacts()\nfor artifact in artifacts:\n    print(f\"{artifact['name']}: {artifact['type']}\")\n\n# Filter by artifact type\nmodels = project.get_artifacts(artifact_type=\"model\")\n</code></pre>"},{"location":"user-guide/projects/#project-statistics","title":"Project Statistics","text":"<pre><code>stats = project.get_stats()\n\nprint(f\"\"\"\nProject: {project.name}\nCreated: {stats['created_at']}\nPipelines: {stats['total_pipelines']}\nRuns: {stats['total_runs']}\nArtifacts: {stats['total_artifacts']}\n\"\"\")\n</code></pre>"},{"location":"user-guide/projects/#multi-tenant-architecture","title":"Multi-Tenant Architecture \ud83c\udfd8\ufe0f","text":""},{"location":"user-guide/projects/#isolating-client-data","title":"Isolating Client Data","text":"<pre><code># Setup for multiple clients\nclients = [\"acme_corp\", \"tech_startup\", \"enterprise_inc\"]\n\nfor client in clients:\n    # Each client gets their own project\n    project = manager.create_project(\n        client,\n        description=f\"ML pipelines for {client}\"\n    )\n\n    # Create client-specific pipelines\n    pipeline = project.create_pipeline(\"recommendation_engine\")\n    pipeline.add_step(load_client_data)  # Client-specific data\n    pipeline.add_step(train_model)\n\n    # Run in isolation\n    result = pipeline.run()\n</code></pre>"},{"location":"user-guide/projects/#resource-tracking","title":"Resource Tracking","text":"<pre><code>def get_client_usage(client_name):\n    project = manager.get_project(client_name)\n    stats = project.get_stats()\n\n    return {\n        \"client\": client_name,\n        \"pipelines\": stats['total_pipelines'],\n        \"runs\": stats['total_runs'],\n        \"artifacts\": stats['total_artifacts'],\n        \"storage_usage_mb\": project.get_storage_usage()\n    }\n\n# Generate usage report\nfor client in clients:\n    usage = get_client_usage(client)\n    print(f\"{client}: {usage['runs']} runs, {usage['storage_usage_mb']}MB\")\n</code></pre>"},{"location":"user-guide/projects/#best-practices","title":"Best Practices \ud83d\udca1","text":""},{"location":"user-guide/projects/#1-project-naming","title":"1. Project Naming","text":"<pre><code># Use descriptive, hierarchical names\nproject = Project(\"company_product_ml\")\n\n# Or organize by team/domain\nproject = Project(\"data_team_recommendations\")\n</code></pre>"},{"location":"user-guide/projects/#2-pipeline-organization","title":"2. Pipeline Organization","text":"<pre><code>project = Project(\"sales_analytics\")\n\n# Group related pipelines\nproject.create_pipeline(\"etl_daily\")\nproject.create_pipeline(\"etl_weekly\")\nproject.create_pipeline(\"reporting_dashboard\")\nproject.create_pipeline(\"forecasting_model\")\n</code></pre>"},{"location":"user-guide/projects/#3-cleanup-old-data","title":"3. Cleanup Old Data","text":"<pre><code># Export project before cleanup\nproject.export_metadata(\"backup.json\")\n\n# List pipelines for review\npipelines = project.get_pipelines()\n\n# Remove if needed (use with caution!)\n# manager.delete_project(\"old_project\", confirm=True)\n</code></pre>"},{"location":"user-guide/projects/#integration-examples","title":"Integration Examples \ud83d\udd0c","text":""},{"location":"user-guide/projects/#with-versioning","title":"With Versioning","text":"<pre><code>from flowyml import VersionedPipeline\n\nproject = Project(\"ml_prod\")\n\n# Create versioned pipeline within project\npipeline = project.create_pipeline(\"training\")\nversioned = VersionedPipeline(\"training\")\nversioned.version = \"v1.0.0\"\n\n# Use project's storage\nversioned.runs_dir = project.runs_dir\nversioned.metadata_store = project.metadata_store\n\nversioned.add_step(train)\nversioned.save_version()\nversioned.run()\n</code></pre>"},{"location":"user-guide/projects/#with-scheduling","title":"With Scheduling","text":"<pre><code>from flowyml import PipelineScheduler\n\nproject = Project(\"automated_ml\")\nscheduler = PipelineScheduler()\n\ndef run_project_pipeline():\n    pipeline = project.create_pipeline(\"daily_training\")\n    pipeline.add_step(train_model)\n    return pipeline.run()\n\nscheduler.schedule_daily(\n    name=f\"{project.name}_daily_run\",\n    pipeline_func=run_project_pipeline,\n    hour=2\n)\n</code></pre>"},{"location":"user-guide/projects/#api-reference","title":"API Reference \ud83d\udcda","text":""},{"location":"user-guide/projects/#project","title":"Project","text":"<p>Constructor: <pre><code>Project(\n    name: str,\n    description: str = \"\",\n    projects_dir: str = \".flowyml/projects\"\n)\n</code></pre></p> <p>Methods: - <code>create_pipeline(name: str, **kwargs) -&gt; Pipeline</code> - Create pipeline in project - <code>get_pipelines() -&gt; List[str]</code> - List all pipeline names - <code>list_runs(pipeline_name: Optional[str] = None, limit: int = 100) -&gt; List[Dict]</code> - <code>get_artifacts(artifact_type: Optional[str] = None, limit: int = 100) -&gt; List[Dict]</code> - <code>get_stats() -&gt; Dict</code> - Get project statistics - <code>export_metadata(path: str)</code> - Export project metadata</p>"},{"location":"user-guide/projects/#projectmanager","title":"ProjectManager","text":"<p>Methods: - <code>create_project(name: str, description: str = \"\") -&gt; Project</code> - Create new project - <code>get_project(name: str) -&gt; Optional[Project]</code> - Get existing project - <code>list_projects() -&gt; List[Dict]</code> - List all projects - <code>delete_project(name: str, confirm: bool = False)</code> - Delete project</p>"},{"location":"user-guide/projects/#faq","title":"FAQ \u2753","text":"<p>Q: Can I move a pipeline from one project to another? A: Currently, pipelines are tied to their project's metadata store. You would need to export/import the pipeline definition manually.</p> <p>Q: How do I backup a project? A: Use <code>project.export_metadata()</code> and copy the entire project directory from <code>.flowyml/projects/{project_name}/</code>.</p> <p>Q: What happens when I delete a project? A: All pipelines, runs, and artifacts associated with the project are removed. Always export metadata first!</p> <p>Q: Can projects share artifacts? A: No, projects are fully isolated by design. This ensures multi-tenant security and resource tracking.</p>"},{"location":"user-guide/scheduling/","title":"Pipeline Scheduling \u23f0","text":"<p>Automate pipeline execution so you never miss a deadline.</p> <p>[!NOTE] What you'll learn: How to schedule pipelines for recurring execution with zero manual intervention</p> <p>Key insight: Manual pipeline execution doesn't scale. Scheduling turns ad-hoc jobs into reliable automation.</p>"},{"location":"user-guide/scheduling/#why-scheduling-matters","title":"Why Scheduling Matters","text":"<p>Without scheduling: - Manual overhead: \"Did someone run the daily ETL?\" - Missed deadlines: Forgetting to run the weekly report - No reliability: Pipelines run only when someone remembers</p> <p>With flowyml scheduling: - Zero manual work: Pipelines run automatically - Multi-timezone: Run at 9 AM local time for each region - Fault-tolerant: Survives restarts, prevents duplicate runs</p>"},{"location":"user-guide/scheduling/#decision-guide-scheduling-strategy","title":"Decision Guide: Scheduling Strategy","text":"Use Case Schedule Type Example Business Reports <code>Daily</code> at specific time \"Run sales report at 8 AM\" Data Sync <code>Interval</code> (minutes/hours) \"Poll API every 15 minutes\" Complex Timing <code>Cron</code> expression \"Every weekday at 9 AM, except holidays\" High Frequency <code>Hourly</code> at specific minute \"Update cache at :00 past each hour\""},{"location":"user-guide/scheduling/#overview-i","title":"Overview \u2139\ufe0f","text":"<p>The <code>PipelineScheduler</code> provides: - Cron schedules: Complex schedules using standard cron syntax - Daily schedules: Run at specific times each day - Hourly schedules: Run at specific minute each hour - Interval schedules: Run at regular intervals - Timezone support: Schedule in any timezone - Persistence: Schedules survive restarts (SQLite backed) - Distributed: Coordinate across multiple servers (Redis/File locking)</p>"},{"location":"user-guide/scheduling/#quick-start","title":"Quick Start \ud83d\ude80","text":"<pre><code>from flowyml import Pipeline, PipelineScheduler, step\n\n# Define pipeline\n@step(outputs=[\"data\"])\ndef fetch_data():\n    return api.get_daily_data()\n\npipeline = Pipeline(\"daily_etl\")\npipeline.add_step(fetch_data)\n\n# Create scheduler (persistence enabled by default)\nscheduler = PipelineScheduler()\n\n# Schedule for daily execution at 2 AM New York time\nscheduler.schedule_daily(\n    name=\"daily_etl_job\",\n    pipeline_func=lambda: pipeline.run(),\n    hour=2,\n    minute=0,\n    timezone=\"America/New_York\"\n)\n\n# Start the scheduler\nscheduler.start()\n</code></pre>"},{"location":"user-guide/scheduling/#schedule-types","title":"Schedule Types \ud83d\udcc5","text":""},{"location":"user-guide/scheduling/#cron-schedule-new","title":"Cron Schedule (New!)","text":"<p>Use standard cron expressions for complex schedules. Requires <code>croniter</code>.</p> <pre><code># Run every weekday at 9 AM\nscheduler.schedule_cron(\n    name=\"weekday_report\",\n    pipeline_func=run_report,\n    cron_expression=\"0 9 * * 1-5\",\n    timezone=\"Europe/London\"\n)\n\n# Run on the first day of every month\nscheduler.schedule_cron(\n    name=\"monthly_billing\",\n    pipeline_func=run_billing,\n    cron_expression=\"0 0 1 * *\",\n    timezone=\"UTC\"\n)\n</code></pre>"},{"location":"user-guide/scheduling/#daily-schedule","title":"Daily Schedule","text":"<p>Run at a specific time each day.</p> <pre><code>scheduler.schedule_daily(\n    name=\"morning_report\",\n    pipeline_func=run_morning_pipeline,\n    hour=9,      # 9 AM\n    minute=30,   # 9:30 AM\n    timezone=\"Asia/Tokyo\"\n)\n</code></pre>"},{"location":"user-guide/scheduling/#hourly-schedule","title":"Hourly Schedule","text":"<p>Run at a specific minute each hour.</p> <pre><code>scheduler.schedule_hourly(\n    name=\"hourly_sync\",\n    pipeline_func=run_sync_pipeline,\n    minute=15    # Run at :15 past each hour (1:15, 2:15, 3:15, ...)\n)\n</code></pre>"},{"location":"user-guide/scheduling/#interval-schedule","title":"Interval Schedule","text":"<p>Run at regular intervals.</p> <pre><code># Run every 30 minutes\nscheduler.schedule_interval(\n    name=\"frequent_check\",\n    pipeline_func=run_check_pipeline,\n    minutes=30\n)\n</code></pre>"},{"location":"user-guide/scheduling/#advanced-features","title":"Advanced Features \u26a1","text":""},{"location":"user-guide/scheduling/#persistence","title":"Persistence","text":"<p>Schedules are automatically persisted to a local SQLite database (<code>.flowyml_scheduler.db</code>). This ensures that schedules are not lost if the application restarts.</p> <p>To configure persistence:</p> <pre><code>from flowyml.core.scheduler_config import SchedulerConfig\n\nconfig = SchedulerConfig(\n    persist_schedules=True,\n    db_path=\"/path/to/scheduler.db\"\n)\nscheduler = PipelineScheduler(config=config)\n</code></pre>"},{"location":"user-guide/scheduling/#distributed-scheduling","title":"Distributed Scheduling","text":"<p>For multi-server deployments, flowyml supports distributed locking to prevent duplicate executions.</p> <p>File-based Locking (Default): Good for single-machine, multi-process setups. Redis Locking: Recommended for multi-server setups.</p> <pre><code>config = SchedulerConfig(\n    distributed=True,\n    lock_backend=\"redis\",\n    redis_url=\"redis://localhost:6379/0\"\n)\nscheduler = PipelineScheduler(config=config)\n</code></pre>"},{"location":"user-guide/scheduling/#timezone-support","title":"Timezone Support","text":"<p>All schedule methods accept a <code>timezone</code> argument. Requires <code>pytz</code>.</p> <pre><code>scheduler.schedule_daily(\n    \"global_sync\",\n    run_sync,\n    hour=0,\n    timezone=\"UTC\"\n)\n</code></pre>"},{"location":"user-guide/scheduling/#monitoring-health","title":"Monitoring &amp; Health","text":"<p>The scheduler tracks metrics and health status.</p> <pre><code># Get health status\nhealth = scheduler.health_check()\nprint(f\"Status: {health['status']}\")\nprint(f\"Success Rate: {health['metrics']['success_rate']:.1%}\")\n</code></pre>"},{"location":"user-guide/scheduling/#managing-schedules","title":"Managing Schedules \ud83d\udee0\ufe0f","text":""},{"location":"user-guide/scheduling/#list-all-schedules","title":"List All Schedules","text":"<pre><code>schedules = scheduler.list_schedules()\n\nfor schedule in schedules:\n    status = \"\u2705 Enabled\" if schedule.enabled else \"\u23f8\ufe0f Paused\"\n    print(f\"{status} {schedule.pipeline_name}\")\n    print(f\"  Type: {schedule.schedule_type}\")\n    print(f\"  Next run: {schedule.next_run} ({schedule.timezone})\")\n</code></pre>"},{"location":"user-guide/scheduling/#enabledisableremove","title":"Enable/Disable/Remove","text":"<pre><code># Pause\nscheduler.disable(\"daily_etl_job\")\n\n# Resume\nscheduler.enable(\"daily_etl_job\")\n\n# Remove\nscheduler.unschedule(\"daily_etl_job\")\n\n# Clear all\nscheduler.clear()\n</code></pre>"},{"location":"user-guide/scheduling/#api-integration","title":"API Integration \ud83d\udd0c","text":"<p>The scheduler is fully integrated with the flowyml Backend API.</p> <p>Endpoints: - <code>GET /api/schedules</code>: List all schedules - <code>POST /api/schedules</code>: Create a new schedule - <code>GET /api/scheduler/health</code>: Get scheduler health metrics - <code>POST /api/schedules/{name}/enable</code>: Enable schedule - <code>POST /api/schedules/{name}/disable</code>: Disable schedule - <code>DELETE /api/schedules/{name}</code>: Delete schedule</p>"},{"location":"user-guide/scheduling/#deployment","title":"Deployment \ud83d\ude80","text":""},{"location":"user-guide/scheduling/#docker-with-persistence","title":"Docker with Persistence","text":"<p>Mount a volume to persist the scheduler database.</p> <pre><code>VOLUME /app/.flowyml_scheduler.db\nCMD [\"python\", \"scheduler.py\"]\n</code></pre>"},{"location":"user-guide/scheduling/#environment-variables","title":"Environment Variables","text":"<p>Configure the scheduler via environment variables:</p> <ul> <li><code>flowyml_SCHEDULER_PERSIST</code>: \"true\"/\"false\"</li> <li><code>flowyml_SCHEDULER_DB_PATH</code>: Path to SQLite DB</li> <li><code>flowyml_SCHEDULER_DISTRIBUTED</code>: \"true\"/\"false\"</li> <li><code>flowyml_SCHEDULER_REDIS_URL</code>: Redis connection string</li> <li><code>flowyml_SCHEDULER_TIMEZONE</code>: Default timezone</li> </ul>"},{"location":"user-guide/scheduling/#api-reference","title":"API Reference \ud83d\udcda","text":""},{"location":"user-guide/scheduling/#pipelinescheduler","title":"PipelineScheduler","text":"<pre><code>PipelineScheduler(config: Optional[SchedulerConfig] = None)\n</code></pre> <p>Methods: - <code>schedule_cron(name, func, cron_expression, timezone=\"UTC\")</code> - <code>schedule_daily(name, func, hour, minute, timezone=\"UTC\")</code> - <code>schedule_hourly(name, func, minute, timezone=\"UTC\")</code> - <code>schedule_interval(name, func, hours, minutes, seconds, timezone=\"UTC\")</code> - <code>health_check() -&gt; Dict</code> - <code>clear()</code></p>"},{"location":"user-guide/scheduling/#schedulerconfig","title":"SchedulerConfig","text":"<p>Configuration object for the scheduler.</p> <ul> <li><code>persist_schedules: bool</code></li> <li><code>db_path: str</code></li> <li><code>distributed: bool</code></li> <li><code>lock_backend: str</code></li> <li><code>redis_url: str</code></li> <li><code>timezone: str</code></li> </ul>"},{"location":"user-guide/ui/","title":"flowyml UI Guide \ud83d\udda5\ufe0f","text":"<p>The flowyml UI provides a real-time, interactive dashboard for monitoring and managing your ML pipelines. It offers a beautiful, modern interface to visualize execution graphs, inspect artifacts, and track metrics.</p>"},{"location":"user-guide/ui/#overview","title":"Overview","text":"<p>The UI consists of two main components: 1.  Backend Server: A FastAPI-based service that manages state and serves the API. 2.  Frontend Dashboard: A React/Vite application that provides the visual interface.</p>"},{"location":"user-guide/ui/#getting-started","title":"Getting Started \ud83d\ude80","text":""},{"location":"user-guide/ui/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have installed flowyml with UI support:</p> <pre><code>pip install \"flowyml[ui]\"\n</code></pre>"},{"location":"user-guide/ui/#starting-the-ui","title":"Starting the UI","text":"<p>To start the UI server, run:</p> <pre><code>flowyml ui start\n</code></pre> <p>This will: 1.  Start the backend API server (default port: 8000). 2.  Serve the frontend dashboard (default port: 8080). 3.  Open your browser to <code>http://localhost:8080</code>.</p>"},{"location":"user-guide/ui/#stopping-the-ui","title":"Stopping the UI","text":"<p>To stop the UI server:</p> <pre><code>flowyml ui stop\n</code></pre>"},{"location":"user-guide/ui/#features","title":"Features \u2728","text":""},{"location":"user-guide/ui/#dashboard","title":"\ud83d\udcca Dashboard","text":"<p>The main dashboard provides a high-level view of your system: - Recent Runs: See the status of the latest pipeline executions. - Pipeline Statistics: Success rates, average duration, and total runs. - System Health: Status of the backend and connected workers.</p>"},{"location":"user-guide/ui/#pipeline-visualization","title":"\ud83d\udd17 Pipeline Visualization","text":"<ul> <li>DAG View: Visualize your pipeline steps as a Directed Acyclic Graph.</li> <li>Step Details: Click on any step to view its inputs, outputs, logs, and execution time.</li> <li>Real-time Updates: Watch the graph animate as steps execute in real-time.</li> </ul>"},{"location":"user-guide/ui/#artifact-inspection","title":"\ud83d\udce6 Artifact Inspection","text":"<ul> <li>Asset Browser: Browse all generated datasets, models, and metrics.</li> <li>Lineage Tracking: See exactly which run produced an artifact and which steps consumed it.</li> <li>Preview: View snippets of dataframes, model summaries, and metric plots directly in the browser.</li> </ul>"},{"location":"user-guide/ui/#experiment-tracking","title":"\ud83d\udcc8 Experiment Tracking","text":"<ul> <li>Compare Runs: Select multiple runs to compare their metrics and parameters side-by-side.</li> <li>Metric Plots: Visualize loss curves, accuracy trends, and other metrics over time.</li> </ul>"},{"location":"user-guide/ui/#integration-with-pipelines","title":"Integration with Pipelines \ud83d\udd0c","text":"<p>flowyml pipelines automatically integrate with the UI when it is running. No special code is required!</p>"},{"location":"user-guide/ui/#automatic-registration","title":"Automatic Registration","text":"<p>When you run a pipeline in your Python script:</p> <pre><code>from flowyml import Pipeline, step\n\n@step\ndef my_step():\n    return \"Hello flowyml\"\n\npipeline = Pipeline(\"my_pipeline\")\npipeline.add_step(my_step)\npipeline.run()\n</code></pre> <p>If the UI server is running, the pipeline will automatically: 1.  Register itself with the backend. 2.  Report execution status for each step. 3.  Log artifacts and metrics to the UI.</p>"},{"location":"user-guide/ui/#logging-metrics","title":"Logging Metrics","text":"<p>You can log custom metrics from your steps to appear in the UI:</p> <pre><code>from flowyml import step, context\n\n@step\ndef train_model(epochs: int):\n    for epoch in range(epochs):\n        loss = calculate_loss()\n        # Metrics are automatically captured if returned or logged\n        # (Future: context.log_metric(\"loss\", loss))\n    return {\"final_loss\": loss}\n</code></pre>"},{"location":"user-guide/ui/#troubleshooting","title":"Troubleshooting \ud83d\udd27","text":""},{"location":"user-guide/ui/#ui-not-showing-runs","title":"UI Not Showing Runs","text":"<ol> <li>Ensure the UI server is running: <code>flowyml ui status</code></li> <li>Check if your pipeline script has access to the <code>.flowyml</code> directory.</li> <li>Verify that <code>enable_cache</code> is not preventing re-execution if you expect new runs.</li> </ol>"},{"location":"user-guide/ui/#port-conflicts","title":"Port Conflicts","text":"<p>If port 8080 is in use, you can specify a different port:</p> <pre><code>flowyml ui start --port 8081\n</code></pre>"},{"location":"user-guide/ui/#configuration","title":"Configuration \u2699\ufe0f","text":"<p>You can configure UI settings in your <code>flowyml.yaml</code> or <code>pyproject.toml</code>:</p> <pre><code># flowyml.yaml\nui:\n  host: \"0.0.0.0\"\n  port: 8080\n  backend_port: 8000\n</code></pre>"},{"location":"user-guide/versioning/","title":"Pipeline Versioning \ud83d\udd04","text":"<p>Track changes to your pipelines like you track code with Git.</p> <p>[!NOTE] What you'll learn: How to version your pipelines and compare changes over time</p> <p>Key insight: \"What changed between v1 and v2?\" is a question you'll ask every week. Versioning answers it instantly.</p>"},{"location":"user-guide/versioning/#why-versioning-matters","title":"Why Versioning Matters","text":"<p>Without versioning: - No audit trail: \"Who changed the data loader last month?\" - Risky deployments: You don't know what changed - Lost history: Can't rollback to a working version</p> <p>With flowyml versioning: - Full history: See exactly what changed and when - Safe releases: Compare new version with production before deploying - Easy rollback: Restore a previous version instantly</p>"},{"location":"user-guide/versioning/#when-to-version","title":"When to Version","text":"Scenario Action Before deploying to production Always save a version After adding/removing steps Save with descriptive metadata Major logic changes Bump version number (v1.0 \u2192 v2.0) Daily experiments Not necessary\u2014only version what you'll deploy"},{"location":"user-guide/versioning/#overview-i","title":"Overview \u2139\ufe0f","text":"<p>The <code>VersionedPipeline</code> class provides: - Save pipeline versions with metadata - Compare versions to see what changed - Track pipeline evolution over time - Maintain a history of pipeline configurations</p>"},{"location":"user-guide/versioning/#basic-usage","title":"Basic Usage \ud83d\ude80","text":"<pre><code>from flowyml import VersionedPipeline, step\n\n# Create a versioned pipeline\npipeline = VersionedPipeline(\"training_pipeline\")\npipeline.version = \"v1.0.0\"\n\n# Add steps\n@step(outputs=[\"data\"])\ndef load_data():\n    return load_from_source()\n\n@step(inputs=[\"data\"], outputs=[\"model\"])\ndef train(data):\n    return train_model(data)\n\npipeline.add_step(load_data)\npipeline.add_step(train)\n\n# Save the version\npipeline.save_version(metadata={\"description\": \"Initial training pipeline\"})\n</code></pre>"},{"location":"user-guide/versioning/#comparing-versions","title":"Comparing Versions \ud83d\udd0d","text":"<pre><code># Make changes to the pipeline\n@step(inputs=[\"model\"], outputs=[\"metrics\"])\ndef evaluate(model):\n    return evaluate_model(model)\n\npipeline.add_step(evaluate)\npipeline.version = \"v1.1.0\"\npipeline.save_version(metadata={\"description\": \"Added evaluation step\"})\n\n# Compare versions\ndiff = pipeline.compare_with(\"v1.0.0\")\nprint(diff)\n# Output:\n# {\n#   'current_version': 'v1.1.0',\n#   'compared_to': 'v1.0.0',\n#   'added_steps': ['evaluate'],\n#   'removed_steps': [],\n#   'modified_steps': [],\n#   'step_order_changed': True,\n#   'context_changes': {...}\n# }\n\n# Display comparison in readable format\npipeline.display_comparison(\"v1.0.0\")\n</code></pre>"},{"location":"user-guide/versioning/#listing-versions","title":"Listing Versions \ud83d\udccb","text":"<pre><code># Get all saved versions\nversions = pipeline.list_versions()\nprint(versions)  # ['v1.0.0', 'v1.1.0']\n\n# Get specific version details\nversion_info = pipeline.get_version(\"v1.0.0\")\nprint(version_info.steps)\nprint(version_info.created_at)\nprint(version_info.metadata)\n</code></pre>"},{"location":"user-guide/versioning/#version-storage","title":"Version Storage \ud83d\udcbe","text":"<p>Versions are stored as JSON files in <code>.flowyml/versions/{pipeline_name}/</code> by default. You can customize the storage location:</p> <pre><code>pipeline = VersionedPipeline(\n    \"my_pipeline\",\n    versions_dir=\"/custom/path/versions\"\n)\n</code></pre>"},{"location":"user-guide/versioning/#best-practices","title":"Best Practices \ud83d\udca1","text":""},{"location":"user-guide/versioning/#1-use-semantic-versioning","title":"1. Use Semantic Versioning","text":"<pre><code>pipeline.version = \"v1.0.0\"  # Major.Minor.Patch\n</code></pre>"},{"location":"user-guide/versioning/#2-add-descriptive-metadata","title":"2. Add Descriptive Metadata","text":"<pre><code>pipeline.save_version(metadata={\n    \"description\": \"Added data validation step\",\n    \"author\": \"data-team\"impact\": \"improved data quality\",\n    \"breaking_changes\": False\n})\n</code></pre>"},{"location":"user-guide/versioning/#3-compare-before-deploying","title":"3. Compare Before Deploying","text":"<pre><code># Always compare with production version before deploying\nif pipeline.version != \"v1.0.0\":  # production version\n    diff = pipeline.compare_with(\"v1.0.0\")\n    if diff['removed_steps'] or diff['modified_steps']:\n        print(\"\u26a0\ufe0f Breaking changes detected!\")\n        pipeline.display_comparison(\"v1.0.0\")\n</code></pre>"},{"location":"user-guide/versioning/#advanced-features","title":"Advanced Features \u26a1","text":""},{"location":"user-guide/versioning/#hash-based-change-detection","title":"Hash-Based Change Detection","text":"<p>The versioning system uses content hashing to detect changes in step implementations:</p> <pre><code># Same step name, different implementation\n@step(outputs=[\"data\"])\ndef load_data():\n    # Modified implementation\n    return load_from_new_source()  # Changed!\n\npipeline.save_version()\n# Will detect that load_data was modified\n</code></pre>"},{"location":"user-guide/versioning/#context-parameter-tracking","title":"Context Parameter Tracking","text":"<p>Changes to context parameters are automatically tracked:</p> <pre><code>from flowyml import context\n\nctx1 = context(learning_rate=0.001)\npipeline = VersionedPipeline(\"training\", context=ctx1)\npipeline.save_version()\n\n# Change context\nctx2 = context(learning_rate=0.01)  # Changed!\npipeline.context = ctx2\npipeline.save_version()\n\ndiff = pipeline.compare_with(previous_version)\n# Will show context_changes\n</code></pre>"},{"location":"user-guide/versioning/#integration-with-cicd","title":"Integration with CI/CD \ud83d\ude80","text":"<pre><code># In your CI pipeline\ndef verify_version_changes():\n    pipeline = VersionedPipeline.load(\"production_pipeline\")\n\n    # Get current production version\n    prod_version = get_production_version()\n\n    # Compare\n    diff = pipeline.compare_with(prod_version)\n\n    # Enforce policies\n    if diff['removed_steps']:\n        raise ValueError(\"Cannot remove steps in minor version update\")\n\n    if diff['modified_steps']:\n        # Require integration tests\n        run_integration_tests()\n\n    return diff\n</code></pre>"},{"location":"user-guide/versioning/#api-reference","title":"API Reference \ud83d\udcda","text":""},{"location":"user-guide/versioning/#versionedpipeline","title":"VersionedPipeline","text":"<p>Constructor: <pre><code>VersionedPipeline(\n    name: str,\n    version: str = \"v0.1.0\",\n    versions_dir: str = \".flowyml/versions\"\n)\n</code></pre></p> <p>Methods: - <code>save_version(metadata: Optional[Dict] = None)</code> - Save current version - <code>list_versions() -&gt; List[str]</code> - List all saved versions - <code>get_version(version: str) -&gt; PipelineVersion</code> - Get version details - <code>compare_with(other_version: str) -&gt; Dict</code> - Compare with another version - <code>display_comparison(other_version: str)</code> - Pretty print comparison - <code>run(*args, **kwargs)</code> - Run the pipeline (inherited from Pipeline)</p>"},{"location":"user-guide/versioning/#pipelineversion","title":"PipelineVersion","text":"<p>Attributes: - <code>version: str</code> - Version identifier - <code>pipeline_name: str</code> - Pipeline name - <code>created_at: str</code> - Creation timestamp - <code>steps: List[str]</code> - List of step names - <code>step_hashes: Dict[str, str]</code> - Step content hashes - <code>context_params: Dict</code> - Context parameters - <code>metadata: Dict</code> - Custom metadata</p>"}]}